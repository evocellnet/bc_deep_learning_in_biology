{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Protein stability prediction\n",
    "\n",
    "In the project you will try to predict protein stability changes upon point mutations. \n",
    "We will use acuumulated data from experimental databases, i.e. the Megascale dataset. A current [paper](https://www.pnas.org/doi/10.1073/pnas.2314853121) has already preprocessed the dataset and created homology reduced data splits. We will reuse these.\n",
    "\n",
    "The downloaded data includes measurements of changes in the Gibbs free enrgy ($\\Delta \\Delta G $). \n",
    "This will be the value that you will have to predict for a given protein with a point mutation. \n",
    "\n",
    "Here we will use protein embeddings computed by ESM as input. \n",
    "We provide precomputed embeddings from the last layer of the smallest ESM model. You can adjust the Dataloader's code to load the embedding of the wild type or of the mutated sequence or both. You can use it however you like. This is just to provide you easy access to embeddings. If you want to compute your own embeddings from other layers or models you can do that, too. \n",
    "\n",
    "Below we provide you with a strcuture for the project that you can start with.  \n",
    "Edit the cells to your liking and add more code to create your final model.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn.metrics as skmetrics\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import lightning as L\n",
    "\n",
    "import torchmetrics\n",
    "from torchmetrics.regression import PearsonCorrCoef"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloading\n",
    "\n",
    "We are using the Megascale dataset. The train, validation and test sets are already predefined.  \n",
    "As mentioned, we provide embeddings from the last layer of ESM as input. You can access either the wild type or the mutated sequence and you could also further adjsut the embeddings. \n",
    "Here we have an embedding representing the complete sequence. It was computed by averaging over the embeddings per residue in the sequence. \n",
    "\n",
    "The ``Dataset`` classes return tuples of ``(embedding, ddg_value)``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataloaders load the tensors from memory one by one, could potentially become a bottleneck\n",
    "\n",
    "class ProtEmbeddingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for the embeddings of the mutated sequences\n",
    "    You can the get_item() method to return the data in the format you want\n",
    "    \"\"\"\n",
    "    def __init__(self, tensor_folder, csv_file, id_col=\"name\", label_col=\"ddG_ML\"):\n",
    "        \"\"\"\n",
    "        Initialize the dataset\n",
    "        input at init: \n",
    "            tensor_folder: path to the directory with the embeddings we want to use, eg. \"/home/data/mega_train_embeddings\"\n",
    "            cvs_file: path to the csv file corresponding to the data, eg. \"home/data/mega_train.csv\"\n",
    "        \"\"\"\n",
    "        self.tensor_folder = tensor_folder\n",
    "        self.df = pd.read_csv(csv_file, sep=\",\")\n",
    "        # only use the mutation rows\n",
    "        self.df = self.df[self.df.mut_type!=\"wt\"]\n",
    "        # get the labels and ids\n",
    "        self.labels = torch.tensor(self.df[label_col].values)\n",
    "        self.ids = self.df[id_col].values\n",
    "        self.wt_names = self.df[\"WT_name\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load embeddings\n",
    "        # mutation embedding\n",
    "        tensor_path = os.path.join(self.tensor_folder, self.ids[idx] + \".pt\")\n",
    "        tensor = torch.load(tensor_path)['mean_representations'][6]\n",
    "\n",
    "        # wildtype embedding, uncomment if you want to use this, too\n",
    "        \"\"\"\n",
    "        tensor_wt = None\n",
    "        try:\n",
    "            # try loading with _wte.pt suffix\n",
    "            tensor_path_wt = os.path.join(self.tensor_folder, self.wt_names[idx] + \"_wte.pt\")\n",
    "            tensor_wt = torch.load(tensor_path_wt)['mean_representations'][6]\n",
    "        except FileNotFoundError:\n",
    "            try:\n",
    "                # try loading with _wty.pt suffix\n",
    "                tensor_path_wt = os.path.join(self.tensor_folder, self.wt_names[idx] + \"_wty.pt\")\n",
    "                tensor_wt = torch.load(tensor_path_wt)['mean_representations'][6]\n",
    "            except FileNotFoundError:\n",
    "                print(\"No file found for WT embedding of \", self.wt_names[idx], \" with either _wte.pt or _wty.pt suffix.\")\n",
    "                pass  # neither file found, tensor_wt remains None\n",
    "        \"\"\"\n",
    "\n",
    "        label = self.labels[idx] # ddG value\n",
    "        # returns a tuple of the input embedding and the target ddG values\n",
    "        return tensor, label.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usage \n",
    "# make sure to adjust the paths to where your files are located\n",
    "dataset_train = ProtEmbeddingDataset('project_data/mega_train_embeddings', 'project_data/mega_train.csv')\n",
    "dataset_val = ProtEmbeddingDataset('project_data/mega_val_embeddings', 'project_data/mega_val.csv')\n",
    "# dataset_test = ProtEmbeddingDataset('project_data/mega_test_embeddings', 'project_data/mega_test.csv') # only once available\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=1024, shuffle=True, num_workers=16)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=512, shuffle=False, num_workers=16)\n",
    "# dataloader_test = DataLoader(dataset_test, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "\n",
    "Explore the data and try to understand what we are working with. How is the data structured? How is it distributed? What do the values mean? How is the data represented and how else could it be represented? \n",
    "Your approach depends on your understanding of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture and training\n",
    "\n",
    "Now it's your turn. Create a model trained on the embeddings and the corresponding ddG values.  \n",
    "Be aware that this is not a classification task, but a regression task. You want to predict a continuous number that is as close to the measured $\\Delta \\Delta G $ value as possible.\n",
    "You will need to adjust your architecture and loss accordingly.\n",
    "\n",
    "Train the model with the predefined dataloaders. And try to improve the model. \n",
    "Only test on the test set at the very end, when you have finished fine-tuning you model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation and visualization\n",
    "\n",
    "To get a good feeling of how the model is performing and to compare with literature, compute the Pearson and Spearman correlations.\n",
    "You can also plot the predictions in a scatterplot. We have added some code for that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds =[]\n",
    "all_y = []\n",
    "# save all predictions\n",
    "for batch in dataloader_val:\n",
    "    # adjust this to work with your model\n",
    "    x,y = batch\n",
    "    y_hat = model(x)\n",
    "    preds.append(y_hat.squeeze().detach().numpy())\n",
    "    all_y.append(y.detach().numpy())\n",
    "\n",
    "# concatenate and plot\n",
    "preds= np.concatenate(preds)\n",
    "all_y = np.concatenate(all_y)\n",
    "\n",
    "sns.regplot(x=preds,y=all_y)\n",
    "plt.xlabel(\"Predicted ddG\")\n",
    "plt.ylabel(\"Measured ddG\")\n",
    "\n",
    "# get RMSE, Pearson and Spearman correlation \n",
    "print(\"RMSE:\", skmetrics.mean_squared_error(all_y, preds, squared=\"False\"))\n",
    "print(\"Pearson r:\", scipy.stats.pearsonr(preds, all_y))\n",
    "print(\"Spearman r:\", scipy.stats.spearmanr(preds, all_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "Try to analyse and interpret your model and/or predictions in the context of the biological question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on the test set\n",
    "\n",
    "At the very end, after choosing your best model, see how well it performs on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5fa888489dcef296c36d3d3b759d2bbafdf14549bdfa862d5619a4427d05b08f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
