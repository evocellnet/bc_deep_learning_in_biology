{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Protein stability prediction\n",
    "\n",
    "In the project you will try to predict protein stability changes upon point mutations. \n",
    "We will use acuumulated data from experimental databases, i.e. the Megascale dataset. A current [paper](https://www.pnas.org/doi/10.1073/pnas.2314853121) has already preprocessed the dataset and created homology reduced data splits. We will reuse these.\n",
    "\n",
    "The data you have downloaded includes measurements of changes in the Gibbs free enrgy ($\\Delta \\Delta G $). \n",
    "\n",
    "Here we will use the sequence as input. \n",
    "The model will predict the $\\Delta \\Delta G $ of point mutations in this sequence. To make training more efficient, the model should directly predict the values for all possible mutations at each position in the sequence. So the expected output is a sequence of $ L \\ (sequence \\ length) \\ x \\ 20 \\ (number \\ amino \\ acids)$. This will be shown in detail later. \n",
    "\n",
    "Below we provide you with a strcuture for the project that you can start with.  \n",
    "Edit the cells to your liking and add more code to create your final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn.metrics as skmetrics\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import lightning as L\n",
    "\n",
    "import torchmetrics\n",
    "from torchmetrics.regression import PearsonCorrCoef"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloading\n",
    "\n",
    "We are using the Megascale dataset. The train, validation and test sets are already predefined.\n",
    "We provide code to load the data and helper functions to encode the sequences to numerical vectors (one-hot encoding). You can use this code as a starting point, adjust it, or use your own data loading. \n",
    "\n",
    "Each sequence will be treated as one batch (easier to deal with different leghts this way). The class below returns a dictionary containing the one-hot encoded sequence of dimension $Lx20$, the target sequence of dimension $Lx20$, containing the $\\Delta \\Delta G $ values, and a mask of the same dimension which indicates with a 1 if an experimental value is available for that position. Only compute your loss on the positions where an experimental value is available. So compute your loss similar to this example:\n",
    "\n",
    "``` \n",
    "prediction = model(x)\n",
    "loss = loss(prediction[mask==1],labels[mask==1])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_alphabet = 'ACDEFGHIKLMNPQRSTVWY' # amino acid alphabet\n",
    "aa_to_int = {aa: i for i, aa in enumerate(aa_alphabet)} # mapping from amino acid to number\n",
    "\n",
    "# function to one hot encode sequence\n",
    "def one_hot_encode(sequence):\n",
    "    # initialize a zero matrix of shape (len(sequence), len(amino_acids))\n",
    "    one_hot = torch.zeros(len(sequence), len(aa_alphabet))\n",
    "    for i, aa in enumerate(sequence):\n",
    "        # set the column corresponding to the amino acid to 1\n",
    "        one_hot[i].scatter_(0, torch.tensor([aa_to_int[aa]]), 1)\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "# sequence data, comes already batched, so treat accordingly in dataloader (batch_size=1)\n",
    "class SequenceData(Dataset):\n",
    "    def __init__(self, csv_file, label_col=\"ddG_ML\"):\n",
    "        \"\"\"\n",
    "        Initializes the dataset. \n",
    "        input:\n",
    "            csv_file: path to the relevant data file, eg. \"/home/data/mega_train.csv\"\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_file, sep=\",\")\n",
    "        self.label_col = label_col\n",
    "        # only have mutation rows\n",
    "        self.df = self.df[self.df.mut_type!=\"wt\"]\n",
    "        # process the mutation row\n",
    "        self.df[\"mutation_pos\"] = self.df[\"mut_type\"].apply(lambda x: int(x[1:-1])-1) # make position start at zero\n",
    "        self.df[\"mutation_to\"] = self.df[\"mut_type\"].apply(lambda x: aa_to_int[x[-1]]) # give numerical label to mutation\n",
    "\n",
    "        # group by wild type\n",
    "        self.df = self.df.groupby(\"WT_name\").agg(list)\n",
    "        # get wild type names\n",
    "        self.wt_names = self.df.index.values\n",
    "        # precompute one-hot encoding for faster training\n",
    "        self.encoded_seqs = {}\n",
    "        for wt_name in self.wt_names:\n",
    "            # get the correct row\n",
    "            mut_row = self.df.loc[wt_name]\n",
    "            seq = mut_row[\"wt_seq\"][0]\n",
    "            self.encoded_seqs[wt_name] = one_hot_encode(seq)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get the wild type name\n",
    "        wt_name = self.wt_names[idx]\n",
    "        # get the correct row\n",
    "        mut_row = self.df.loc[wt_name]\n",
    "        # get the wt sequence in one hot encoding\n",
    "        sequence_encoding = self.encoded_seqs[wt_name]\n",
    "\n",
    "        # create mask and target tensors\n",
    "        mask = torch.zeros((1, len(sequence_encoding),20)) # will be 1 where we have a measurement\n",
    "        target = torch.zeros((1, len(sequence_encoding),20)) # ddg values\n",
    "        # all mutations from df\n",
    "        positions = torch.tensor(mut_row[\"mutation_pos\"])\n",
    "        amino_acids = torch.tensor(mut_row[\"mutation_to\"])\n",
    "        # get the labels\n",
    "        labels = torch.tensor(mut_row[self.label_col])\n",
    "\n",
    "        for i in range(len(sequence_encoding)):\n",
    "            mask[0,i,amino_acids[positions==i]] = 1 # one where we have data\n",
    "            target[0,i,amino_acids[positions==i]] = labels[positions==i] # fill with ddG values\n",
    "        \n",
    "        # returns encoded sequence, mask and target sequence \n",
    "        return {\"sequence\": sequence_encoding[None,:,:].float(), \"mask\": mask, \"labels\": target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usage\n",
    "dataset_train = SequenceData('project_data/mega_train.csv')\n",
    "dataset_val= SequenceData('project_data/mega_val.csv')\n",
    "# dataset_test = SequenceData('project_data/mega_test.csv') # only once available\n",
    "\n",
    "# use batch_size=1 bc we treat each sequence as one batch\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=1, shuffle=False)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=1, shuffle=True)\n",
    "# dataloader_test = DataLoader(dataset_test, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "\n",
    "Explore the data and try to understand what we are working with. How is the data structured? How is it distributed? What do the values mean? How is the data represented and how else could it be represented? \n",
    "Your approach depends on your understanding of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture and training\n",
    "\n",
    "Now it's your turn. Create a model trained on the given sequences.  \n",
    "Be aware that this is not a classification task, but a regression task. You want to predict continuous numbers as close to the measured $\\Delta \\Delta G $ value as possible.\n",
    "You will need to adjust your architecture and loss accordingly.\n",
    "\n",
    "Train the model with the predefined dataloaders and try to improve it. \n",
    "Only test on the test set at the very end, when you have finished fine-tuning you model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation and visualization\n",
    "\n",
    "To get a good feeling of how the model is performing and to compare with literature, compute the Pearson and Spearman correlations.\n",
    "You can also plot the predictions in a scatterplot. We have added some code for that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds =[]\n",
    "all_y = []\n",
    "\n",
    "for batch in dataloader_val:\n",
    "    # read from batch\n",
    "    x = batch[\"sequence\"][0]\n",
    "    mask = batch[\"mask\"][0]\n",
    "    target = batch[\"labels\"][0]\n",
    "    ## adjust to work with your model\n",
    "    # predict\n",
    "    prediction = model(x)\n",
    "    preds.append(prediction[mask==1].flatten().detach().numpy()) # flatten to create one dimensional vector from 2D sequence\n",
    "    all_y.append(target[mask==1].flatten().detach().numpy()) # flatten to create one dimensional vector from 2D sequence\n",
    "\n",
    "# concatenate and plot\n",
    "preds= np.concatenate(preds)\n",
    "all_y = np.concatenate(all_y)\n",
    "\n",
    "sns.regplot(x=preds,y=all_y)\n",
    "plt.xlabel(\"Predicted ddG\")\n",
    "plt.ylabel(\"Measured ddG\")\n",
    "\n",
    "# get RMSE, Pearson and Spearman correlation \n",
    "print(\"RMSE:\", skmetrics.mean_squared_error(all_y, preds, squared=\"False\"))\n",
    "print(\"Pearson r:\", scipy.stats.pearsonr(preds, all_y))\n",
    "print(\"Spearman r:\", scipy.stats.spearmanr(preds, all_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "Try to analyse and interpret your model and/or predictions in the context of the biological question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on the test set\n",
    "\n",
    "At the very end, after choosing your best model, see how well it performs on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5fa888489dcef296c36d3d3b759d2bbafdf14549bdfa862d5619a4427d05b08f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
