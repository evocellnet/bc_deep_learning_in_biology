{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10ad0467-fc31-4586-9d84-fea0948fb7a1",
   "metadata": {},
   "source": [
    "# Lab 1: Perceptron\n",
    "In this tutorial we introduce some of the concepts for working with neural networks using [Pytorch](https://pytorch.org/tutorials/recipes/recipes_index.html). The entire notebook can be executed as-is, given the lack of time for this first lab session. We encourage you to explore the code yourselves to get comfortable with the concepts of deel learning in the context of biology. A few questions at the end challenge you to play around with the code and try things for yourselves.\n",
    "\n",
    "In this session, you will create a simple neural network that classifies any given DNA sequence as protein coding or not. As a starting point, we use as examples the coding DNA sequences from humans (homo sapiens (HS)). As negatives, we use random sequences of DNA where each nucleotide is drawn from a uniform distribution over the possible nucleotides. We then train a neural network on de [codon frequencies](https://en.wikipedia.org/wiki/DNA_and_RNA_codon_tables) of these sequences.\n",
    "\n",
    "In addition to human DNA sequences, we also take a look at coding sequences from mice ([mus musculus (MM)](https://en.wikipedia.org/wiki/House_mouse)) and yeast ([saccharomyces cerevisiae (SC)](https://en.wikipedia.org/wiki/Saccharomyces_cerevisiae)). There are subtle differences between the coding frequencies of these species. You will test how well your human-trained model is able to recover the coding sequences for mice and yeast (think of your results in the context of evolutionary distances between species). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d75f7f4-c7b1-46a8-8637-5cba8634b516",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# import basic functionality\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# libraries for plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4f5ede-7ed4-4cfc-bfdd-a9fddddbabbd",
   "metadata": {},
   "source": [
    "# Step 1: Pre-processing the data\n",
    "Here we download and pre-process the dataset. We only consider DNA sequences that are protein coding, contain a integer number of codons, have a start and stop codon, and do not contain any uncertain nucleotides. Finally, we remove duplicates and randomly mix the sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318b2d30-85ab-4aa6-837c-d2d6693584ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# download and unpack DNA coding sequences for human, mouse and yeast\n",
    "############################\n",
    "\n",
    "!mkdir -p ~/all_seqs\n",
    "\n",
    "!wget -P ~/all_seqs/ https://ftp.ensembl.org/pub/current_fasta/homo_sapiens/cds/Homo_sapiens.GRCh38.cds.all.fa.gz\n",
    "!gzip -df \"all_seqs/Homo_sapiens.GRCh38.cds.all.fa.gz\"\n",
    "\n",
    "!wget -P ~/all_seqs/ https://ftp.ensembl.org/pub/current_fasta/saccharomyces_cerevisiae/cds/Saccharomyces_cerevisiae.R64-1-1.cds.all.fa.gz\n",
    "!gzip -df \"all_seqs/Saccharomyces_cerevisiae.R64-1-1.cds.all.fa.gz\"\n",
    "\n",
    "!wget -P ~/all_seqs/ https://ftp.ensembl.org/pub/current_fasta/mus_musculus/cds/Mus_musculus.GRCm39.cds.all.fa.gz\n",
    "!gzip -df \"all_seqs/Mus_musculus.GRCm39.cds.all.fa.gz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b21b02-c373-4f98-895e-8424ae1fbd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# function that loads and processes a FASTA file containing coding sequences\n",
    "def load_species_cds(file_name):\n",
    "    train_cds = []\n",
    "    train_prot = []\n",
    "    for record in SeqIO.parse(file_name, \"fasta\"):\n",
    "        # ensure that sequences are protein coding\n",
    "        if 'gene_biotype:protein_coding' in record.description:\n",
    "            if 'transcript_biotype:protein_coding' in record.description:\n",
    "                if ' cds ' in record.description:\n",
    "                    if len(record.seq) % 3 == 0:\n",
    "                        train_cds.append(str(record.seq))\n",
    "                        train_prot.append(str(record.seq.translate()))\n",
    "                        \n",
    "    # keep sequences that are protein coding\n",
    "    train_cds_filtered = []\n",
    "    train_prot_filtered = []\n",
    "    for i in range(len(train_prot)):\n",
    "        if (train_prot[i][0]=='M') & (train_prot[i][-1]=='*'):\n",
    "            train_cds_filtered.append(train_cds[i])\n",
    "            train_prot_filtered.append(train_prot[i])\n",
    "\n",
    "    # avoid sequences with undetermined/uncertain nucleotides\n",
    "    # restrict to sequences with at least 100 aa for codon frequency estimation\n",
    "    train_prot_filtered = [train_prot_filtered[i] for i in range(len(train_cds_filtered)) if ('N' not in train_cds_filtered[i]) and (len(train_cds_filtered[i])>=300)]\n",
    "    train_cds_filtered = [train_cds_filtered[i] for i in range(len(train_cds_filtered)) if ('N' not in train_cds_filtered[i]) and (len(train_cds_filtered[i])>=300)]\n",
    "\n",
    "    \n",
    "    # remove duplicates and randomly mix the list of sequences\n",
    "    seqs = list(zip(train_cds_filtered, train_prot_filtered))\n",
    "    seqs = list(set(seqs))\n",
    "    random.shuffle(seqs)\n",
    "    train_cds_filtered, train_prot_filtered = zip(*seqs)\n",
    "    \n",
    "    return list(train_cds_filtered), list(train_prot_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc9f681-fc16-46e5-9565-26e8010694e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load coding sequences for different species\n",
    "print('loading human proteins')\n",
    "cds_hs, prot_hs = load_species_cds(\"all_seqs/Homo_sapiens.GRCh38.cds.all.fa\")\n",
    "\n",
    "print('loading yeast proteins')\n",
    "cds_sc, prot_sc = load_species_cds(\"all_seqs/Saccharomyces_cerevisiae.R64-1-1.cds.all.fa\")\n",
    "\n",
    "print('loading mouse proteins')\n",
    "cds_mm, prot_mm = load_species_cds(\"all_seqs/Mus_musculus.GRCm39.cds.all.fa\")\n",
    "\n",
    "# take a look at some sequences\n",
    "[cds_hs[i][0:40]+'...' for i in range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d668a3b0-5e51-4867-bf0c-3c79015d24ff",
   "metadata": {},
   "source": [
    "# Step 2: Encoding sequences as codon frequencies\n",
    "The steps above give us a set of unique coding sequences for humans, mice and yeast. To train a neural network on de coding frequencies of these sequences, we encode the sequences by converting each sequence to an array of frequencies for each possible codon. Each codon gets assigned a index in the array. We first create a dictionary that contains all possible codons for a given codon length and input sequence. This dictionary then allows us to convert between codon (e.g., 'ATG') and indices in the array (e.g., 'ATG' -> 0) to keep track of the codon frequencies.\n",
    "\n",
    "Biologically, [DNA codons](https://en.wikipedia.org/wiki/DNA_and_RNA_codon_tables) consist of three nucleotides, encoding amino acids. However, since we are training a neural network to classify a sequence to be protein coding or not, we can choose any number of nucleotides to represent a 'codon'. For example, we can choose a \"codon length\" (codon_length) of a single nucleotide (which would result in us training the model on the [frequencies of nucleotides in DNA](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC403801/)), or a codon length of two nucleotides (no biological meaning as this does not represent a biological unit - we do not expect a model to learn any biology at all), or a codon length of 6 nucleotides (representing pairs of amino acids - would this yield a model that \"learns\" any biology?). You can play around with the codon_length yourself, but we start with a codon length of 3 nucleotides - represeting exactly one amino acid. Finally, you could also train the model on the frequencies of amino acids by using protein sequences as input. You can try this yourself by changing the training_seqs variable to prot_hs that was defined above (protein sequences for humans)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef05a7a-18e1-491b-9daa-8138f869e354",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_seqs = cds_hs.copy()\n",
    "training_seqs_mm = cds_mm.copy()\n",
    "training_seqs_sc = cds_sc.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e192214c-c2a6-4dee-b338-4f1d3b35df06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the alphabet for the current set of sequences\n",
    "import itertools\n",
    "alphabet = list(set(itertools.chain.from_iterable([list(set(training_seqs[i])) for i in range(len(training_seqs))])))\n",
    "\n",
    "# number of nucleotides per codon\n",
    "codon_length = 3\n",
    "codons = [''.join(cur_tok) for cur_tok in list(itertools.product(alphabet, repeat = codon_length))]\n",
    "\n",
    "# define dictionary for encoding the codons\n",
    "codon_to_int = dict((c, i) for i, c in enumerate(codons))\n",
    "int_to_codon = dict((i, c) for i, c in enumerate(codons))\n",
    "\n",
    "# take a look at the alphabet, codons, and encoding\n",
    "alphabet, codons[0:5],int_to_codon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3c37d9-8b9a-49c8-ac33-8c7b55701e9b",
   "metadata": {},
   "source": [
    "Having established a dictionary to convert between codon and array indices, we can encode our DNA sequences into codon frequencies. Here, we define two functions to do that for us. The first converts DNA sequences to an array of codon frequencies using the defined dictionary. The second function creates a random sequence of codons following the same sequence length distribution as the DNA sequences. These nucleotides are drawn from a random uniform distribution over the possible codons.\n",
    "\n",
    "Finally, for both functions, each sequence gets assigned a label that we can use to train our model (i.e., label = 1 tells the model that a sequence is protein coding, label = 0 tells the model that a sequence is not protein coding). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d4516e-b117-47dc-84f7-de75a68dc1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# function to encode DNA sequence into codons and convert to codon frequency using defined codon dictionary\n",
    "def encode_seq_to_codon_freq(sequences, use_label):\n",
    "    sequences_freqs = []\n",
    "    sequences_labels = []\n",
    "\n",
    "    for cur_seq in sequences:\n",
    "        # initialize array for codon counts\n",
    "        seq_to_codon_freq = np.zeros(len(codons))\n",
    "\n",
    "        # split sequence into codons\n",
    "        seq_codons = [cur_seq[i:i+codon_length] for i in range(0, len(cur_seq), codon_length)]\n",
    "\n",
    "        # count codon frequencies\n",
    "        for cur_codon in seq_codons:\n",
    "            if len(cur_codon)==codon_length:\n",
    "                seq_to_codon_freq[codon_to_int[cur_codon]]+=1\n",
    "        seq_to_codon_freq /= len(seq_codons)\n",
    "\n",
    "        # add codon frequencies and label for current sequence to collection\n",
    "        sequences_freqs.append( seq_to_codon_freq )\n",
    "        sequences_labels.append(use_label)\n",
    "    return sequences_freqs, sequences_labels\n",
    "\n",
    "\n",
    "# function to generate random codon frequency sample given nr of codons in sequences\n",
    "def random_codon_freq(sequences, use_label):\n",
    "    sequences_freqs = []\n",
    "    sequences_labels = []\n",
    "\n",
    "    for cur_seq in sequences:\n",
    "        # initialize array for codon counts\n",
    "        seq_to_codon_freq = np.zeros(len(codons))\n",
    "        \n",
    "        # define nr of codons in sequence\n",
    "        nr_codons = round(len(cur_seq)/codon_length)\n",
    "\n",
    "        # generate random sequence of codons given the current sequence length\n",
    "        seq_rnd = list(np.random.randint(low=0, high=len(codons), size = nr_codons, dtype=int))\n",
    "\n",
    "        # count codon frequencies\n",
    "        for i in seq_rnd:\n",
    "            seq_to_codon_freq[i]+=1\n",
    "        seq_to_codon_freq /= nr_codons\n",
    "\n",
    "        # add codon frequencies and label for current sequence to collection\n",
    "        sequences_freqs.append( seq_to_codon_freq )\n",
    "        sequences_labels.append(use_label)\n",
    "    return sequences_freqs, sequences_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010bb335-a6d9-48c9-a75b-4ddd04e989e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# using functions and codons as defined above, encode sequences with their codon frequencies\n",
    "seqs_codon_freqs_hs, seqs_codon_labels_hs = encode_seq_to_codon_freq(training_seqs, 1) # sequences, label (is sequence / no sequence)\n",
    "rnd_codon_freqs_hs,  rnd_codon_labels_hs = random_codon_freq(training_seqs, 0)\n",
    "\n",
    "seqs_codon_freqs_mm, seqs_codon_labels_mm = encode_seq_to_codon_freq(training_seqs_mm, 0)\n",
    "seqs_codon_freqs_sc, seqs_codon_labels_sc = encode_seq_to_codon_freq(training_seqs_sc, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51f9f00-f397-4bd4-bf8c-2f5e1c802f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show us one sample (codon frequencies and label)\n",
    "seqs_codon_freqs_hs[0], seqs_codon_labels_hs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ad4b4f-1b0f-4930-91a7-cc9423930baf",
   "metadata": {},
   "source": [
    "We have now encoded all DNA sequences in the same format: an array of codon frequencies of length 64 and a label, and the different sequences can now be used as input for a model (i.e., we converted the DNA sequences (variable-length text) into a consistent format (array of numbers of fixed length) where each position in the array has the same meaning across sequences. To illustrate this, we plot the codon frequencies across species and our random sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac18de5-0967-4cc5-9228-1e31f4e2f6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot coding frequencies for different species\n",
    "# initialize figure\n",
    "sns.set(rc={'figure.figsize':(5,15)})\n",
    "sns.set(font=\"Arial\")\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# average and merge codon frequencies of the different species and random sequences\n",
    "codon_freqs = pd.concat([pd.DataFrame(np.mean(np.array(seqs_codon_freqs_hs),axis=0)),\n",
    "                         pd.DataFrame(np.mean(np.array(seqs_codon_freqs_mm),axis=0)),\n",
    "                         pd.DataFrame(np.mean(np.array(seqs_codon_freqs_sc),axis=0)),\n",
    "                         pd.DataFrame(np.mean(np.array(rnd_codon_freqs_hs),axis=0))\n",
    "                         ],axis=1)\n",
    "\n",
    "# label codons and sort by human frequency\n",
    "codon_freqs.index = [int_to_codon[cur_codon] for cur_codon in list(codon_freqs.index)]\n",
    "codon_freqs.reset_index(inplace=True)\n",
    "codon_freqs.columns = ['codon','human','mouse','yeast','random']\n",
    "codon_freqs.sort_values(by='human',ascending=False,inplace=True)\n",
    "\n",
    "# stack dataframe of frequencies\n",
    "plot_freqs = codon_freqs.set_index('codon').stack().reset_index()\n",
    "plot_freqs.columns = ['codon','origin','frequency']\n",
    "\n",
    "# plot codon frequencies for different species\n",
    "sns.barplot(data=plot_freqs,x='frequency',y='codon',hue='origin')#, x = 'codon', y='frequency', hue='origin')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd187f1-4a50-4aa9-b65a-9662b4d83c19",
   "metadata": {},
   "source": [
    "# Step 3: Creating a dataloader\n",
    "Having encoded our DNA sequences as codon frequencies, we are ready to prepare the data for training a neural network. We will create a 'dataloader' that converts the arrays into Tensors (appropriate format for pytorch) and takes care of splitting the data into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f334547-e066-4b86-974a-28fab683b9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function defining the data loader\n",
    "def get_dataloader_simple(train_samples_x, train_samples_y, batch_size):\n",
    "    train_x = np.array(train_samples_x)\n",
    "    train_y = np.array(train_samples_y)\n",
    "    train_data = TensorDataset(torch.from_numpy(train_x).float(), torch.from_numpy(train_y).float())\n",
    "    \n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dl = DataLoader(\n",
    "        dataset=train_data,\n",
    "        batch_size=batch_size,\n",
    "        sampler=train_sampler,\n",
    "        drop_last=True\n",
    "    )\n",
    "    return train_dl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d01963f-3aaf-42eb-a6e9-09194556a7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a set of positive and negative samples, print number of available samples\n",
    "positive_set = (seqs_codon_freqs_hs, seqs_codon_labels_hs)\n",
    "negative_set = (rnd_codon_freqs_hs, rnd_codon_labels_hs)\n",
    "len(positive_set[0]), len(negative_set[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bc5562-cc15-4d67-ad57-c80db0bd4426",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define number of samples to take for training and validation. We pick small numbers to speed up the training process\n",
    "batch_size = 300\n",
    "nr_train_samples = 5000\n",
    "nr_val_samples = 2000\n",
    "\n",
    "# define training (train+test) and validation data by mixing a balanced number of positive and negative samples\n",
    "nr_train_samples = round(nr_train_samples/2)\n",
    "nr_val_samples = round(nr_val_samples/2)\n",
    "train_dl = get_dataloader_simple(positive_set[0][0:nr_train_samples]+negative_set[0][0:nr_train_samples], \n",
    "                                 positive_set[1][0:nr_train_samples]+negative_set[1][0:nr_train_samples],\n",
    "                                 batch_size)\n",
    "\n",
    "val_dl = get_dataloader_simple(positive_set[0][nr_train_samples:(nr_train_samples+nr_val_samples)]+negative_set[0][nr_train_samples:(nr_train_samples+nr_val_samples)],\n",
    "                               positive_set[1][nr_train_samples:(nr_train_samples+nr_val_samples)]+negative_set[1][nr_train_samples:(nr_train_samples+nr_val_samples)],\n",
    "                               1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d11caf-2312-44d9-b79b-7fab1a70a3a9",
   "metadata": {},
   "source": [
    "# Step 4: Define model\n",
    "As a final preparation, we define our [model](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). We create a class that instructs pytorch to follow a certain architecture for the model. Our model initializes all relevant parts (init function) and tells pytorch how to compute the output for a given input (forward function). For our perceptron, we use a single dense linear layer with a sigmoidal activation function. You can play around with the model architecture later - several options are left as comments.\n",
    "We are trying to train a model for solving a classfication problem. The labels for our samples are binary (ones and zeros). We therefore use a binary loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8622d3d0-e4c0-45a2-8ce4-a29f687f05a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the model architecture\n",
    "class myModel(nn.Module):\n",
    "    def __init__(self, input_param, hidden_param, output_param, dropout_prob):\n",
    "        super(myModel, self).__init__()\n",
    "        \n",
    "        self.input_param = input_param\n",
    "        self.hidden_param = hidden_param\n",
    "        self.output_param = output_param\n",
    "        \n",
    "#        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.linear0 = nn.Linear(input_param, output_param)\n",
    "#        self.linear1 = nn.Linear(input_param, hidden_param)\n",
    "#        self.relu = nn.ReLU()\n",
    "#        self.linear2 = nn.Linear(hidden_param, output_param)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inp):\n",
    "#        inp_drop = self.dropout(inp)\n",
    "        layer0 = self.linear0(inp)\n",
    "        return self.sigmoid(layer0)\n",
    "\n",
    "#        layer1 = self.linear1(inp)\n",
    "#        layer1_act = self.relu(layer1)\n",
    "#        layer2 = self.linear2(layer1_act)\n",
    "#        return self.sigmoid(layer2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a0b0e8-d022-449b-af7c-7e781e8ae1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda8ce7f-391d-45c8-afdf-7e1a2731eb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# use binary cross entropy los for this classification problem\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# initialize an instance of our model class (a variable that is a model following the architecture we defined above)\n",
    "model = myModel(len(codon_to_int), 20, 1, 0).to(device)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(count_parameters(model))\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8497c6-3537-47a4-9d87-a65fc4ae284d",
   "metadata": {},
   "source": [
    "# Step 5: Train simple model \n",
    "To train our model, we need a \"training loop\". \n",
    "1) First, we tell pytorch that we want to train our model (so it has to keep track of gradients). \n",
    "2) We then iterate over our data in batches to speed up computations (there is little advantage for computing the gradient with all samples over, say, a few hundred samples). \n",
    "3) We set the gradients to zero (we don't want to re-use previous computations for our next training step).\n",
    "4) We compute the output of the model for the given input sequences. \n",
    "5) We then compute the loss of the model output for the given target labels of the input sequences and [backpropagate](https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html) the loss through the network to compute the gradient. \n",
    "6) Finally, we instruct the optimizer to use the gradient and perform one appropriately-sized [step](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.step.html) to update the model weights. \n",
    "7) To keep track of our effors we compute the accuracy and training loss for the current samples.\n",
    "\n",
    "Finally, we train our model using the given data and training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bd0758-d940-4da9-8d1b-0ec4b610bce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the training loop\n",
    "def train(model, train_loader, optimizer, device):\n",
    "    # training mode\n",
    "    model.train(True)\n",
    "    \n",
    "    # Enabling gradient calculation\n",
    "    with torch.set_grad_enabled(True):\n",
    "        collect_loss = 0\n",
    "        correct = 0\n",
    "        nr_samples = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # send features and labels to GPU/CPU\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # zero the gradients\n",
    "            model.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # compute output of model\n",
    "            output = model(data)\n",
    "\n",
    "            # compute the loss and update model parameters\n",
    "            loss = criterion(output, target.unsqueeze(1))\n",
    "            loss.backward()\n",
    "\n",
    "            # adjust learning weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # store training loss\n",
    "            collect_loss += loss.item()\n",
    "            \n",
    "            # compute accuracy of training data\n",
    "            pred = torch.round(output,decimals=0)\n",
    "            correct += (pred.eq(target.view_as(pred)).sum().item())\n",
    "            nr_samples += len(target)\n",
    "            \n",
    "        return collect_loss*batch_size/nr_samples, correct/nr_samples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79926869-0e09-4179-a115-64860d5e0837",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 30\n",
    "\n",
    "# initialize model\n",
    "model = myModel(len(codon_to_int), 20, 1, 0).to(device)\n",
    "\n",
    "# use stochastic gradient descent with the given learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# Train the model on the current fold\n",
    "for epoch in range(1, n_epochs):\n",
    "    # train the model and get training loss\n",
    "    train_loss = train(model, train_dl, optimizer, device)\n",
    "    print(epoch, train_loss[0], train_loss[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dafc6ad-7bbb-4770-9ad8-8877e21b4365",
   "metadata": {},
   "source": [
    "Similar to the training loop, we need a \"test loop\" to get the output of the model for a given set of validation samples on which we do not train the model. \n",
    "1) First, we tell pytorch that we do NOT want to train our model (no keeping track of gradients - evaluation mode). \n",
    "2) We then iterate over our validation data. \n",
    "3) We compute the output of the model for the given input sequences. \n",
    "4) We then compute the loss of the model output for the given target labels of the input sequences.\n",
    "5) We compute the accuracy and training loss for the current samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842a1b3c-8732-48cc-ba0c-af9e13538a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define the test loop\n",
    "def test(model, test_loader, device, batched = True):\n",
    "    # Evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        collect_loss = 0\n",
    "        correct = 0\n",
    "        nr_samples = 0\n",
    "        for data, target in test_loader:\n",
    "            # send features and labels to GPU/CPU\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # compute output of model\n",
    "            output = model(data)\n",
    "\n",
    "            # store test loss\n",
    "            collect_loss += criterion(output, target.unsqueeze(1)).item()\n",
    "            \n",
    "            # compute accuracy for test data\n",
    "            pred = torch.round(output,decimals=0)\n",
    "            correct += (pred.eq(target.view_as(pred)).sum().item())\n",
    "            nr_samples += len(target)\n",
    "            \n",
    "        if batched:\n",
    "            collect_loss *= batch_size/nr_samples\n",
    "        else:\n",
    "            collect_loss /= nr_samples\n",
    "            \n",
    "        return collect_loss, correct/nr_samples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdd5f2e-5737-4f00-94c8-43c5f9a86256",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Validation error, accuracy:')\n",
    "test(model, val_dl, device, batched = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff335ce-77f6-4ca2-b8d2-e8b3faeb5941",
   "metadata": {},
   "source": [
    "# Step 6: Cross-validation\n",
    "For cross-validation, we perform the same steps but create the dataloader for each split of test and training data. The training and testing is identical to the above, with a slightly extended loop for tracking the errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87416e7f-1033-4b25-a181-db7a39261a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function defining the data loaders for k-fold cross-validation\n",
    "def get_dataloader_kfold(cur_dataset, train_idx, test_idx, batch_size):    \n",
    "    train_dl = DataLoader(\n",
    "        dataset=cur_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=torch.utils.data.SubsetRandomSampler(train_idx),\n",
    "        drop_last=True\n",
    "    )\n",
    "    test_dl = DataLoader(\n",
    "        dataset=cur_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=torch.utils.data.SubsetRandomSampler(test_idx),\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    return train_dl, test_dl\n",
    "\n",
    "\n",
    "# function to package samples into a Tensor dataset\n",
    "def samples_to_dataset(train_samples_x, train_samples_y):\n",
    "    train_x = np.array(train_samples_x)\n",
    "    train_y = np.array(train_samples_y)\n",
    "    return TensorDataset(torch.from_numpy(train_x).float(), torch.from_numpy(train_y).float())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef7593c-035b-41cb-b756-8538258020f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a set of positive and negative samples\n",
    "positive_set = (seqs_codon_freqs_hs, seqs_codon_labels_hs)\n",
    "negative_set = (rnd_codon_freqs_hs, rnd_codon_labels_hs)\n",
    "len(positive_set[0]), len(negative_set[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0d9308-af3e-4c76-8ed8-aa1f90531a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define number of samples to take for training and validation. We pick small numbers to speed up the process\n",
    "nr_train_samples = 5000\n",
    "nr_val_samples = 2000\n",
    "\n",
    "# define training (train+test) and validation data. \n",
    "# (!) These are NOT dataloaders but TensorDatasets because we split them into folds and convert to dataloaders during training\n",
    "nr_train_samples = round(nr_train_samples/2)\n",
    "nr_val_samples = round(nr_val_samples/2)\n",
    "train_dataset = samples_to_dataset(positive_set[0][0:nr_train_samples]+negative_set[0][0:nr_train_samples], \n",
    "                                   positive_set[1][0:nr_train_samples]+negative_set[1][0:nr_train_samples])\n",
    "\n",
    "val_dataset = samples_to_dataset(positive_set[0][nr_train_samples:(nr_train_samples+nr_val_samples)]+negative_set[0][nr_train_samples:(nr_train_samples+nr_val_samples)],\n",
    "                                 positive_set[1][nr_train_samples:(nr_train_samples+nr_val_samples)]+negative_set[1][nr_train_samples:(nr_train_samples+nr_val_samples)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3409f77e-35e5-4965-b2a3-3699f56f2f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "k_folds = 5\n",
    "batch_size = 300\n",
    "\n",
    "# Initialize the k-fold cross validation\n",
    "# make sure to shuffle the data before splitting into folds (our input data may be ordered!)\n",
    "kf = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "# prepare dataframe to store training errors\n",
    "train_error = pd.DataFrame(index=range(n_epochs),columns=range(k_folds))\n",
    "test_error = pd.DataFrame(index=range(n_epochs),columns=range(k_folds))\n",
    "train_acc = pd.DataFrame(index=range(n_epochs),columns=range(k_folds))\n",
    "test_acc = pd.DataFrame(index=range(n_epochs),columns=range(k_folds))\n",
    "\n",
    "# Loop through each fold\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(train_dataset)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    print(\"-------\")\n",
    "    print(\"epoch: \",end='')\n",
    "\n",
    "    # define training and test data for given fold and batch size\n",
    "    train_dl, test_dl = get_dataloader_kfold(train_dataset, train_idx, test_idx, batch_size)\n",
    "    \n",
    "    # initialize a model\n",
    "    model = myModel(len(codon_to_int), 20, 1, 0).to(device)\n",
    "\n",
    "    # define optimizer with the given learning rate\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "        \n",
    "    # Train the model on the current fold\n",
    "    for epoch in range(1, n_epochs):\n",
    "        # train the model and get training loss\n",
    "        train_loss = train(model, train_dl, optimizer, device)\n",
    "        \n",
    "        # test the model on training data\n",
    "        test_loss = test(model, test_dl, device, batched = True)\n",
    "        \n",
    "        # save training error and accuracy\n",
    "        train_error.loc[epoch, fold] = train_loss[0]\n",
    "        test_error.loc[epoch, fold] = test_loss[0]\n",
    "        train_acc.loc[epoch, fold] = train_loss[1]\n",
    "        test_acc.loc[epoch, fold] = test_loss[1]\n",
    "        \n",
    "        print(epoch, end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad486ed-82f3-4bd1-ad2d-4aa937c19222",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Using last model & last epoch from cross-validation; error, accuracy:')\n",
    "test(model, DataLoader(dataset = val_dataset), device, batched = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f776f5f0-19be-49b5-b6f3-01983a522ae5",
   "metadata": {},
   "source": [
    "# Step 7: Plotting training/testing error and accuracy\n",
    "Using the output from training, we can plot the results for each epoch to look at the learning of our model. For this, we average the errors over the different folds from training. Carefully look at the training and testing error to choose an appropriate number of epochs for training (to avoid overfitting). \n",
    "\n",
    "NOTE: the initial model included trains very fast with a small error and high accuracy. The error and accuracy become interesting when looking at training on other species as negative samples in the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222bba0e-7699-47fd-aade-13e9513915ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize figure\n",
    "sns.set(rc={'figure.figsize':(5,5)})\n",
    "sns.set(font=\"Arial\")\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# format loss data\n",
    "plot_loss_df = pd.concat([train_error.mean(axis=1), test_error.mean(axis=1)],axis=1).iloc[1:,].reset_index()\n",
    "plot_loss_df.columns = ['epoch','training','test']\n",
    "plot_loss_df = plot_loss_df.set_index('epoch').stack().reset_index()\n",
    "plot_loss_df.columns = ['epoch','dataset','loss']\n",
    "\n",
    "# plot training and test loss as function of epoch\n",
    "sns.lineplot(data=plot_loss_df, x='epoch', y='loss',hue='dataset')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02a974c-c592-4672-abdf-b903cdaa2839",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize figure\n",
    "sns.set(rc={'figure.figsize':(5,5)})\n",
    "sns.set(font=\"Arial\")\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# format loss data\n",
    "plot_loss_df = pd.concat([train_acc.mean(axis=1), test_acc.mean(axis=1)],axis=1).iloc[1:,].reset_index()\n",
    "plot_loss_df.columns = ['epoch','training','test']\n",
    "plot_loss_df = plot_loss_df.set_index('epoch').stack().reset_index()\n",
    "plot_loss_df.columns = ['epoch','dataset','loss']\n",
    "\n",
    "# plot training and test loss as function of epoch\n",
    "sns.lineplot(data=plot_loss_df, x='epoch', y='loss',hue='dataset')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4deadf2-9143-4ac3-81d9-1dad19fb445f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, using a trained model, we can compute a 'probability' that a given input sequence is encoding a protein\n",
    "# here we pick a random human sequence that was not used for training, you can change this to any sequence you would like (change 'cds_hs' for human to 'cds_mm' for mice etc).\n",
    "random_validation_sample = np.random.randint(low=nr_train_samples, high=(nr_train_samples+nr_val_samples), size = 1, dtype=int)[0]\n",
    "cur_test_seq = cds_hs[random_validation_sample]\n",
    "print('sequence: ',cur_test_seq[0:50])\n",
    "\n",
    "# Evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # send features and labels to GPU/CPU\n",
    "    data = torch.from_numpy(encode_seq_to_codon_freq([cur_test_seq],0)[0][0]).float().to(device)\n",
    "\n",
    "    # compute output of model\n",
    "    output = model(data)\n",
    "    print('probability: ', output.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ae809e-65bd-4080-9894-c84966a4d225",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 8: Questions\n",
    "1) how many parameters does your model have?\n",
    "2) what happens if you change the codon length?\n",
    "3) adapt the code for computing the probability of a sequence being coding to compute the probability for mouse and yeast DNA sequences. Having trained the model on human DNA sequences, what is the difference in average probabilities between species? Why?\n",
    "4) train the model using mouse or yeast sequences as negative samples for coding sequences. This creates a model that learns to classify sequences as being likely from mice/yeast or from humans\n",
    "5) change model architecture (e.g., more layers, other parameters, dropout parameters) / minimize training time / minimize number of parameters\n",
    "6) what happens to the probablities when sequences are frameshifted? can you train the model using frameshifted sequences as input?\n",
    "7) train the model on amino acid frequencies instead of codons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561f95e6-cbb1-451f-bbd6-751cf5335818",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
