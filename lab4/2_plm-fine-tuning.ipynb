{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning protein language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6.** Describe the problem of predicting the subcellular location of (prokaryotic) proteins as described in [Moreno et al., 2024](https://doi.org/10.1093/bioinformatics/btae677)? Think of a biological question one could answer with proteome-wide predictions of subcellular location, and potential follow-up experiments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, sklearn.preprocessing\n",
    "import datasets, evaluate, transformers # Hugging Face libraries https://doi.org/10.18653/v1/2020.emnlp-demos.6\n",
    "import Bio.SeqIO.FastaIO # Biopython for reading fasta files\n",
    "from IPython.display import display, HTML\n",
    "random_number = 4 # https://xkcd.com/221/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 5492k  100 5492k    0     0   459k      0  0:00:11  0:00:11 --:--:--  452k\n"
     ]
    }
   ],
   "source": [
    "# Uncomment & execute once to download data from https://services.healthtech.dtu.dk/services/DeepLocPro-1.0/\n",
    "#!mkdir -p data\n",
    "#!curl https://services.healthtech.dtu.dk/services/DeepLocPro-1.0/data/graphpart_set.fasta -o data/graphpart_set.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7.** Look at the contents of `df_data`, how was the column `fold_id` defined in the paper? What exact set of sequences are in this data set (check number of rows)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniprot_id</th>\n",
       "      <th>subcellular_location</th>\n",
       "      <th>organism_group</th>\n",
       "      <th>fold_id</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8560</th>\n",
       "      <td>Q8A0Z3</td>\n",
       "      <td>Cytoplasmic</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>MAVTMADITKLRKMTGAGMMDCKNALTEAEGDYDKAMEIIRKKGQA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8568</th>\n",
       "      <td>Q8A2N8</td>\n",
       "      <td>Cytoplasmic</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>MIMSKETLIKSIREIPDFPIPGILFYDVTTLFKDPWCLQELSNIMF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2593</th>\n",
       "      <td>P32709</td>\n",
       "      <td>CYtoplasmicMembrane</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>MTQTSAFHFESLVWDWPIAIYLFLIGISAGLVTLAVLLRRFYPQAG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>E7FHF8</td>\n",
       "      <td>Cytoplasmic</td>\n",
       "      <td>archaea</td>\n",
       "      <td>0</td>\n",
       "      <td>MKLGVFELTDCGGCALNLLFLYDKLLDLLEFYEIAEFHMATSKKSR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>E7FHU4</td>\n",
       "      <td>Cytoplasmic</td>\n",
       "      <td>archaea</td>\n",
       "      <td>0</td>\n",
       "      <td>MGKVRIGFYALTSCYGCQLQLAMMDELLQLIPNAEIVCWFMIDRDS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5239</th>\n",
       "      <td>Q97F85</td>\n",
       "      <td>Cytoplasmic</td>\n",
       "      <td>positive</td>\n",
       "      <td>4</td>\n",
       "      <td>MRKLFTSESVTEGHPDKICDQISDAILDAILEKDPNGRVACETTVT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5226</th>\n",
       "      <td>P33656</td>\n",
       "      <td>Cytoplasmic</td>\n",
       "      <td>positive</td>\n",
       "      <td>4</td>\n",
       "      <td>MKNKTEVKNGGEKKNSKKVSKEESAKEKNEKMKIVKNLIDKGKKSG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11894</th>\n",
       "      <td>P13949</td>\n",
       "      <td>OuterMembrane</td>\n",
       "      <td>negative</td>\n",
       "      <td>4</td>\n",
       "      <td>MCALDRRERPLNSQSVNKYILNVQNIYRNSPVPVCVRNKNRKILYA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11890</th>\n",
       "      <td>P42185</td>\n",
       "      <td>Extracellular</td>\n",
       "      <td>negative</td>\n",
       "      <td>4</td>\n",
       "      <td>MRLRFSVPLFFFGCVFVHGVFAGPFPPPGMSLPEYWGEEHVWWDGR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11902</th>\n",
       "      <td>Q0P986</td>\n",
       "      <td>OuterMembrane</td>\n",
       "      <td>negative</td>\n",
       "      <td>4</td>\n",
       "      <td>MKTRFSLILSACLLSSSLFAKNTDDEITKLQKQLAQIQAELAQIRK...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11906 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      uniprot_id subcellular_location organism_group  fold_id  \\\n",
       "8560      Q8A0Z3          Cytoplasmic       negative        0   \n",
       "8568      Q8A2N8          Cytoplasmic       negative        0   \n",
       "2593      P32709  CYtoplasmicMembrane       negative        0   \n",
       "61        E7FHF8          Cytoplasmic        archaea        0   \n",
       "63        E7FHU4          Cytoplasmic        archaea        0   \n",
       "...          ...                  ...            ...      ...   \n",
       "5239      Q97F85          Cytoplasmic       positive        4   \n",
       "5226      P33656          Cytoplasmic       positive        4   \n",
       "11894     P13949        OuterMembrane       negative        4   \n",
       "11890     P42185        Extracellular       negative        4   \n",
       "11902     Q0P986        OuterMembrane       negative        4   \n",
       "\n",
       "                                                sequence  \n",
       "8560   MAVTMADITKLRKMTGAGMMDCKNALTEAEGDYDKAMEIIRKKGQA...  \n",
       "8568   MIMSKETLIKSIREIPDFPIPGILFYDVTTLFKDPWCLQELSNIMF...  \n",
       "2593   MTQTSAFHFESLVWDWPIAIYLFLIGISAGLVTLAVLLRRFYPQAG...  \n",
       "61     MKLGVFELTDCGGCALNLLFLYDKLLDLLEFYEIAEFHMATSKKSR...  \n",
       "63     MGKVRIGFYALTSCYGCQLQLAMMDELLQLIPNAEIVCWFMIDRDS...  \n",
       "...                                                  ...  \n",
       "5239   MRKLFTSESVTEGHPDKICDQISDAILDAILEKDPNGRVACETTVT...  \n",
       "5226   MKNKTEVKNGGEKKNSKKVSKEESAKEKNEKMKIVKNLIDKGKKSG...  \n",
       "11894  MCALDRRERPLNSQSVNKYILNVQNIYRNSPVPVCVRNKNRKILYA...  \n",
       "11890  MRLRFSVPLFFFGCVFVHGVFAGPFPPPGMSLPEYWGEEHVWWDGR...  \n",
       "11902  MKTRFSLILSACLLSSSLFAKNTDDEITKLQKQLAQIQAELAQIRK...  \n",
       "\n",
       "[11906 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/graphpart_set.fasta') as handle:\n",
    "    fasta_cols = ['header', 'sequence']\n",
    "    df_data = pd.DataFrame.from_records([values for values in Bio.SeqIO.FastaIO.SimpleFastaParser(handle)], columns=fasta_cols)\n",
    "header_cols = ['uniprot_id', 'subcellular_location', 'organism_group', 'fold_id']\n",
    "df_data[header_cols] = df_data['header'].str.split('|', expand=True)\n",
    "final_cols = ['uniprot_id']\n",
    "df_data = df_data[['uniprot_id', 'subcellular_location', 'organism_group', 'fold_id', 'sequence']].astype({'fold_id': int}).sort_values('fold_id')\n",
    "# The data set has 11 906 items; paper assembles 11 970 sequences, but has to remove 64 proteins to avoid homologous sequences between different folds (GraphPart)\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniprot_id</th>\n",
       "      <th>subcellular_location</th>\n",
       "      <th>organism_group</th>\n",
       "      <th>fold_id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8560</th>\n",
       "      <td>Q8A0Z3</td>\n",
       "      <td>Cytoplasmic</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>MAVTMADITKLRKMTGAGMMDCKNALTEAEGDYDKAMEIIRKKGQA...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8568</th>\n",
       "      <td>Q8A2N8</td>\n",
       "      <td>Cytoplasmic</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>MIMSKETLIKSIREIPDFPIPGILFYDVTTLFKDPWCLQELSNIMF...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2593</th>\n",
       "      <td>P32709</td>\n",
       "      <td>CYtoplasmicMembrane</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>MTQTSAFHFESLVWDWPIAIYLFLIGISAGLVTLAVLLRRFYPQAG...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>E7FHF8</td>\n",
       "      <td>Cytoplasmic</td>\n",
       "      <td>archaea</td>\n",
       "      <td>0</td>\n",
       "      <td>MKLGVFELTDCGGCALNLLFLYDKLLDLLEFYEIAEFHMATSKKSR...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>E7FHU4</td>\n",
       "      <td>Cytoplasmic</td>\n",
       "      <td>archaea</td>\n",
       "      <td>0</td>\n",
       "      <td>MGKVRIGFYALTSCYGCQLQLAMMDELLQLIPNAEIVCWFMIDRDS...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5239</th>\n",
       "      <td>Q97F85</td>\n",
       "      <td>Cytoplasmic</td>\n",
       "      <td>positive</td>\n",
       "      <td>4</td>\n",
       "      <td>MRKLFTSESVTEGHPDKICDQISDAILDAILEKDPNGRVACETTVT...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5226</th>\n",
       "      <td>P33656</td>\n",
       "      <td>Cytoplasmic</td>\n",
       "      <td>positive</td>\n",
       "      <td>4</td>\n",
       "      <td>MKNKTEVKNGGEKKNSKKVSKEESAKEKNEKMKIVKNLIDKGKKSG...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11894</th>\n",
       "      <td>P13949</td>\n",
       "      <td>OuterMembrane</td>\n",
       "      <td>negative</td>\n",
       "      <td>4</td>\n",
       "      <td>MCALDRRERPLNSQSVNKYILNVQNIYRNSPVPVCVRNKNRKILYA...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11890</th>\n",
       "      <td>P42185</td>\n",
       "      <td>Extracellular</td>\n",
       "      <td>negative</td>\n",
       "      <td>4</td>\n",
       "      <td>MRLRFSVPLFFFGCVFVHGVFAGPFPPPGMSLPEYWGEEHVWWDGR...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11902</th>\n",
       "      <td>Q0P986</td>\n",
       "      <td>OuterMembrane</td>\n",
       "      <td>negative</td>\n",
       "      <td>4</td>\n",
       "      <td>MKTRFSLILSACLLSSSLFAKNTDDEITKLQKQLAQIQAELAQIRK...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11906 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      uniprot_id subcellular_location organism_group  fold_id  \\\n",
       "8560      Q8A0Z3          Cytoplasmic       negative        0   \n",
       "8568      Q8A2N8          Cytoplasmic       negative        0   \n",
       "2593      P32709  CYtoplasmicMembrane       negative        0   \n",
       "61        E7FHF8          Cytoplasmic        archaea        0   \n",
       "63        E7FHU4          Cytoplasmic        archaea        0   \n",
       "...          ...                  ...            ...      ...   \n",
       "5239      Q97F85          Cytoplasmic       positive        4   \n",
       "5226      P33656          Cytoplasmic       positive        4   \n",
       "11894     P13949        OuterMembrane       negative        4   \n",
       "11890     P42185        Extracellular       negative        4   \n",
       "11902     Q0P986        OuterMembrane       negative        4   \n",
       "\n",
       "                                                sequence  label  \n",
       "8560   MAVTMADITKLRKMTGAGMMDCKNALTEAEGDYDKAMEIIRKKGQA...      2  \n",
       "8568   MIMSKETLIKSIREIPDFPIPGILFYDVTTLFKDPWCLQELSNIMF...      2  \n",
       "2593   MTQTSAFHFESLVWDWPIAIYLFLIGISAGLVTLAVLLRRFYPQAG...      0  \n",
       "61     MKLGVFELTDCGGCALNLLFLYDKLLDLLEFYEIAEFHMATSKKSR...      2  \n",
       "63     MGKVRIGFYALTSCYGCQLQLAMMDELLQLIPNAEIVCWFMIDRDS...      2  \n",
       "...                                                  ...    ...  \n",
       "5239   MRKLFTSESVTEGHPDKICDQISDAILDAILEKDPNGRVACETTVT...      2  \n",
       "5226   MKNKTEVKNGGEKKNSKKVSKEESAKEKNEKMKIVKNLIDKGKKSG...      2  \n",
       "11894  MCALDRRERPLNSQSVNKYILNVQNIYRNSPVPVCVRNKNRKILYA...      4  \n",
       "11890  MRLRFSVPLFFFGCVFVHGVFAGPFPPPGMSLPEYWGEEHVWWDGR...      3  \n",
       "11902  MKTRFSLILSACLLSSSLFAKNTDDEITKLQKQLAQIQAELAQIRK...      4  \n",
       "\n",
       "[11906 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode subcellular location as numerical labels\n",
    "subcellular_location_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "subcellular_location_encoder.fit(df_data['subcellular_location'])\n",
    "df_data['label'] = subcellular_location_encoder.transform(df_data['subcellular_location'])\n",
    "df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8.** How were the data partitioned during training and evaluation in the paper? What does the code below do, and how does it compare to the approach taken in the paper?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7336 records in training data:\n",
      "subcellular_location\n",
      "Cytoplasmic            4345\n",
      "CYtoplasmicMembrane    1534\n",
      "Extracellular           645\n",
      "OuterMembrane           433\n",
      "Periplasmic             320\n",
      "Cellwall                 59\n",
      "Name: count, dtype: int64\n",
      "\n",
      "2579 records in eval data:\n",
      "subcellular_location\n",
      "Cytoplasmic            1568\n",
      "CYtoplasmicMembrane     476\n",
      "Extracellular           224\n",
      "OuterMembrane           159\n",
      "Periplasmic             136\n",
      "Cellwall                 16\n",
      "Name: count, dtype: int64\n",
      "\n",
      "1991 records in test data:\n",
      "subcellular_location\n",
      "Cytoplasmic            972\n",
      "CYtoplasmicMembrane    525\n",
      "Extracellular          208\n",
      "OuterMembrane          164\n",
      "Periplasmic            110\n",
      "Cellwall                12\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_id = {0, 1, 2}\n",
    "eval_id = {3}\n",
    "test_id = {4}\n",
    "\n",
    "df_train = df_data.query('fold_id in @train_id')#.groupby('subcellular_location').sample(n=50, random_state=random_number)\n",
    "df_eval = df_data.query('fold_id in @eval_id')\n",
    "df_test = df_data.query('fold_id in @test_id')\n",
    "print(len(df_train), 'records in training data:')\n",
    "print(df_train['subcellular_location'].value_counts())\n",
    "print()\n",
    "print(len(df_eval), 'records in eval data:')\n",
    "print(df_eval['subcellular_location'].value_counts())\n",
    "print()\n",
    "print(len(df_test), 'records in test data:')\n",
    "print(df_test['subcellular_location'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train/eval/test data sets for ESM2 model\n",
    "model_checkpoint = 'facebook/esm2_t6_8M_UR50D'\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "train_tokenized = tokenizer(df_train['sequence'].tolist(), truncation=True, max_length=1024)\n",
    "eval_tokenized = tokenizer(df_eval['sequence'].tolist(), truncation=True, max_length=1024)\n",
    "test_tokenized = tokenizer(df_test['sequence'].tolist(), truncation=True, max_length=1024)\n",
    "\n",
    "train_dataset = datasets.Dataset.from_dict(train_tokenized).add_column('labels', df_train['label'].tolist())\n",
    "eval_dataset = datasets.Dataset.from_dict(eval_tokenized).add_column('labels', df_eval['label'].tolist())\n",
    "test_dataset = datasets.Dataset.from_dict(test_tokenized).add_column('labels', df_test['label'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q9.** There's a warning about uninitialized weights when loading the ESM-2 model using `AutoModelForSequenceClassification` below. Describe the part of the network that has the uninitialized weights. How does the new part connect to the rest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09199cd7858845d9914f5516b32f1f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/107 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mEsmForSequenceClassification LOAD REPORT\u001b[0m from: facebook/esm2_t6_8M_UR50D\n",
      "Key                         | Status     | \n",
      "----------------------------+------------+-\n",
      "lm_head.dense.bias          | UNEXPECTED | \n",
      "lm_head.dense.weight        | UNEXPECTED | \n",
      "lm_head.bias                | UNEXPECTED | \n",
      "lm_head.layer_norm.weight   | UNEXPECTED | \n",
      "lm_head.layer_norm.bias     | UNEXPECTED | \n",
      "esm.embeddings.position_ids | UNEXPECTED | \n",
      "classifier.out_proj.weight  | MISSING    | \n",
      "classifier.dense.weight     | MISSING    | \n",
      "classifier.dense.bias       | MISSING    | \n",
      "classifier.out_proj.bias    | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EsmForSequenceClassification(\n",
       "  (esm): EsmModel(\n",
       "    (embeddings): EsmEmbeddings(\n",
       "      (word_embeddings): Embedding(33, 320, padding_idx=1)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): EsmEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x EsmLayer(\n",
       "          (attention): EsmAttention(\n",
       "            (self): EsmSelfAttention(\n",
       "              (query): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (key): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (value): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (rotary_embeddings): RotaryEmbedding()\n",
       "            )\n",
       "            (output): EsmSelfOutput(\n",
       "              (dense): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): EsmIntermediate(\n",
       "            (dense): Linear(in_features=320, out_features=1280, bias=True)\n",
       "          )\n",
       "          (output): EsmOutput(\n",
       "            (dense): Linear(in_features=1280, out_features=320, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (emb_layer_norm_after): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (contact_head): EsmContactPredictionHead(\n",
       "      (regression): Linear(in_features=120, out_features=1, bias=True)\n",
       "      (activation): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (classifier): EsmClassificationHead(\n",
       "    (dense): Linear(in_features=320, out_features=320, bias=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (out_proj): Linear(in_features=320, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uninitialised weights are in EsmClassificationHead which implements feed-forward network on the embeddings of the start-of-sequence/CLS token\n",
    "# https://github.com/huggingface/transformers/blob/main/src/transformers/models/esm/modeling_esm.py#L1239-L1246 \n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=df_data['label'].nunique())\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dc3f52da2354aeda8248e022778ff35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72f3858a169640988eaf1fadc3b9c6a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Track accuracy and macro F1 score throughout the training\n",
    "# https://huggingface.co/docs/transformers/en/training#evaluate\n",
    "metric_accuracy = evaluate.load('accuracy')\n",
    "metric_f1 = evaluate.load('f1')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        'accuracy': metric_accuracy.compute(predictions=predictions, references=labels)['accuracy'],\n",
    "        'f1_macro': metric_f1.compute(predictions=predictions, references=labels, average='macro')['f1'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q10.** The fine-tuning may fail by running out of GPU memory. Look up `per_device_train_batch_size` and `per_device_eval_batch_size` in the [TrainingArguments docs](https://huggingface.co/docs/transformers/v4.49.0/en/main_classes/trainer#transformers.TrainingArguments). How would you adjust these parameters to use less GPU memory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5502' max='5502' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5502/5502 07:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.495489</td>\n",
       "      <td>0.561619</td>\n",
       "      <td>0.852656</td>\n",
       "      <td>0.587045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.320575</td>\n",
       "      <td>0.515505</td>\n",
       "      <td>0.886778</td>\n",
       "      <td>0.678913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.237697</td>\n",
       "      <td>0.484380</td>\n",
       "      <td>0.897247</td>\n",
       "      <td>0.756088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732c230ba8d74d2a9f8826ce0f250ba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab8e1952dad4c438f7b59cd62a4ac98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eca2f301532436ebebf905e6d995b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['esm.encoder.layer.0.attention.LayerNorm.weight', 'esm.encoder.layer.0.attention.LayerNorm.bias', 'esm.encoder.layer.0.LayerNorm.weight', 'esm.encoder.layer.0.LayerNorm.bias', 'esm.encoder.layer.1.attention.LayerNorm.weight', 'esm.encoder.layer.1.attention.LayerNorm.bias', 'esm.encoder.layer.1.LayerNorm.weight', 'esm.encoder.layer.1.LayerNorm.bias', 'esm.encoder.layer.2.attention.LayerNorm.weight', 'esm.encoder.layer.2.attention.LayerNorm.bias', 'esm.encoder.layer.2.LayerNorm.weight', 'esm.encoder.layer.2.LayerNorm.bias', 'esm.encoder.layer.3.attention.LayerNorm.weight', 'esm.encoder.layer.3.attention.LayerNorm.bias', 'esm.encoder.layer.3.LayerNorm.weight', 'esm.encoder.layer.3.LayerNorm.bias', 'esm.encoder.layer.4.attention.LayerNorm.weight', 'esm.encoder.layer.4.attention.LayerNorm.bias', 'esm.encoder.layer.4.LayerNorm.weight', 'esm.encoder.layer.4.LayerNorm.bias', 'esm.encoder.layer.5.attention.LayerNorm.weight', 'esm.encoder.layer.5.attention.LayerNorm.bias', 'esm.encoder.layer.5.LayerNorm.weight', 'esm.encoder.layer.5.LayerNorm.bias'].\n",
      "There were unexpected keys in the checkpoint model loaded: ['esm.encoder.layer.0.attention.LayerNorm.beta', 'esm.encoder.layer.0.attention.LayerNorm.gamma', 'esm.encoder.layer.0.LayerNorm.beta', 'esm.encoder.layer.0.LayerNorm.gamma', 'esm.encoder.layer.1.attention.LayerNorm.beta', 'esm.encoder.layer.1.attention.LayerNorm.gamma', 'esm.encoder.layer.1.LayerNorm.beta', 'esm.encoder.layer.1.LayerNorm.gamma', 'esm.encoder.layer.2.attention.LayerNorm.beta', 'esm.encoder.layer.2.attention.LayerNorm.gamma', 'esm.encoder.layer.2.LayerNorm.beta', 'esm.encoder.layer.2.LayerNorm.gamma', 'esm.encoder.layer.3.attention.LayerNorm.beta', 'esm.encoder.layer.3.attention.LayerNorm.gamma', 'esm.encoder.layer.3.LayerNorm.beta', 'esm.encoder.layer.3.LayerNorm.gamma', 'esm.encoder.layer.4.attention.LayerNorm.beta', 'esm.encoder.layer.4.attention.LayerNorm.gamma', 'esm.encoder.layer.4.LayerNorm.beta', 'esm.encoder.layer.4.LayerNorm.gamma', 'esm.encoder.layer.5.attention.LayerNorm.beta', 'esm.encoder.layer.5.attention.LayerNorm.gamma', 'esm.encoder.layer.5.LayerNorm.beta', 'esm.encoder.layer.5.LayerNorm.gamma'].\n"
     ]
    }
   ],
   "source": [
    "# Set up fine-tuning\n",
    "trainer_args = transformers.TrainingArguments(\n",
    "    output_dir=f'{model_checkpoint}-subcellular_location',\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    "    # Reduce from default (8) to 4 to run on MOL GPUs\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model,\n",
    "    trainer_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "retrained = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.6742244362831116,\n",
       " 'test_accuracy': 0.8674033149171271,\n",
       " 'test_f1_macro': 0.7179987356034735,\n",
       " 'test_runtime': 10.1843,\n",
       " 'test_samples_per_second': 195.497,\n",
       " 'test_steps_per_second': 48.899}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use fine-tuned model to predict on held-out test data\n",
    "retrained_predict = trainer.predict(test_dataset=test_dataset)\n",
    "retrained_predict.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8674033149171271\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 2, 4, ..., 2, 5, 4], shape=(1991,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert probabilities into discrete predictions by taking the max probability\n",
    "test_labels = np.argmax(retrained_predict.predictions, axis=-1)\n",
    "# Sanity-check by manualy calculating the accuracy\n",
    "print(sum(test_labels == test_dataset['labels']) / len(test_dataset))\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q11.** Adjust the code below to re-produce the cross-validation results as in Table 2 of the paper. Fine-tune+test separately on every fold, and gather the results in `predicted_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2, 3, 4} 1 0 6668 2684 2554\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7779ec036cae488b90ea3b6d8c6f1b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/107 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mEsmForSequenceClassification LOAD REPORT\u001b[0m from: facebook/esm2_t6_8M_UR50D\n",
      "Key                         | Status     | \n",
      "----------------------------+------------+-\n",
      "lm_head.dense.bias          | UNEXPECTED | \n",
      "lm_head.dense.weight        | UNEXPECTED | \n",
      "lm_head.bias                | UNEXPECTED | \n",
      "lm_head.layer_norm.weight   | UNEXPECTED | \n",
      "lm_head.layer_norm.bias     | UNEXPECTED | \n",
      "esm.embeddings.position_ids | UNEXPECTED | \n",
      "classifier.out_proj.weight  | MISSING    | \n",
      "classifier.dense.weight     | MISSING    | \n",
      "classifier.dense.bias       | MISSING    | \n",
      "classifier.out_proj.bias    | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5001' max='5001' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5001/5001 06:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.475988</td>\n",
       "      <td>0.402221</td>\n",
       "      <td>0.903502</td>\n",
       "      <td>0.665130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.373211</td>\n",
       "      <td>0.443891</td>\n",
       "      <td>0.906855</td>\n",
       "      <td>0.675967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.247448</td>\n",
       "      <td>0.444569</td>\n",
       "      <td>0.904620</td>\n",
       "      <td>0.675866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1063b54e436e43df9828027a6ab89a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d4ca44aac1d4b3bb4c14c58095aeb7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c43ec4100f465e845af9430c731484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['esm.encoder.layer.0.attention.LayerNorm.weight', 'esm.encoder.layer.0.attention.LayerNorm.bias', 'esm.encoder.layer.0.LayerNorm.weight', 'esm.encoder.layer.0.LayerNorm.bias', 'esm.encoder.layer.1.attention.LayerNorm.weight', 'esm.encoder.layer.1.attention.LayerNorm.bias', 'esm.encoder.layer.1.LayerNorm.weight', 'esm.encoder.layer.1.LayerNorm.bias', 'esm.encoder.layer.2.attention.LayerNorm.weight', 'esm.encoder.layer.2.attention.LayerNorm.bias', 'esm.encoder.layer.2.LayerNorm.weight', 'esm.encoder.layer.2.LayerNorm.bias', 'esm.encoder.layer.3.attention.LayerNorm.weight', 'esm.encoder.layer.3.attention.LayerNorm.bias', 'esm.encoder.layer.3.LayerNorm.weight', 'esm.encoder.layer.3.LayerNorm.bias', 'esm.encoder.layer.4.attention.LayerNorm.weight', 'esm.encoder.layer.4.attention.LayerNorm.bias', 'esm.encoder.layer.4.LayerNorm.weight', 'esm.encoder.layer.4.LayerNorm.bias', 'esm.encoder.layer.5.attention.LayerNorm.weight', 'esm.encoder.layer.5.attention.LayerNorm.bias', 'esm.encoder.layer.5.LayerNorm.weight', 'esm.encoder.layer.5.LayerNorm.bias'].\n",
      "There were unexpected keys in the checkpoint model loaded: ['esm.encoder.layer.0.attention.LayerNorm.beta', 'esm.encoder.layer.0.attention.LayerNorm.gamma', 'esm.encoder.layer.0.LayerNorm.beta', 'esm.encoder.layer.0.LayerNorm.gamma', 'esm.encoder.layer.1.attention.LayerNorm.beta', 'esm.encoder.layer.1.attention.LayerNorm.gamma', 'esm.encoder.layer.1.LayerNorm.beta', 'esm.encoder.layer.1.LayerNorm.gamma', 'esm.encoder.layer.2.attention.LayerNorm.beta', 'esm.encoder.layer.2.attention.LayerNorm.gamma', 'esm.encoder.layer.2.LayerNorm.beta', 'esm.encoder.layer.2.LayerNorm.gamma', 'esm.encoder.layer.3.attention.LayerNorm.beta', 'esm.encoder.layer.3.attention.LayerNorm.gamma', 'esm.encoder.layer.3.LayerNorm.beta', 'esm.encoder.layer.3.LayerNorm.gamma', 'esm.encoder.layer.4.attention.LayerNorm.beta', 'esm.encoder.layer.4.attention.LayerNorm.gamma', 'esm.encoder.layer.4.LayerNorm.beta', 'esm.encoder.layer.4.LayerNorm.gamma', 'esm.encoder.layer.5.attention.LayerNorm.beta', 'esm.encoder.layer.5.attention.LayerNorm.gamma', 'esm.encoder.layer.5.LayerNorm.beta', 'esm.encoder.layer.5.LayerNorm.gamma'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 3, 4} 2 1 7124 2098 2684\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b883757802f47699b5978b33b4547c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/107 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mEsmForSequenceClassification LOAD REPORT\u001b[0m from: facebook/esm2_t6_8M_UR50D\n",
      "Key                         | Status     | \n",
      "----------------------------+------------+-\n",
      "lm_head.dense.bias          | UNEXPECTED | \n",
      "lm_head.dense.weight        | UNEXPECTED | \n",
      "lm_head.bias                | UNEXPECTED | \n",
      "lm_head.layer_norm.weight   | UNEXPECTED | \n",
      "lm_head.layer_norm.bias     | UNEXPECTED | \n",
      "esm.embeddings.position_ids | UNEXPECTED | \n",
      "classifier.out_proj.weight  | MISSING    | \n",
      "classifier.dense.weight     | MISSING    | \n",
      "classifier.dense.bias       | MISSING    | \n",
      "classifier.out_proj.bias    | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5343' max='5343' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5343/5343 06:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.472218</td>\n",
       "      <td>0.465843</td>\n",
       "      <td>0.882745</td>\n",
       "      <td>0.651349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.314894</td>\n",
       "      <td>0.508820</td>\n",
       "      <td>0.878932</td>\n",
       "      <td>0.703463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.242119</td>\n",
       "      <td>0.489318</td>\n",
       "      <td>0.894185</td>\n",
       "      <td>0.762663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d24bc2dcd247448ae5a24ae2ec853f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cbfe4603f714f549375dde36ea5cb71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d13acd9504894e97886e649e2fc41c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['esm.encoder.layer.0.attention.LayerNorm.weight', 'esm.encoder.layer.0.attention.LayerNorm.bias', 'esm.encoder.layer.0.LayerNorm.weight', 'esm.encoder.layer.0.LayerNorm.bias', 'esm.encoder.layer.1.attention.LayerNorm.weight', 'esm.encoder.layer.1.attention.LayerNorm.bias', 'esm.encoder.layer.1.LayerNorm.weight', 'esm.encoder.layer.1.LayerNorm.bias', 'esm.encoder.layer.2.attention.LayerNorm.weight', 'esm.encoder.layer.2.attention.LayerNorm.bias', 'esm.encoder.layer.2.LayerNorm.weight', 'esm.encoder.layer.2.LayerNorm.bias', 'esm.encoder.layer.3.attention.LayerNorm.weight', 'esm.encoder.layer.3.attention.LayerNorm.bias', 'esm.encoder.layer.3.LayerNorm.weight', 'esm.encoder.layer.3.LayerNorm.bias', 'esm.encoder.layer.4.attention.LayerNorm.weight', 'esm.encoder.layer.4.attention.LayerNorm.bias', 'esm.encoder.layer.4.LayerNorm.weight', 'esm.encoder.layer.4.LayerNorm.bias', 'esm.encoder.layer.5.attention.LayerNorm.weight', 'esm.encoder.layer.5.attention.LayerNorm.bias', 'esm.encoder.layer.5.LayerNorm.weight', 'esm.encoder.layer.5.LayerNorm.bias'].\n",
      "There were unexpected keys in the checkpoint model loaded: ['esm.encoder.layer.0.attention.LayerNorm.beta', 'esm.encoder.layer.0.attention.LayerNorm.gamma', 'esm.encoder.layer.0.LayerNorm.beta', 'esm.encoder.layer.0.LayerNorm.gamma', 'esm.encoder.layer.1.attention.LayerNorm.beta', 'esm.encoder.layer.1.attention.LayerNorm.gamma', 'esm.encoder.layer.1.LayerNorm.beta', 'esm.encoder.layer.1.LayerNorm.gamma', 'esm.encoder.layer.2.attention.LayerNorm.beta', 'esm.encoder.layer.2.attention.LayerNorm.gamma', 'esm.encoder.layer.2.LayerNorm.beta', 'esm.encoder.layer.2.LayerNorm.gamma', 'esm.encoder.layer.3.attention.LayerNorm.beta', 'esm.encoder.layer.3.attention.LayerNorm.gamma', 'esm.encoder.layer.3.LayerNorm.beta', 'esm.encoder.layer.3.LayerNorm.gamma', 'esm.encoder.layer.4.attention.LayerNorm.beta', 'esm.encoder.layer.4.attention.LayerNorm.gamma', 'esm.encoder.layer.4.LayerNorm.beta', 'esm.encoder.layer.4.LayerNorm.gamma', 'esm.encoder.layer.5.attention.LayerNorm.beta', 'esm.encoder.layer.5.attention.LayerNorm.gamma', 'esm.encoder.layer.5.LayerNorm.beta', 'esm.encoder.layer.5.LayerNorm.gamma'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 4} 3 2 7229 2579 2098\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1688ab2906f41969f8f5a3f286a2003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/107 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mEsmForSequenceClassification LOAD REPORT\u001b[0m from: facebook/esm2_t6_8M_UR50D\n",
      "Key                         | Status     | \n",
      "----------------------------+------------+-\n",
      "lm_head.dense.bias          | UNEXPECTED | \n",
      "lm_head.dense.weight        | UNEXPECTED | \n",
      "lm_head.bias                | UNEXPECTED | \n",
      "lm_head.layer_norm.weight   | UNEXPECTED | \n",
      "lm_head.layer_norm.bias     | UNEXPECTED | \n",
      "esm.embeddings.position_ids | UNEXPECTED | \n",
      "classifier.out_proj.weight  | MISSING    | \n",
      "classifier.dense.weight     | MISSING    | \n",
      "classifier.dense.bias       | MISSING    | \n",
      "classifier.out_proj.bias    | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5424' max='5424' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5424/5424 07:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.462430</td>\n",
       "      <td>0.527369</td>\n",
       "      <td>0.869329</td>\n",
       "      <td>0.634203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.323035</td>\n",
       "      <td>0.496199</td>\n",
       "      <td>0.886002</td>\n",
       "      <td>0.652925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.254737</td>\n",
       "      <td>0.522164</td>\n",
       "      <td>0.884451</td>\n",
       "      <td>0.664897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4d9ae2f8acb4b63b9833aaa097ae0cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ea9270c1574c11bd46b9af6984c6a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b4d459fb5a84e75afbd5e8e57630f18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['esm.encoder.layer.0.attention.LayerNorm.weight', 'esm.encoder.layer.0.attention.LayerNorm.bias', 'esm.encoder.layer.0.LayerNorm.weight', 'esm.encoder.layer.0.LayerNorm.bias', 'esm.encoder.layer.1.attention.LayerNorm.weight', 'esm.encoder.layer.1.attention.LayerNorm.bias', 'esm.encoder.layer.1.LayerNorm.weight', 'esm.encoder.layer.1.LayerNorm.bias', 'esm.encoder.layer.2.attention.LayerNorm.weight', 'esm.encoder.layer.2.attention.LayerNorm.bias', 'esm.encoder.layer.2.LayerNorm.weight', 'esm.encoder.layer.2.LayerNorm.bias', 'esm.encoder.layer.3.attention.LayerNorm.weight', 'esm.encoder.layer.3.attention.LayerNorm.bias', 'esm.encoder.layer.3.LayerNorm.weight', 'esm.encoder.layer.3.LayerNorm.bias', 'esm.encoder.layer.4.attention.LayerNorm.weight', 'esm.encoder.layer.4.attention.LayerNorm.bias', 'esm.encoder.layer.4.LayerNorm.weight', 'esm.encoder.layer.4.LayerNorm.bias', 'esm.encoder.layer.5.attention.LayerNorm.weight', 'esm.encoder.layer.5.attention.LayerNorm.bias', 'esm.encoder.layer.5.LayerNorm.weight', 'esm.encoder.layer.5.LayerNorm.bias'].\n",
      "There were unexpected keys in the checkpoint model loaded: ['esm.encoder.layer.0.attention.LayerNorm.beta', 'esm.encoder.layer.0.attention.LayerNorm.gamma', 'esm.encoder.layer.0.LayerNorm.beta', 'esm.encoder.layer.0.LayerNorm.gamma', 'esm.encoder.layer.1.attention.LayerNorm.beta', 'esm.encoder.layer.1.attention.LayerNorm.gamma', 'esm.encoder.layer.1.LayerNorm.beta', 'esm.encoder.layer.1.LayerNorm.gamma', 'esm.encoder.layer.2.attention.LayerNorm.beta', 'esm.encoder.layer.2.attention.LayerNorm.gamma', 'esm.encoder.layer.2.LayerNorm.beta', 'esm.encoder.layer.2.LayerNorm.gamma', 'esm.encoder.layer.3.attention.LayerNorm.beta', 'esm.encoder.layer.3.attention.LayerNorm.gamma', 'esm.encoder.layer.3.LayerNorm.beta', 'esm.encoder.layer.3.LayerNorm.gamma', 'esm.encoder.layer.4.attention.LayerNorm.beta', 'esm.encoder.layer.4.attention.LayerNorm.gamma', 'esm.encoder.layer.4.LayerNorm.beta', 'esm.encoder.layer.4.LayerNorm.gamma', 'esm.encoder.layer.5.attention.LayerNorm.beta', 'esm.encoder.layer.5.attention.LayerNorm.gamma', 'esm.encoder.layer.5.LayerNorm.beta', 'esm.encoder.layer.5.LayerNorm.gamma'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2} 4 3 7336 1991 2579\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aec665c71d04e8090a32f2049c3ce39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/107 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mEsmForSequenceClassification LOAD REPORT\u001b[0m from: facebook/esm2_t6_8M_UR50D\n",
      "Key                         | Status     | \n",
      "----------------------------+------------+-\n",
      "lm_head.dense.bias          | UNEXPECTED | \n",
      "lm_head.dense.weight        | UNEXPECTED | \n",
      "lm_head.bias                | UNEXPECTED | \n",
      "lm_head.layer_norm.weight   | UNEXPECTED | \n",
      "lm_head.layer_norm.bias     | UNEXPECTED | \n",
      "esm.embeddings.position_ids | UNEXPECTED | \n",
      "classifier.out_proj.weight  | MISSING    | \n",
      "classifier.dense.weight     | MISSING    | \n",
      "classifier.dense.bias       | MISSING    | \n",
      "classifier.out_proj.bias    | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5502' max='5502' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5502/5502 07:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.480130</td>\n",
       "      <td>0.625098</td>\n",
       "      <td>0.842290</td>\n",
       "      <td>0.635833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.316636</td>\n",
       "      <td>0.635441</td>\n",
       "      <td>0.866901</td>\n",
       "      <td>0.697758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.224002</td>\n",
       "      <td>0.639548</td>\n",
       "      <td>0.873933</td>\n",
       "      <td>0.750461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f933f6561d94aaaa25df36fe224e37d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c878d8498643ceb0cb7f0b8d813185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8957610f50547cba09a55660e9b5009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['esm.encoder.layer.0.attention.LayerNorm.weight', 'esm.encoder.layer.0.attention.LayerNorm.bias', 'esm.encoder.layer.0.LayerNorm.weight', 'esm.encoder.layer.0.LayerNorm.bias', 'esm.encoder.layer.1.attention.LayerNorm.weight', 'esm.encoder.layer.1.attention.LayerNorm.bias', 'esm.encoder.layer.1.LayerNorm.weight', 'esm.encoder.layer.1.LayerNorm.bias', 'esm.encoder.layer.2.attention.LayerNorm.weight', 'esm.encoder.layer.2.attention.LayerNorm.bias', 'esm.encoder.layer.2.LayerNorm.weight', 'esm.encoder.layer.2.LayerNorm.bias', 'esm.encoder.layer.3.attention.LayerNorm.weight', 'esm.encoder.layer.3.attention.LayerNorm.bias', 'esm.encoder.layer.3.LayerNorm.weight', 'esm.encoder.layer.3.LayerNorm.bias', 'esm.encoder.layer.4.attention.LayerNorm.weight', 'esm.encoder.layer.4.attention.LayerNorm.bias', 'esm.encoder.layer.4.LayerNorm.weight', 'esm.encoder.layer.4.LayerNorm.bias', 'esm.encoder.layer.5.attention.LayerNorm.weight', 'esm.encoder.layer.5.attention.LayerNorm.bias', 'esm.encoder.layer.5.LayerNorm.weight', 'esm.encoder.layer.5.LayerNorm.bias'].\n",
      "There were unexpected keys in the checkpoint model loaded: ['esm.encoder.layer.0.attention.LayerNorm.beta', 'esm.encoder.layer.0.attention.LayerNorm.gamma', 'esm.encoder.layer.0.LayerNorm.beta', 'esm.encoder.layer.0.LayerNorm.gamma', 'esm.encoder.layer.1.attention.LayerNorm.beta', 'esm.encoder.layer.1.attention.LayerNorm.gamma', 'esm.encoder.layer.1.LayerNorm.beta', 'esm.encoder.layer.1.LayerNorm.gamma', 'esm.encoder.layer.2.attention.LayerNorm.beta', 'esm.encoder.layer.2.attention.LayerNorm.gamma', 'esm.encoder.layer.2.LayerNorm.beta', 'esm.encoder.layer.2.LayerNorm.gamma', 'esm.encoder.layer.3.attention.LayerNorm.beta', 'esm.encoder.layer.3.attention.LayerNorm.gamma', 'esm.encoder.layer.3.LayerNorm.beta', 'esm.encoder.layer.3.LayerNorm.gamma', 'esm.encoder.layer.4.attention.LayerNorm.beta', 'esm.encoder.layer.4.attention.LayerNorm.gamma', 'esm.encoder.layer.4.LayerNorm.beta', 'esm.encoder.layer.4.LayerNorm.gamma', 'esm.encoder.layer.5.attention.LayerNorm.beta', 'esm.encoder.layer.5.attention.LayerNorm.gamma', 'esm.encoder.layer.5.LayerNorm.beta', 'esm.encoder.layer.5.LayerNorm.gamma'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1, 2, 3} 0 4 7361 2554 1991\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00694f4c68e1433eacf34ee5a5aa4e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/107 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mEsmForSequenceClassification LOAD REPORT\u001b[0m from: facebook/esm2_t6_8M_UR50D\n",
      "Key                         | Status     | \n",
      "----------------------------+------------+-\n",
      "lm_head.dense.bias          | UNEXPECTED | \n",
      "lm_head.dense.weight        | UNEXPECTED | \n",
      "lm_head.bias                | UNEXPECTED | \n",
      "lm_head.layer_norm.weight   | UNEXPECTED | \n",
      "lm_head.layer_norm.bias     | UNEXPECTED | \n",
      "esm.embeddings.position_ids | UNEXPECTED | \n",
      "classifier.out_proj.weight  | MISSING    | \n",
      "classifier.dense.weight     | MISSING    | \n",
      "classifier.dense.bias       | MISSING    | \n",
      "classifier.out_proj.bias    | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5523' max='5523' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5523/5523 07:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.485785</td>\n",
       "      <td>0.442032</td>\n",
       "      <td>0.886061</td>\n",
       "      <td>0.644352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.311491</td>\n",
       "      <td>0.511591</td>\n",
       "      <td>0.888019</td>\n",
       "      <td>0.681165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.215487</td>\n",
       "      <td>0.500322</td>\n",
       "      <td>0.898199</td>\n",
       "      <td>0.732979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a4337618d6c42ef8774a199cf515d68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "093abab02fed4a05ac121ed42581a330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e606ca30fb411db2cc328387bfff23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['esm.encoder.layer.0.attention.LayerNorm.weight', 'esm.encoder.layer.0.attention.LayerNorm.bias', 'esm.encoder.layer.0.LayerNorm.weight', 'esm.encoder.layer.0.LayerNorm.bias', 'esm.encoder.layer.1.attention.LayerNorm.weight', 'esm.encoder.layer.1.attention.LayerNorm.bias', 'esm.encoder.layer.1.LayerNorm.weight', 'esm.encoder.layer.1.LayerNorm.bias', 'esm.encoder.layer.2.attention.LayerNorm.weight', 'esm.encoder.layer.2.attention.LayerNorm.bias', 'esm.encoder.layer.2.LayerNorm.weight', 'esm.encoder.layer.2.LayerNorm.bias', 'esm.encoder.layer.3.attention.LayerNorm.weight', 'esm.encoder.layer.3.attention.LayerNorm.bias', 'esm.encoder.layer.3.LayerNorm.weight', 'esm.encoder.layer.3.LayerNorm.bias', 'esm.encoder.layer.4.attention.LayerNorm.weight', 'esm.encoder.layer.4.attention.LayerNorm.bias', 'esm.encoder.layer.4.LayerNorm.weight', 'esm.encoder.layer.4.LayerNorm.bias', 'esm.encoder.layer.5.attention.LayerNorm.weight', 'esm.encoder.layer.5.attention.LayerNorm.bias', 'esm.encoder.layer.5.LayerNorm.weight', 'esm.encoder.layer.5.LayerNorm.bias'].\n",
      "There were unexpected keys in the checkpoint model loaded: ['esm.encoder.layer.0.attention.LayerNorm.beta', 'esm.encoder.layer.0.attention.LayerNorm.gamma', 'esm.encoder.layer.0.LayerNorm.beta', 'esm.encoder.layer.0.LayerNorm.gamma', 'esm.encoder.layer.1.attention.LayerNorm.beta', 'esm.encoder.layer.1.attention.LayerNorm.gamma', 'esm.encoder.layer.1.LayerNorm.beta', 'esm.encoder.layer.1.LayerNorm.gamma', 'esm.encoder.layer.2.attention.LayerNorm.beta', 'esm.encoder.layer.2.attention.LayerNorm.gamma', 'esm.encoder.layer.2.LayerNorm.beta', 'esm.encoder.layer.2.LayerNorm.gamma', 'esm.encoder.layer.3.attention.LayerNorm.beta', 'esm.encoder.layer.3.attention.LayerNorm.gamma', 'esm.encoder.layer.3.LayerNorm.beta', 'esm.encoder.layer.3.LayerNorm.gamma', 'esm.encoder.layer.4.attention.LayerNorm.beta', 'esm.encoder.layer.4.attention.LayerNorm.gamma', 'esm.encoder.layer.4.LayerNorm.beta', 'esm.encoder.layer.4.LayerNorm.gamma', 'esm.encoder.layer.5.attention.LayerNorm.beta', 'esm.encoder.layer.5.attention.LayerNorm.gamma', 'esm.encoder.layer.5.LayerNorm.beta', 'esm.encoder.layer.5.LayerNorm.gamma'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fold_id = set(df_data.fold_id)\n",
    "predicted_labels = []\n",
    "for test_id in sorted(fold_id):\n",
    "    eval_id = (test_id + 1) % 5\n",
    "    train_id = fold_id - set([eval_id, test_id])\n",
    "\n",
    "    df_train = df_data.query('fold_id in @train_id')#.groupby('subcellular_location').sample(n=10, random_state=random_number)\n",
    "    df_eval = df_data.query('fold_id == @eval_id')\n",
    "    df_test = df_data.query('fold_id == @test_id')\n",
    "    print(train_id, eval_id, test_id, len(df_train), len(df_eval), len(df_test))\n",
    "\n",
    "    # Re-train model using train/eval/test as defined abobe\n",
    "    model = transformers.AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=df_data['label'].nunique())\n",
    "\n",
    "    # Prepare train/eval/test data sets for ESM2 model\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "    train_tokenized = tokenizer(df_train['sequence'].tolist(), truncation=True, max_length=1024)\n",
    "    eval_tokenized = tokenizer(df_eval['sequence'].tolist(), truncation=True, max_length=1024)\n",
    "    test_tokenized = tokenizer(df_test['sequence'].tolist(), truncation=True, max_length=1024)\n",
    "\n",
    "    train_dataset = datasets.Dataset.from_dict(train_tokenized).add_column('labels', df_train['label'].tolist())\n",
    "    eval_dataset = datasets.Dataset.from_dict(eval_tokenized).add_column('labels', df_eval['label'].tolist())\n",
    "    test_dataset = datasets.Dataset.from_dict(test_tokenized).add_column('labels', df_test['label'].tolist())\n",
    "\n",
    "    # Set up fine-tuning\n",
    "    trainer_args = transformers.TrainingArguments(\n",
    "        output_dir=f'{model_checkpoint}-subcellular_location-{test_id}',\n",
    "        eval_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='accuracy',\n",
    "        # Adjust if needed\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "    )\n",
    "\n",
    "    trainer = transformers.Trainer(\n",
    "        model,\n",
    "        trainer_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        processing_class=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    retrained = trainer.train()\n",
    "\n",
    "    # Predict labels, and gather the predictions for the held-out test data into predicted_labels\n",
    "    retrained_predict = trainer.predict(test_dataset=test_dataset)\n",
    "    predicted_labels += list(np.argmax(retrained_predict.predictions, axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q12.** Discuss differences between the methodology and the resulting performance of the approach taken in the paper, and the reproduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Overall</th>\n",
       "      <th>Archaea</th>\n",
       "      <th>Gram pos</th>\n",
       "      <th>Gram neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>size</th>\n",
       "      <td>11906</td>\n",
       "      <td>283</td>\n",
       "      <td>3206</td>\n",
       "      <td>8417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_macro</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Overall Archaea Gram pos Gram neg\n",
       "size       11906     283     3206     8417\n",
       "accuracy    0.89    0.85     0.92     0.89\n",
       "f1_macro    0.71    0.45     0.49     0.70"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show table with performance metrics split by organism to match Table 2\n",
    "def calculate_stats_(df):\n",
    "    accuracy = metric_accuracy.compute(predictions=df.predicted_labels.values, references=df.label.values)['accuracy']\n",
    "    f1_macro = metric_f1.compute(predictions=df.predicted_labels.values, references=df.label.values, average='macro')['f1']\n",
    "    return pd.Series({\n",
    "        'size': '{:d}'.format(len(df)),\n",
    "        'accuracy': '{:.2f}'.format(accuracy),\n",
    "        'f1_macro': '{:.2f}'.format(f1_macro),\n",
    "    })\n",
    "\n",
    "# Paper vs reproduction:\n",
    "# 650M vs 8M parameters\n",
    "# 60 vs 3 epochs\n",
    "# optimise hyperparameters (learning rate, batch size, dropout rate) vs use defaults\n",
    "# ??? vs 30 min training time\n",
    "# 0.92 vs 0.89 accuracy (overall)\n",
    "# 0.80 vs 0.73 F1 score (overall)\n",
    "df_data['predicted_labels'] = predicted_labels\n",
    "pd.concat([\n",
    "    calculate_stats_(df_data),\n",
    "    calculate_stats_(df_data.query('organism_group == \"archaea\"')),\n",
    "    calculate_stats_(df_data.query('organism_group == \"positive\"')),\n",
    "    calculate_stats_(df_data.query('organism_group == \"negative\"')),\n",
    "], axis=1).set_axis(['Overall', 'Archaea' , 'Gram pos', 'Gram neg'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
