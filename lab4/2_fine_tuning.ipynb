{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning protein language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment & execute once to download data\n",
    "# https://services.healthtech.dtu.dk/services/DeepLocPro-1.0/\n",
    "#!mkdir -p data\n",
    "#!curl https://services.healthtech.dtu.dk/services/DeepLocPro-1.0/data/graphpart_set.fasta -o data/graphpart_set.fasta\n",
    "#!curl https://services.healthtech.dtu.dk/services/DeepLocPro-1.0/data/benchmarking_dataset.fasta -o data/benchmarking_dataset.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. Describe the problem of predicting the subcellular location of (prokaryotic) proteins as described in Moreno2024 (https://doi.org/10.1101/2024.01.04.574157)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, sklearn.preprocessing\n",
    "import Bio.SeqIO.FastaIO # Biopython for reading fasta files\n",
    "import datasets, evaluate, transformers # Hugging Face libraries https://doi.org/10.18653/v1/2020.emnlp-demos.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. How were the training/benchmark data sets constructed? How were the cross-validation folds defined?\n",
    "def read_DeepLocPro(file, columns=None):\n",
    "    with open(file) as handle:\n",
    "        fasta_cols = ['header', 'sequence']\n",
    "        df = pd.DataFrame.from_records([values for values in Bio.SeqIO.FastaIO.SimpleFastaParser(handle)], columns=fasta_cols)\n",
    "    if columns is None:\n",
    "        return df\n",
    "    else:\n",
    "        df[columns] = df['header'].str.split('|', expand=True)\n",
    "        return df[columns + ['sequence']]\n",
    "\n",
    "columns = ['uniprot_id', 'subcellular_location', 'organism_group']\n",
    "df_graphpart = read_DeepLocPro('data/graphpart_set.fasta', columns=columns + ['fold_id'])\n",
    "df_benchmarking = read_DeepLocPro('data/benchmarking_dataset.fasta', columns=columns)\n",
    "df_graphpart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subcellular_location_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "subcellular_location_encoder.fit(df_graphpart['subcellular_location'])\n",
    "for df in df_benchmarking, df_graphpart:\n",
    "    df['label'] = subcellular_location_encoder.transform(df['subcellular_location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. How were the training/benchmark data sets constructed? How were the cross-validation folds defined?\n",
    "# Subsample training/eval data from the homology-partitioned sequences in the preprint\n",
    "random_number = 4 # https://xkcd.com/221/\n",
    "train_query = 'fold_id == \"1\" or fold_id == \"2\" or fold_id == \"3\" or fold_id == \"4\"'\n",
    "df_train = df_graphpart.query(train_query).groupby('subcellular_location').sample(n=10, random_state=random_number)\n",
    "df_eval = df_graphpart.query(f'~({train_query})').groupby('subcellular_location').sample(n=10, random_state=random_number)\n",
    "print(len(df_train), 'records in training data:')\n",
    "print(df_train['subcellular_location'].value_counts())\n",
    "print()\n",
    "print(len(df_eval), 'records in eval data:')\n",
    "print(df_eval['subcellular_location'].value_counts())\n",
    "print()\n",
    "print(len(df_benchmarking), 'records in benchmarking data:')\n",
    "print(df_benchmarking['subcellular_location'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'facebook/esm2_t6_8M_UR50D'\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = tokenizer(df_train['sequence'].tolist(), truncation=True, max_length=1024)\n",
    "eval_tokenized = tokenizer(df_eval['sequence'].tolist(), truncation=True, max_length=1024)\n",
    "benchmarking_tokenized = tokenizer(df_benchmarking['sequence'].tolist(), truncation=True, max_length=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.Dataset.from_dict(train_tokenized).add_column('labels', df_train['label'].tolist())\n",
    "eval_dataset = datasets.Dataset.from_dict(eval_tokenized).add_column('labels', df_eval['label'].tolist())\n",
    "benchmarking_dataset = datasets.Dataset.from_dict(benchmarking_tokenized).add_column('labels', df_benchmarking['label'].tolist())\n",
    "benchmarking_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. Describe the difference between EsmModel, and EsmForSequenceClassification?\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=df_benchmarking['label'].nunique())\n",
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_esm = transformers.EsmModel.from_pretrained(model_checkpoint)\n",
    "#model_esm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = model_checkpoint.split('/')[-1]\n",
    "\n",
    "args = transformers.TrainingArguments(\n",
    "    #f'{model_name}-subcellular_location',\n",
    "    output_dir='esm2_subcellular_location',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    "    #per_device_train_batch_size=4,\n",
    "    #per_device_eval_batch_size=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The paper uses accuracy and macro F1 score to characterise the performance; we will trace both throughout the training\n",
    "metric_accuracy = evaluate.load('accuracy')\n",
    "metric_f1 = evaluate.load('f1')\n",
    "\n",
    "def compute_metrics(eval_pred): # https://huggingface.co/docs/transformers/en/training#evaluate\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        'accuracy': metric_accuracy.compute(predictions=predictions, references=labels)['accuracy'],\n",
    "        'f1_macro': metric_f1.compute(predictions=predictions, references=labels, average='macro')['f1'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now fine-tune the network, reporting the performance at the end of every epoch\n",
    "retrained = trainer.train()\n",
    "retrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9. How did the parameters change during re-training? Compare (a subset) of weights in the (retrained) model to model_esm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We evaluate the fine-tuned model on the benchmark data set (globally)\n",
    "# How does the fine-tuned model compare to DeepLocPro as reported in Table 3 of the preprint?\n",
    "trainer.evaluate(eval_dataset=benchmarking_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll take a closer look at the individual predictions\n",
    "benchmarking_predictions = trainer.predict(test_dataset=benchmarking_dataset)\n",
    "df_benchmarking['label_predicted'] = np.argmax(benchmarking_predictions.predictions, axis=-1)\n",
    "print(len(df_benchmarking.query('label == label_predicted')))\n",
    "df_benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show table with performance metrics split by organism to match Table 3 in preprint\n",
    "def apply_(df):\n",
    "    return pd.Series({\n",
    "        'size': len(df),\n",
    "        'accuracy': metric_accuracy.compute(predictions=df.label_predicted.values, references=df.label.values)['accuracy'],\n",
    "        'f1_macro': metric_f1.compute(predictions=df.label_predicted.values, references=df.label.values, average='macro')['f1'],\n",
    "    })\n",
    "\n",
    "# Q10. Re-train on the whole data; compare to DeepLoc Pro\n",
    "print(df_benchmarking.groupby('organism_group').apply(apply_).transpose()[['archaea', 'positive', 'negative']].to_string(float_format='%.2f'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
