{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning protein language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment & execute once to download data\n",
    "# https://services.healthtech.dtu.dk/services/DeepLocPro-1.0/\n",
    "#!mkdir -p data\n",
    "#!curl https://services.healthtech.dtu.dk/services/DeepLocPro-1.0/data/graphpart_set.fasta -o data/graphpart_set.fasta\n",
    "#!curl https://services.healthtech.dtu.dk/services/DeepLocPro-1.0/data/benchmarking_dataset.fasta -o data/benchmarking_dataset.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. Describe the problem of predicting the subcellular location of (prokaryotic) proteins as described in Moreno2024 (https://doi.org/10.1101/2024.01.04.574157)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/course/anaconda3/lib/python3.11/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/course/anaconda3/lib/python3.11/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, sklearn.preprocessing\n",
    "import Bio.SeqIO.FastaIO # Biopython for reading fasta files\n",
    "import datasets, evaluate, transformers # Hugging Face libraries https://doi.org/10.18653/v1/2020.emnlp-demos.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniprot_id</th>\n",
       "      <th>subcellular_location</th>\n",
       "      <th>organism_group</th>\n",
       "      <th>fold_id</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q8A0Z3</td>\n",
       "      <td>Cytoplasmic</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>MAVTMADITKLRKMTGAGMMDCKNALTEAEGDYDKAMEIIRKKGQA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q8A2N8</td>\n",
       "      <td>Cytoplasmic</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>MIMSKETLIKSIREIPDFPIPGILFYDVTTLFKDPWCLQELSNIMF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P32709</td>\n",
       "      <td>CYtoplasmicMembrane</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>MTQTSAFHFESLVWDWPIAIYLFLIGISAGLVTLAVLLRRFYPQAG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E7FHF8</td>\n",
       "      <td>Cytoplasmic</td>\n",
       "      <td>archaea</td>\n",
       "      <td>0</td>\n",
       "      <td>MKLGVFELTDCGGCALNLLFLYDKLLDLLEFYEIAEFHMATSKKSR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E7FHU4</td>\n",
       "      <td>Cytoplasmic</td>\n",
       "      <td>archaea</td>\n",
       "      <td>0</td>\n",
       "      <td>MGKVRIGFYALTSCYGCQLQLAMMDELLQLIPNAEIVCWFMIDRDS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11901</th>\n",
       "      <td>Q97F85</td>\n",
       "      <td>Cytoplasmic</td>\n",
       "      <td>positive</td>\n",
       "      <td>4</td>\n",
       "      <td>MRKLFTSESVTEGHPDKICDQISDAILDAILEKDPNGRVACETTVT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11902</th>\n",
       "      <td>P33656</td>\n",
       "      <td>Cytoplasmic</td>\n",
       "      <td>positive</td>\n",
       "      <td>4</td>\n",
       "      <td>MKNKTEVKNGGEKKNSKKVSKEESAKEKNEKMKIVKNLIDKGKKSG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11903</th>\n",
       "      <td>P13949</td>\n",
       "      <td>OuterMembrane</td>\n",
       "      <td>negative</td>\n",
       "      <td>4</td>\n",
       "      <td>MCALDRRERPLNSQSVNKYILNVQNIYRNSPVPVCVRNKNRKILYA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11904</th>\n",
       "      <td>P42185</td>\n",
       "      <td>Extracellular</td>\n",
       "      <td>negative</td>\n",
       "      <td>4</td>\n",
       "      <td>MRLRFSVPLFFFGCVFVHGVFAGPFPPPGMSLPEYWGEEHVWWDGR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11905</th>\n",
       "      <td>Q0P986</td>\n",
       "      <td>OuterMembrane</td>\n",
       "      <td>negative</td>\n",
       "      <td>4</td>\n",
       "      <td>MKTRFSLILSACLLSSSLFAKNTDDEITKLQKQLAQIQAELAQIRK...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11906 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      uniprot_id subcellular_location organism_group  fold_id  \\\n",
       "0         Q8A0Z3          Cytoplasmic       negative        0   \n",
       "1         Q8A2N8          Cytoplasmic       negative        0   \n",
       "2         P32709  CYtoplasmicMembrane       negative        0   \n",
       "3         E7FHF8          Cytoplasmic        archaea        0   \n",
       "4         E7FHU4          Cytoplasmic        archaea        0   \n",
       "...          ...                  ...            ...      ...   \n",
       "11901     Q97F85          Cytoplasmic       positive        4   \n",
       "11902     P33656          Cytoplasmic       positive        4   \n",
       "11903     P13949        OuterMembrane       negative        4   \n",
       "11904     P42185        Extracellular       negative        4   \n",
       "11905     Q0P986        OuterMembrane       negative        4   \n",
       "\n",
       "                                                sequence  \n",
       "0      MAVTMADITKLRKMTGAGMMDCKNALTEAEGDYDKAMEIIRKKGQA...  \n",
       "1      MIMSKETLIKSIREIPDFPIPGILFYDVTTLFKDPWCLQELSNIMF...  \n",
       "2      MTQTSAFHFESLVWDWPIAIYLFLIGISAGLVTLAVLLRRFYPQAG...  \n",
       "3      MKLGVFELTDCGGCALNLLFLYDKLLDLLEFYEIAEFHMATSKKSR...  \n",
       "4      MGKVRIGFYALTSCYGCQLQLAMMDELLQLIPNAEIVCWFMIDRDS...  \n",
       "...                                                  ...  \n",
       "11901  MRKLFTSESVTEGHPDKICDQISDAILDAILEKDPNGRVACETTVT...  \n",
       "11902  MKNKTEVKNGGEKKNSKKVSKEESAKEKNEKMKIVKNLIDKGKKSG...  \n",
       "11903  MCALDRRERPLNSQSVNKYILNVQNIYRNSPVPVCVRNKNRKILYA...  \n",
       "11904  MRLRFSVPLFFFGCVFVHGVFAGPFPPPGMSLPEYWGEEHVWWDGR...  \n",
       "11905  MKTRFSLILSACLLSSSLFAKNTDDEITKLQKQLAQIQAELAQIRK...  \n",
       "\n",
       "[11906 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q7. How were the training/benchmark data sets constructed? How were the cross-validation folds defined?\n",
    "def read_DeepLocPro(file, columns=None):\n",
    "    with open(file) as handle:\n",
    "        fasta_cols = ['header', 'sequence']\n",
    "        df = pd.DataFrame.from_records([values for values in Bio.SeqIO.FastaIO.SimpleFastaParser(handle)], columns=fasta_cols)\n",
    "    if columns is None:\n",
    "        return df\n",
    "    else:\n",
    "        df[columns] = df['header'].str.split('|', expand=True)\n",
    "        return df[columns + ['sequence']]\n",
    "\n",
    "columns = ['uniprot_id', 'subcellular_location', 'organism_group']\n",
    "df_graphpart = read_DeepLocPro('data/graphpart_set.fasta', columns=columns + ['fold_id']).astype({'fold_id': int}).sort_values('fold_id').reset_index(drop=True)\n",
    "df_graphpart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subcellular_location_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "subcellular_location_encoder.fit(df_graphpart['subcellular_location'])\n",
    "df_graphpart['label'] = subcellular_location_encoder.transform(df_graphpart['subcellular_location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6668 records in training data:\n",
      "subcellular_location\n",
      "Cytoplasmic            3707\n",
      "CYtoplasmicMembrane    1488\n",
      "Extracellular           625\n",
      "OuterMembrane           457\n",
      "Periplasmic             341\n",
      "Cellwall                 50\n",
      "Name: count, dtype: int64\n",
      "\n",
      "2554 records in eval data:\n",
      "subcellular_location\n",
      "Cytoplasmic            1540\n",
      "CYtoplasmicMembrane     525\n",
      "Extracellular           195\n",
      "OuterMembrane           163\n",
      "Periplasmic             110\n",
      "Cellwall                 21\n",
      "Name: count, dtype: int64\n",
      "\n",
      "2684 records in test data:\n",
      "subcellular_location\n",
      "Cytoplasmic            1638\n",
      "CYtoplasmicMembrane     522\n",
      "Extracellular           257\n",
      "OuterMembrane           136\n",
      "Periplasmic             115\n",
      "Cellwall                 16\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Q7. How were the training/benchmark data sets constructed? How were the cross-validation folds defined?\n",
    "# Subsample training/eval data from the homology-partitioned sequences in the preprint\n",
    "random_number = 4 # https://xkcd.com/221/\n",
    "train_id = [2, 3, 4]\n",
    "eval_id = [0]\n",
    "test_id = [1]\n",
    "\n",
    "df_train = df_graphpart.query('fold_id in @train_id')#.groupby('subcellular_location').sample(n=20, random_state=random_number)\n",
    "df_eval = df_graphpart.query('fold_id in @eval_id')#.groupby('subcellular_location').sample(n=10, random_state=random_number)\n",
    "df_test = df_graphpart.query('fold_id in @test_id')\n",
    "print(len(df_train), 'records in training data:')\n",
    "print(df_train['subcellular_location'].value_counts())\n",
    "print()\n",
    "print(len(df_eval), 'records in eval data:')\n",
    "print(df_eval['subcellular_location'].value_counts())\n",
    "print()\n",
    "print(len(df_test), 'records in test data:')\n",
    "print(df_test['subcellular_location'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'facebook/esm2_t6_8M_UR50D'\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = tokenizer(df_train['sequence'].tolist(), truncation=True, max_length=1024)\n",
    "eval_tokenized = tokenizer(df_eval['sequence'].tolist(), truncation=True, max_length=1024)\n",
    "test_tokenized = tokenizer(df_test['sequence'].tolist(), truncation=True, max_length=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.Dataset.from_dict(train_tokenized).add_column('labels', df_train['label'].tolist())\n",
    "eval_dataset = datasets.Dataset.from_dict(eval_tokenized).add_column('labels', df_eval['label'].tolist())\n",
    "test_dataset = datasets.Dataset.from_dict(test_tokenized).add_column('labels', df_test['label'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Q8. Describe the difference between EsmModel, and EsmForSequenceClassification?\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=df_graphpart['label'].nunique())\n",
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = model_checkpoint.split('/')[-1]\n",
    "\n",
    "args = transformers.TrainingArguments(\n",
    "    #f'{model_name}-subcellular_location',\n",
    "    output_dir='esm2_subcellular_location',\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The paper uses accuracy and macro F1 score to characterise the performance; we will trace both throughout the training\n",
    "metric_accuracy = evaluate.load('accuracy')\n",
    "metric_f1 = evaluate.load('f1')\n",
    "\n",
    "def compute_metrics(eval_pred): # https://huggingface.co/docs/transformers/en/training#evaluate\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        'accuracy': metric_accuracy.compute(predictions=predictions, references=labels)['accuracy'],\n",
    "        'f1_macro': metric_f1.compute(predictions=predictions, references=labels, average='macro')['f1'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_162865/2008595096.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = transformers.Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5001' max='5001' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5001/5001 07:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.481200</td>\n",
       "      <td>0.467312</td>\n",
       "      <td>0.885278</td>\n",
       "      <td>0.650214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.371300</td>\n",
       "      <td>0.487173</td>\n",
       "      <td>0.892717</td>\n",
       "      <td>0.655939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.237300</td>\n",
       "      <td>0.494693</td>\n",
       "      <td>0.897024</td>\n",
       "      <td>0.664147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5001, training_loss=0.39647074702157276, metrics={'train_runtime': 436.7619, 'train_samples_per_second': 45.801, 'train_steps_per_second': 11.45, 'total_flos': 598238789673168.0, 'train_loss': 0.39647074702157276, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now fine-tune the network, reporting the performance at the end of every epoch\n",
    "retrained = trainer.train()\n",
    "retrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q. Accuracy/F1 Macro are calculated on what? train, test or eval?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9. How did the parameters change during re-training? Compare (a subset) of weights in the (retrained) model to model_esm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='671' max='671' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [671/671 00:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.47166430950164795,\n",
       " 'eval_accuracy': 0.9001490312965723,\n",
       " 'eval_f1_macro': 0.6711529687340612,\n",
       " 'eval_runtime': 20.8774,\n",
       " 'eval_samples_per_second': 128.56,\n",
       " 'eval_steps_per_second': 32.14,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We evaluate the fine-tuned model on the benchmark data set (globally)\n",
    "# How does the fine-tuned model compare to DeepLocPro as reported in Table 3 of the preprint?\n",
    "trainer.evaluate(eval_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2, 3, 4} 1 0 6668 2684 2554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_162865/496392120.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = transformers.Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5001' max='5001' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5001/5001 07:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.476000</td>\n",
       "      <td>0.399561</td>\n",
       "      <td>0.904247</td>\n",
       "      <td>0.670197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.375900</td>\n",
       "      <td>0.481482</td>\n",
       "      <td>0.895678</td>\n",
       "      <td>0.663600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.238700</td>\n",
       "      <td>0.466033</td>\n",
       "      <td>0.903130</td>\n",
       "      <td>0.672721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 3, 4} 2 1 7124 2098 2684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_162865/496392120.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = transformers.Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5343' max='5343' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5343/5343 07:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.472200</td>\n",
       "      <td>0.464613</td>\n",
       "      <td>0.881792</td>\n",
       "      <td>0.649585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.312000</td>\n",
       "      <td>0.514720</td>\n",
       "      <td>0.879886</td>\n",
       "      <td>0.716558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.241700</td>\n",
       "      <td>0.496437</td>\n",
       "      <td>0.889895</td>\n",
       "      <td>0.741874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 4} 3 2 7229 2579 2098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_162865/496392120.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = transformers.Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2640' max='5424' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2640/5424 03:51 < 04:03, 11.42 it/s, Epoch 1.46/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.462300</td>\n",
       "      <td>0.527522</td>\n",
       "      <td>0.868941</td>\n",
       "      <td>0.635016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 44\u001b[0m\n\u001b[1;32m     25\u001b[0m args \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTrainingArguments(\n\u001b[1;32m     26\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mesm2-subcellular_location-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     27\u001b[0m     eval_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m     per_device_eval_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     33\u001b[0m )\n\u001b[1;32m     35\u001b[0m trainer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     36\u001b[0m     model,\n\u001b[1;32m     37\u001b[0m     args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     42\u001b[0m )\n\u001b[0;32m---> 44\u001b[0m retrained \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     45\u001b[0m test_predictions \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpredict(test_dataset\u001b[38;5;241m=\u001b[39mtest_dataset)\n\u001b[1;32m     46\u001b[0m test_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(test_predictions\u001b[38;5;241m.\u001b[39mpredictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   2172\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   2173\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   2174\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   2175\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   2176\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:2536\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2530\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m   2531\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2534\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2535\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2536\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2537\u001b[0m ):\n\u001b[1;32m   2538\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2540\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fold_id = set(df_graphpart.fold_id)\n",
    "graphpart_labels = []\n",
    "for test_id in sorted(fold_id):\n",
    "    eval_id = (test_id + 1) % 5\n",
    "    train_id = fold_id - set([eval_id, test_id])\n",
    "\n",
    "    df_train = df_graphpart.query('fold_id in @train_id')#.groupby('subcellular_location').sample(n=10, random_state=random_number)\n",
    "    df_eval = df_graphpart.query('fold_id == @eval_id')\n",
    "    df_test = df_graphpart.query('fold_id == @test_id')\n",
    "    print(train_id, eval_id, test_id, len(df_train), len(df_eval), len(df_test))\n",
    "\n",
    "    model_checkpoint = 'facebook/esm2_t6_8M_UR50D'\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "    train_tokenized = tokenizer(df_train['sequence'].tolist(), truncation=True, max_length=1024)\n",
    "    eval_tokenized = tokenizer(df_eval['sequence'].tolist(), truncation=True, max_length=1024)\n",
    "    test_tokenized = tokenizer(df_test['sequence'].tolist(), truncation=True, max_length=1024)\n",
    "\n",
    "    train_dataset = datasets.Dataset.from_dict(train_tokenized).add_column('labels', df_train['label'].tolist())\n",
    "    eval_dataset = datasets.Dataset.from_dict(eval_tokenized).add_column('labels', df_eval['label'].tolist())\n",
    "    test_dataset = datasets.Dataset.from_dict(test_tokenized).add_column('labels', df_test['label'].tolist())\n",
    "\n",
    "    model = transformers.AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=df_graphpart['label'].nunique())\n",
    "    #model_name = model_checkpoint.split('/')[-1]\n",
    "    args = transformers.TrainingArguments(\n",
    "        output_dir=f'esm2-subcellular_location-{eval_id}',\n",
    "        eval_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='accuracy',\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "    )\n",
    "\n",
    "    trainer = transformers.Trainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    retrained = trainer.train()\n",
    "    test_predictions = trainer.predict(test_dataset=test_dataset)\n",
    "    test_labels = np.argmax(test_predictions.predictions, axis=-1)\n",
    "    graphpart_labels += list(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graphpart['label_predicted'] = graphpart_labels\n",
    "print(len(df_graphpart))\n",
    "print(metric_accuracy.compute(predictions=df_graphpart.label_predicted.values, references=df_graphpart.label.values)['accuracy'])\n",
    "print(metric_f1.compute(predictions=df_graphpart.label_predicted.values, references=df_graphpart.label.values, average='macro')['f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show table with performance metrics split by organism to match Table 3 in preprint\n",
    "def apply_(df):\n",
    "    return pd.Series({\n",
    "        'size': len(df),\n",
    "        'accuracy': metric_accuracy.compute(predictions=df.label_predicted.values, references=df.label.values)['accuracy'],\n",
    "        'f1_macro': metric_f1.compute(predictions=df.label_predicted.values, references=df.label.values, average='macro')['f1'],\n",
    "    })\n",
    "\n",
    "# Q10. Re-train on the whole data; compare to DeepLoc Pro\n",
    "print(df_graphpart.groupby('organism_group').apply(apply_).transpose()[['archaea', 'positive', 'negative']].to_string(float_format='%.2f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
