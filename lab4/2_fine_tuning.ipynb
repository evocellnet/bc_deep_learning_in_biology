{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning protein language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment & execute once to download data\n",
    "# https://services.healthtech.dtu.dk/services/DeepLocPro-1.0/\n",
    "#!mkdir -p data\n",
    "#!curl https://services.healthtech.dtu.dk/services/DeepLocPro-1.0/data/graphpart_set.fasta -o data/graphpart_set.fasta\n",
    "#!curl https://services.healthtech.dtu.dk/services/DeepLocPro-1.0/data/benchmarking_dataset.fasta -o data/benchmarking_dataset.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. Describe the problem of predicting the subcellular location of (prokaryotic) proteins as described in Moreno2024 (https://doi.org/10.1101/2024.01.04.574157)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/course/anaconda3/lib/python3.11/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/course/anaconda3/lib/python3.11/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, sklearn.preprocessing\n",
    "import Bio.SeqIO.FastaIO # Biopython for reading fasta files\n",
    "import datasets, evaluate, transformers # Hugging Face libraries https://doi.org/10.18653/v1/2020.emnlp-demos.6\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniprot_id</th>\n",
       "      <th>subcellular_location</th>\n",
       "      <th>organism_group</th>\n",
       "      <th>fold_id</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q8A0Z3</td>\n",
       "      <td>Cytoplasmic</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>MAVTMADITKLRKMTGAGMMDCKNALTEAEGDYDKAMEIIRKKGQA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q8A2N8</td>\n",
       "      <td>Cytoplasmic</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>MIMSKETLIKSIREIPDFPIPGILFYDVTTLFKDPWCLQELSNIMF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P32709</td>\n",
       "      <td>CYtoplasmicMembrane</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>MTQTSAFHFESLVWDWPIAIYLFLIGISAGLVTLAVLLRRFYPQAG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E7FHF8</td>\n",
       "      <td>Cytoplasmic</td>\n",
       "      <td>archaea</td>\n",
       "      <td>0</td>\n",
       "      <td>MKLGVFELTDCGGCALNLLFLYDKLLDLLEFYEIAEFHMATSKKSR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E7FHU4</td>\n",
       "      <td>Cytoplasmic</td>\n",
       "      <td>archaea</td>\n",
       "      <td>0</td>\n",
       "      <td>MGKVRIGFYALTSCYGCQLQLAMMDELLQLIPNAEIVCWFMIDRDS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11901</th>\n",
       "      <td>Q97F85</td>\n",
       "      <td>Cytoplasmic</td>\n",
       "      <td>positive</td>\n",
       "      <td>4</td>\n",
       "      <td>MRKLFTSESVTEGHPDKICDQISDAILDAILEKDPNGRVACETTVT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11902</th>\n",
       "      <td>P33656</td>\n",
       "      <td>Cytoplasmic</td>\n",
       "      <td>positive</td>\n",
       "      <td>4</td>\n",
       "      <td>MKNKTEVKNGGEKKNSKKVSKEESAKEKNEKMKIVKNLIDKGKKSG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11903</th>\n",
       "      <td>P13949</td>\n",
       "      <td>OuterMembrane</td>\n",
       "      <td>negative</td>\n",
       "      <td>4</td>\n",
       "      <td>MCALDRRERPLNSQSVNKYILNVQNIYRNSPVPVCVRNKNRKILYA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11904</th>\n",
       "      <td>P42185</td>\n",
       "      <td>Extracellular</td>\n",
       "      <td>negative</td>\n",
       "      <td>4</td>\n",
       "      <td>MRLRFSVPLFFFGCVFVHGVFAGPFPPPGMSLPEYWGEEHVWWDGR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11905</th>\n",
       "      <td>Q0P986</td>\n",
       "      <td>OuterMembrane</td>\n",
       "      <td>negative</td>\n",
       "      <td>4</td>\n",
       "      <td>MKTRFSLILSACLLSSSLFAKNTDDEITKLQKQLAQIQAELAQIRK...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11906 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      uniprot_id subcellular_location organism_group  fold_id  \\\n",
       "0         Q8A0Z3          Cytoplasmic       negative        0   \n",
       "1         Q8A2N8          Cytoplasmic       negative        0   \n",
       "2         P32709  CYtoplasmicMembrane       negative        0   \n",
       "3         E7FHF8          Cytoplasmic        archaea        0   \n",
       "4         E7FHU4          Cytoplasmic        archaea        0   \n",
       "...          ...                  ...            ...      ...   \n",
       "11901     Q97F85          Cytoplasmic       positive        4   \n",
       "11902     P33656          Cytoplasmic       positive        4   \n",
       "11903     P13949        OuterMembrane       negative        4   \n",
       "11904     P42185        Extracellular       negative        4   \n",
       "11905     Q0P986        OuterMembrane       negative        4   \n",
       "\n",
       "                                                sequence  \n",
       "0      MAVTMADITKLRKMTGAGMMDCKNALTEAEGDYDKAMEIIRKKGQA...  \n",
       "1      MIMSKETLIKSIREIPDFPIPGILFYDVTTLFKDPWCLQELSNIMF...  \n",
       "2      MTQTSAFHFESLVWDWPIAIYLFLIGISAGLVTLAVLLRRFYPQAG...  \n",
       "3      MKLGVFELTDCGGCALNLLFLYDKLLDLLEFYEIAEFHMATSKKSR...  \n",
       "4      MGKVRIGFYALTSCYGCQLQLAMMDELLQLIPNAEIVCWFMIDRDS...  \n",
       "...                                                  ...  \n",
       "11901  MRKLFTSESVTEGHPDKICDQISDAILDAILEKDPNGRVACETTVT...  \n",
       "11902  MKNKTEVKNGGEKKNSKKVSKEESAKEKNEKMKIVKNLIDKGKKSG...  \n",
       "11903  MCALDRRERPLNSQSVNKYILNVQNIYRNSPVPVCVRNKNRKILYA...  \n",
       "11904  MRLRFSVPLFFFGCVFVHGVFAGPFPPPGMSLPEYWGEEHVWWDGR...  \n",
       "11905  MKTRFSLILSACLLSSSLFAKNTDDEITKLQKQLAQIQAELAQIRK...  \n",
       "\n",
       "[11906 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q7. How were the training/benchmark data sets constructed? How were the cross-validation folds defined?\n",
    "def read_DeepLocPro(file, columns=None):\n",
    "    with open(file) as handle:\n",
    "        fasta_cols = ['header', 'sequence']\n",
    "        df = pd.DataFrame.from_records([values for values in Bio.SeqIO.FastaIO.SimpleFastaParser(handle)], columns=fasta_cols)\n",
    "    if columns is None:\n",
    "        return df\n",
    "    else:\n",
    "        df[columns] = df['header'].str.split('|', expand=True)\n",
    "        return df[columns + ['sequence']]\n",
    "\n",
    "columns = ['uniprot_id', 'subcellular_location', 'organism_group']\n",
    "df_graphpart = read_DeepLocPro('data/graphpart_set.fasta', columns=columns + ['fold_id']).astype({'fold_id': int}).sort_values('fold_id').reset_index(drop=True)\n",
    "df_graphpart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subcellular_location_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "subcellular_location_encoder.fit(df_graphpart['subcellular_location'])\n",
    "df_graphpart['label'] = subcellular_location_encoder.transform(df_graphpart['subcellular_location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6668 records in training data:\n",
      "subcellular_location\n",
      "Cytoplasmic            3707\n",
      "CYtoplasmicMembrane    1488\n",
      "Extracellular           625\n",
      "OuterMembrane           457\n",
      "Periplasmic             341\n",
      "Cellwall                 50\n",
      "Name: count, dtype: int64\n",
      "\n",
      "2554 records in eval data:\n",
      "subcellular_location\n",
      "Cytoplasmic            1540\n",
      "CYtoplasmicMembrane     525\n",
      "Extracellular           195\n",
      "OuterMembrane           163\n",
      "Periplasmic             110\n",
      "Cellwall                 21\n",
      "Name: count, dtype: int64\n",
      "\n",
      "2684 records in test data:\n",
      "subcellular_location\n",
      "Cytoplasmic            1638\n",
      "CYtoplasmicMembrane     522\n",
      "Extracellular           257\n",
      "OuterMembrane           136\n",
      "Periplasmic             115\n",
      "Cellwall                 16\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Q7. How were the training/benchmark data sets constructed? How were the cross-validation folds defined?\n",
    "# Subsample training/eval data from the homology-partitioned sequences in the preprint\n",
    "random_number = 4 # https://xkcd.com/221/\n",
    "train_id = [2, 3, 4]\n",
    "eval_id = [0]\n",
    "test_id = [1]\n",
    "\n",
    "df_train = df_graphpart.query('fold_id in @train_id')#.groupby('subcellular_location').sample(n=20, random_state=random_number)\n",
    "df_eval = df_graphpart.query('fold_id in @eval_id')#.groupby('subcellular_location').sample(n=10, random_state=random_number)\n",
    "df_test = df_graphpart.query('fold_id in @test_id')\n",
    "print(len(df_train), 'records in training data:')\n",
    "print(df_train['subcellular_location'].value_counts())\n",
    "print()\n",
    "print(len(df_eval), 'records in eval data:')\n",
    "print(df_eval['subcellular_location'].value_counts())\n",
    "print()\n",
    "print(len(df_test), 'records in test data:')\n",
    "print(df_test['subcellular_location'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'facebook/esm2_t6_8M_UR50D'\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = tokenizer(df_train['sequence'].tolist(), truncation=True, max_length=1024)\n",
    "eval_tokenized = tokenizer(df_eval['sequence'].tolist(), truncation=True, max_length=1024)\n",
    "test_tokenized = tokenizer(df_test['sequence'].tolist(), truncation=True, max_length=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.Dataset.from_dict(train_tokenized).add_column('labels', df_train['label'].tolist())\n",
    "eval_dataset = datasets.Dataset.from_dict(eval_tokenized).add_column('labels', df_eval['label'].tolist())\n",
    "test_dataset = datasets.Dataset.from_dict(test_tokenized).add_column('labels', df_test['label'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Q8. Describe the difference between EsmModel, and EsmForSequenceClassification?\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=df_graphpart['label'].nunique())\n",
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = model_checkpoint.split('/')[-1]\n",
    "\n",
    "args = transformers.TrainingArguments(\n",
    "    #f'{model_name}-subcellular_location',\n",
    "    output_dir='esm2_subcellular_location',\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2acd29eb11734514b154ebdc4efe87cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The paper uses accuracy and macro F1 score to characterise the performance; we will trace both throughout the training\n",
    "metric_accuracy = evaluate.load('accuracy')\n",
    "metric_f1 = evaluate.load('f1')\n",
    "\n",
    "def compute_metrics(eval_pred): # https://huggingface.co/docs/transformers/en/training#evaluate\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        'accuracy': metric_accuracy.compute(predictions=predictions, references=labels)['accuracy'],\n",
    "        'f1_macro': metric_f1.compute(predictions=predictions, references=labels, average='macro')['f1'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_454440/2008595096.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = transformers.Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5001' max='5001' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5001/5001 07:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.464200</td>\n",
       "      <td>0.426776</td>\n",
       "      <td>0.890760</td>\n",
       "      <td>0.650232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.365900</td>\n",
       "      <td>0.496763</td>\n",
       "      <td>0.891151</td>\n",
       "      <td>0.669842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.254100</td>\n",
       "      <td>0.501464</td>\n",
       "      <td>0.891151</td>\n",
       "      <td>0.694605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5001, training_loss=0.3957060627409117, metrics={'train_runtime': 438.5219, 'train_samples_per_second': 45.617, 'train_steps_per_second': 11.404, 'total_flos': 598238789673168.0, 'train_loss': 0.3957060627409117, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now fine-tune the network, reporting the performance at the end of every epoch\n",
    "retrained = trainer.train()\n",
    "retrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q. Accuracy/F1 Macro are calculated on what? train, test or eval?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9. How did the parameters change during re-training? Compare (a subset) of weights in the (retrained) model to model_esm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='671' max='671' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [671/671 00:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4456363320350647,\n",
       " 'eval_accuracy': 0.9016393442622951,\n",
       " 'eval_f1_macro': 0.6649352551667449,\n",
       " 'eval_runtime': 20.378,\n",
       " 'eval_samples_per_second': 131.71,\n",
       " 'eval_steps_per_second': 32.928,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We evaluate the fine-tuned model on the benchmark data set (globally)\n",
    "# How does the fine-tuned model compare to DeepLocPro as reported in Table 3 of the preprint?\n",
    "trainer.evaluate(eval_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2, 3, 4} 1 0 6668 2684 2554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_454440/496392120.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = transformers.Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5001' max='5001' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5001/5001 07:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.476000</td>\n",
       "      <td>0.399561</td>\n",
       "      <td>0.904247</td>\n",
       "      <td>0.670197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.375900</td>\n",
       "      <td>0.481482</td>\n",
       "      <td>0.895678</td>\n",
       "      <td>0.663600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.238700</td>\n",
       "      <td>0.466033</td>\n",
       "      <td>0.903130</td>\n",
       "      <td>0.672721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 3, 4} 2 1 7124 2098 2684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_454440/496392120.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = transformers.Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5343' max='5343' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5343/5343 07:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.472200</td>\n",
       "      <td>0.464613</td>\n",
       "      <td>0.881792</td>\n",
       "      <td>0.649585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.312000</td>\n",
       "      <td>0.514720</td>\n",
       "      <td>0.879886</td>\n",
       "      <td>0.716558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.241700</td>\n",
       "      <td>0.496437</td>\n",
       "      <td>0.889895</td>\n",
       "      <td>0.741874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 4} 3 2 7229 2579 2098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_454440/496392120.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = transformers.Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5424' max='5424' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5424/5424 08:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.462300</td>\n",
       "      <td>0.527522</td>\n",
       "      <td>0.868941</td>\n",
       "      <td>0.635016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.323500</td>\n",
       "      <td>0.494091</td>\n",
       "      <td>0.887166</td>\n",
       "      <td>0.654278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.255000</td>\n",
       "      <td>0.523508</td>\n",
       "      <td>0.883676</td>\n",
       "      <td>0.664307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2} 4 3 7336 1991 2579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_454440/496392120.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = transformers.Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5502' max='5502' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5502/5502 08:17, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.483400</td>\n",
       "      <td>0.629983</td>\n",
       "      <td>0.844802</td>\n",
       "      <td>0.638483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.310500</td>\n",
       "      <td>0.637493</td>\n",
       "      <td>0.866399</td>\n",
       "      <td>0.693083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.217700</td>\n",
       "      <td>0.644620</td>\n",
       "      <td>0.873430</td>\n",
       "      <td>0.740562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1, 2, 3} 0 4 7361 2554 1991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_454440/496392120.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = transformers.Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5523' max='5523' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5523/5523 08:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.485800</td>\n",
       "      <td>0.442423</td>\n",
       "      <td>0.885670</td>\n",
       "      <td>0.643349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.311400</td>\n",
       "      <td>0.511180</td>\n",
       "      <td>0.888802</td>\n",
       "      <td>0.682085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.215500</td>\n",
       "      <td>0.500282</td>\n",
       "      <td>0.898199</td>\n",
       "      <td>0.733145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fold_id = set(df_graphpart.fold_id)\n",
    "graphpart_labels = []\n",
    "for test_id in sorted(fold_id):\n",
    "    eval_id = (test_id + 1) % 5\n",
    "    train_id = fold_id - set([eval_id, test_id])\n",
    "\n",
    "    df_train = df_graphpart.query('fold_id in @train_id')#.groupby('subcellular_location').sample(n=10, random_state=random_number)\n",
    "    df_eval = df_graphpart.query('fold_id == @eval_id')\n",
    "    df_test = df_graphpart.query('fold_id == @test_id')\n",
    "    print(train_id, eval_id, test_id, len(df_train), len(df_eval), len(df_test))\n",
    "\n",
    "    model_checkpoint = 'facebook/esm2_t6_8M_UR50D'\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "    train_tokenized = tokenizer(df_train['sequence'].tolist(), truncation=True, max_length=1024)\n",
    "    eval_tokenized = tokenizer(df_eval['sequence'].tolist(), truncation=True, max_length=1024)\n",
    "    test_tokenized = tokenizer(df_test['sequence'].tolist(), truncation=True, max_length=1024)\n",
    "\n",
    "    train_dataset = datasets.Dataset.from_dict(train_tokenized).add_column('labels', df_train['label'].tolist())\n",
    "    eval_dataset = datasets.Dataset.from_dict(eval_tokenized).add_column('labels', df_eval['label'].tolist())\n",
    "    test_dataset = datasets.Dataset.from_dict(test_tokenized).add_column('labels', df_test['label'].tolist())\n",
    "\n",
    "    model = transformers.AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=df_graphpart['label'].nunique())\n",
    "    #model_name = model_checkpoint.split('/')[-1]\n",
    "    args = transformers.TrainingArguments(\n",
    "        output_dir=f'esm2-subcellular_location-{eval_id}',\n",
    "        eval_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='accuracy',\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "    )\n",
    "\n",
    "    trainer = transformers.Trainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    retrained = trainer.train()\n",
    "    test_predictions = trainer.predict(test_dataset=test_dataset)\n",
    "    test_labels = np.argmax(test_predictions.predictions, axis=-1)\n",
    "    graphpart_labels += list(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11906\n",
      "0.891819250797917\n",
      "0.7068229932223565\n"
     ]
    }
   ],
   "source": [
    "df_graphpart['label_predicted'] = graphpart_labels\n",
    "print(len(df_graphpart))\n",
    "print(metric_accuracy.compute(predictions=df_graphpart.label_predicted.values, references=df_graphpart.label.values)['accuracy'])\n",
    "print(metric_f1.compute(predictions=df_graphpart.label_predicted.values, references=df_graphpart.label.values, average='macro')['f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Overall</th>\n",
       "      <th>Archaea</th>\n",
       "      <th>Gram pos</th>\n",
       "      <th>Gram neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>size</th>\n",
       "      <td>11906</td>\n",
       "      <td>283</td>\n",
       "      <td>3206</td>\n",
       "      <td>8417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_macro</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Overall Archaea Gram pos Gram neg\n",
       "size       11906     283     3206     8417\n",
       "accuracy    0.89    0.84     0.92     0.88\n",
       "f1_macro    0.71    0.43     0.48     0.69"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show table with performance metrics split by organism to match Table 3 in preprint\n",
    "def calculate_stats_(df):\n",
    "    accuracy = metric_accuracy.compute(predictions=df.label_predicted.values, references=df.label.values)['accuracy']\n",
    "    f1_macro = metric_f1.compute(predictions=df.label_predicted.values, references=df.label.values, average='macro')['f1']\n",
    "    return pd.Series({\n",
    "        'size': '{:d}'.format(len(df)),\n",
    "        'accuracy': '{:.2f}'.format(accuracy),\n",
    "        'f1_macro': '{:.2f}'.format(f1_macro),\n",
    "    })\n",
    "\n",
    "# Q10. Re-train on the whole data; compare to DeepLoc Pro\n",
    "#print(df_graphpart.groupby('organism_group').apply(apply_).transpose()[['archaea', 'positive', 'negative']].to_string(float_format='%.2f'))\n",
    "#display(HTML(df_graphpart.groupby('organism_group').apply(apply_).transpose()[['archaea', 'positive', 'negative']].to_html()))\n",
    "pd.concat([\n",
    "    calculate_stats_(df_graphpart),\n",
    "    calculate_stats_(df_graphpart.query('organism_group == \"archaea\"')),\n",
    "    calculate_stats_(df_graphpart.query('organism_group == \"positive\"')),\n",
    "    calculate_stats_(df_graphpart.query('organism_group == \"negative\"')),\n",
    "], axis=1).set_axis(['Overall', 'Archaea' , 'Gram pos', 'Gram neg'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
