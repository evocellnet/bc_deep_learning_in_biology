{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10ad0467-fc31-4586-9d84-fea0948fb7a1",
   "metadata": {},
   "source": [
    "# Lab 1: Introduction to Neural Networks\n",
    "In this tutorial we introduce some of the concepts for working with neural networks using [Pytorch](https://pytorch.org/tutorials/recipes/recipes_index.html). The entire notebook can be executed as-is, given the lack of time for this first lab session. We encourage you to explore the code yourselves to get comfortable with the concepts of deel learning in the context of biology. A few questions at the end challenge you to play around with the code and try things for yourselves.\n",
    "\n",
    "In this session, you will create a simple neural network that classifies any given DNA sequence as protein coding or not. As a starting point, we use as examples the coding DNA sequences from humans (homo sapiens (HS)). As negatives, we use random sequences of DNA where each nucleotide is drawn from a uniform distribution over the possible nucleotides. We then train a neural network on de [codon frequencies](https://en.wikipedia.org/wiki/DNA_and_RNA_codon_tables) of these sequences.\n",
    "\n",
    "In addition to human DNA sequences, we also take a look at coding sequences from mice ([mus musculus (MM)](https://en.wikipedia.org/wiki/House_mouse)) and yeast ([saccharomyces cerevisiae (SC)](https://en.wikipedia.org/wiki/Saccharomyces_cerevisiae)). There are subtle differences between the coding frequencies of these species. You will test how well your human-trained model is able to recover the coding sequences for mice and yeast (think of your results in the context of evolutionary distances between species). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d75f7f4-c7b1-46a8-8637-5cba8634b516",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# import basic functionality\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# libraries for plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4f5ede-7ed4-4cfc-bfdd-a9fddddbabbd",
   "metadata": {},
   "source": [
    "# Step 1: Pre-processing the data\n",
    "Here we download and pre-process the dataset. We only consider DNA sequences that are protein coding, contain a integer number of codons, and have a start and stop codon. Finally, we remove duplicates and randomly mix the sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318b2d30-85ab-4aa6-837c-d2d6693584ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# download and unpack DNA coding sequences for human, mouse and yeast\n",
    "############################\n",
    "\n",
    "!mkdir -p ~/all_seqs\n",
    "%cd ~/\n",
    "\n",
    "!wget -P ~/all_seqs/ https://ftp.ensembl.org/pub/current_fasta/homo_sapiens/cds/Homo_sapiens.GRCh38.cds.all.fa.gz\n",
    "!gzip -df \"all_seqs/Homo_sapiens.GRCh38.cds.all.fa.gz\"\n",
    "\n",
    "!wget -P ~/all_seqs/ https://ftp.ensembl.org/pub/current_fasta/saccharomyces_cerevisiae/cds/Saccharomyces_cerevisiae.R64-1-1.cds.all.fa.gz\n",
    "!gzip -df \"all_seqs/Saccharomyces_cerevisiae.R64-1-1.cds.all.fa.gz\"\n",
    "\n",
    "!wget -P ~/all_seqs/ https://ftp.ensembl.org/pub/current_fasta/mus_musculus/cds/Mus_musculus.GRCm39.cds.all.fa.gz\n",
    "!gzip -df \"all_seqs/Mus_musculus.GRCm39.cds.all.fa.gz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b21b02-c373-4f98-895e-8424ae1fbd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################\n",
    "### some unnecessarily complex code to process the FASTA files for coding sequences\n",
    "############################\n",
    "\n",
    "# function that loads and processes a FASTA file containing coding sequences\n",
    "def load_species_cds(file_name, max_nr_samples):\n",
    "    seqs = []\n",
    "    for record in SeqIO.parse(file_name, \"fasta\"):\n",
    "        # ensure that sequences are protein coding\n",
    "        if 'gene_biotype:protein_coding' in record.description:\n",
    "            if 'transcript_biotype:protein_coding' in record.description:\n",
    "                if ' cds ' in record.description:\n",
    "                    if len(record.seq) % 3 == 0:\n",
    "                        # translate sequence and check for start and stop codons\n",
    "                        code_translation = str(record.seq.translate())\n",
    "                        if (code_translation[0]=='M') & (code_translation[-1]=='*'):\n",
    "                            seqs.append(str(record.seq))\n",
    "\n",
    "    # avoid sequences with undetermined/uncertain nucleotides\n",
    "    # restrict to sequences with at least 100 aa for codon frequency estimation\n",
    "    seqs = [seqs[i] for i in range(len(seqs)) if (len(seqs[i])>=300)]#('N' not in train_cds_filtered[i]) and (len(train_cds_filtered[i])>=300)]\n",
    "    \n",
    "    # remove duplicates and randomly mix the list of sequences\n",
    "    seqs = list(set(seqs))\n",
    "    random.shuffle(seqs)\n",
    "    \n",
    "    return list(seqs)[0:max_nr_samples]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc9f681-fc16-46e5-9565-26e8010694e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# there are many sequences, given the time constraints, we limit the number of sequences to speed up the processing\n",
    "max_nr_samples = 20000\n",
    "\n",
    "# load coding sequences for different species\n",
    "print('loading human proteins')\n",
    "seq_data_human = load_species_cds(\"all_seqs/Homo_sapiens.GRCh38.cds.all.fa\", max_nr_samples)\n",
    "\n",
    "print('loading yeast proteins')\n",
    "seq_data_yeast = load_species_cds(\"all_seqs/Saccharomyces_cerevisiae.R64-1-1.cds.all.fa\", max_nr_samples)\n",
    "\n",
    "print('loading mouse proteins')\n",
    "seq_data_mouse = load_species_cds(\"all_seqs/Mus_musculus.GRCm39.cds.all.fa\", max_nr_samples)\n",
    "\n",
    "# take a look at some sequences\n",
    "[seq_data_human[i][0:20]+'...'+seq_data_human[i][-20:] for i in range(5)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629ead0a",
   "metadata": {},
   "source": [
    "# Step 2: Encoding sequences as codon frequencies\n",
    "The steps above give us a set of unique coding sequences for humans, mice and yeast. To train a neural network on de coding frequencies of these sequences, we encode the sequences by converting each sequence to an array of frequencies for each possible codon. Each codon gets assigned a index in the array. We first create a 'language' that knows all possible words (codons) for a given codon length and input sequence. This language then allows us to convert between codon (e.g., 'ATG') and indices in the array (e.g., 'ATG' -> 0) to keep track of the codon frequencies. Here, we use Tensors - the datatype used for pytorch data - to store the coding frequencies.\n",
    "\n",
    "Biologically, [DNA codons](https://en.wikipedia.org/wiki/DNA_and_RNA_codon_tables) consist of three nucleotides, encoding amino acids. However, since we are training a neural network to classify a sequence to be protein coding or not, we can choose any number of nucleotides to represent a 'codon'. For example, we can choose a \"codon length\" (codon_length) of a single nucleotide (which would result in us training the model on the [frequencies of nucleotides in DNA](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC403801/)), or a codon length of two nucleotides (no biological meaning as this does not represent a biological unit - we do not expect a model to learn any biology at all), or a codon length of 6 nucleotides (representing pairs of amino acids - would this yield a model that \"learns\" any biology?). You can play around with the codon_length yourself, but we start with a codon length of 3 nucleotides - represeting one amino acid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dd4243",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class to store a language\n",
    "class Language:\n",
    "    # initialize the language, as standard we have start of sentence (SOS), end of sentence (EOS) and a padding to equalize sentence lengths (PAD)\n",
    "    def __init__(self, name, codon_len):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.index2word = {}\n",
    "        self.n_words = 0\n",
    "        self.codon_length = codon_len\n",
    "\n",
    "\n",
    "    # function to split sentence in blocks of a given codon_length\n",
    "    def splitSentence(self, sentence):\n",
    "        return [sentence[i:i+self.codon_length] for i in range(0,len(sentence),self.codon_length) if len(sentence[i:i+self.codon_length])==self.codon_length]\n",
    "\n",
    "\n",
    "    # function to add sentence to language (add all new words in the sentence to our language)\n",
    "    def learnWords(self, sentence):\n",
    "        sentence_split = self.splitSentence(sentence)\n",
    "        for word in sentence_split:\n",
    "            if word not in self.word2index:\n",
    "                self.word2index[word] = self.n_words\n",
    "                self.index2word[self.n_words] = word\n",
    "                self.n_words += 1\n",
    "\n",
    "\n",
    "    # function to count the word frequencies in a sentence\n",
    "    def encode(self, sentence):\n",
    "        sentence_split = self.splitSentence(sentence)\n",
    "    \n",
    "        ############################\n",
    "        ### create a tensor having the number of available words as length, fill with with zeros\n",
    "        codon_freqs = torch.zeros(self.n_words)\n",
    "        ############################\n",
    "\n",
    "        # count frequencies of every word in the sentence\n",
    "        word_counts = np.unique(sentence_split, return_counts = True)\n",
    "        for i in range(len(word_counts[0])):\n",
    "            codon_freqs[self.word2index[word_counts[0][i]]] = word_counts[1][i]\n",
    "        codon_freqs /= len(sentence_split)\n",
    "\n",
    "        return codon_freqs\n",
    "\n",
    "\n",
    "    # return a sample of frequencies for all words in a language\n",
    "    # here we're matching the codon frequencies to the sequence length of the provided sequence\n",
    "    def sample_sentence(self, sentence):\n",
    "        # create a tensor having the number of available words as length, fill with with zeros\n",
    "        codon_freqs = torch.zeros(self.n_words)\n",
    "\n",
    "        # sample nr of codons in sequence based on actual data\n",
    "        # generate random sequence of codons given the current sequence length\n",
    "        nr_codons = round(len(sentence)/self.codon_length)\n",
    "        sampled_codons = list(np.random.randint(low=0, high=self.n_words, size = nr_codons, dtype=int))\n",
    "\n",
    "        # count frequencies of every codon\n",
    "        word_counts = np.unique(sampled_codons, return_counts = True)\n",
    "        for i in range(len(word_counts[0])):\n",
    "            codon_freqs[word_counts[0][i]] = word_counts[1][i]\n",
    "        codon_freqs /= nr_codons\n",
    "\n",
    "        return codon_freqs\n",
    "\n",
    "\n",
    "    # here we define a function for encoding an entire dataset sequences\n",
    "    def encode_dataset(self, dataset):\n",
    "        # encode positives\n",
    "        encoded_positives = [{'sample':self.encode(sentence),'label':torch.Tensor([1])} for sentence in dataset]\n",
    "\n",
    "        # sample negatives following the sequence length distribution of positives\n",
    "        encoded_negatives = [{'sample':self.sample_sentence(sentence),'label':torch.Tensor([0])} for sentence in dataset]\n",
    "\n",
    "        # merge datasets and randomly mix positives and negatives\n",
    "        dataset_encoded = encoded_positives + encoded_negatives\n",
    "        random.shuffle(dataset_encoded)\n",
    "        \n",
    "        return dataset_encoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46793ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "codon_length = 3\n",
    "\n",
    "# create a language for human DNA sequences\n",
    "dna_lang = Language(name=\"dna_human\", codon_len=codon_length)\n",
    "\n",
    "# memorize the dna language by parsing all sequences\n",
    "for cur_seq in seq_data_human:\n",
    "    dna_lang.learnWords(cur_seq)\n",
    "\n",
    "# split the sequence data ('seq_data_human') that we defined above into training, validation and test sets\n",
    "train_set_human, val_set_human, test_set_human = torch.utils.data.random_split(seq_data_human, [0.5,0.4,0.1])\n",
    "\n",
    "# encode the training validation and test data\n",
    "train_set_human_encoded = dna_lang.encode_dataset(train_set_human)\n",
    "val_set_human_encoded = dna_lang.encode_dataset(val_set_human)\n",
    "test_set_human_encoded = dna_lang.encode_dataset(test_set_human)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e12354",
   "metadata": {},
   "source": [
    "After creating a language to convert between DNA sequences and coding frequencies, we split our data into training, validation and test sets of appropriate sizes, and we encoded our DNA sequences into codon frequencies. Finally, for each sequence, we also created a random sequence of codons following the same sequence length distribution as the DNA sequences. These nucleotides are drawn from a random uniform distribution over the possible codons. These random sequences are used as negatives (i.e., label = 1 will tell the model that a sequence is protein coding, label = 0 tells the model that a sequence is not protein coding). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ce9fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at one of the samples:\n",
    "train_set_human_encoded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3feb834",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# as before but for the other species:\n",
    "# split the sequence data that we defined above into training, validation and test sets\n",
    "# encode the training and validation data\n",
    "\n",
    "train_set_mouse, val_set_mouse, test_set_mouse = torch.utils.data.random_split(seq_data_mouse, [0.5,0.4,0.1])\n",
    "train_set_mouse_encoded = dna_lang.encode_dataset(train_set_mouse)\n",
    "val_set_mouse_encoded = dna_lang.encode_dataset(val_set_mouse)\n",
    "test_set_mouse_encoded = dna_lang.encode_dataset(test_set_mouse)\n",
    "\n",
    "train_set_yeast, val_set_yeast, test_set_yeast = torch.utils.data.random_split(seq_data_yeast, [0.5,0.4,0.1])\n",
    "train_set_yeast_encoded = dna_lang.encode_dataset(train_set_yeast)\n",
    "val_set_yeast_encoded = dna_lang.encode_dataset(val_set_yeast)\n",
    "test_set_yeast_encoded = dna_lang.encode_dataset(test_set_yeast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a0f6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# explore the correlation of coding frequencies across species\n",
    "# first average and merge codon frequencies of the different species and random sequences\n",
    "codon_freqs = pd.DataFrame([np.mean(np.array([ch['sample'] for ch in train_set_human_encoded if ch['label']==1]),axis=0),\n",
    "                         np.mean(np.array([ch['sample'] for ch in train_set_mouse_encoded if ch['label']==1]),axis=0),\n",
    "                         np.mean(np.array([ch['sample'] for ch in train_set_yeast_encoded if ch['label']==1]),axis=0),\n",
    "                         np.mean(np.array([ch['sample'] for ch in train_set_human_encoded if ch['label']==0]),axis=0)\n",
    "                         ]).T\n",
    "\n",
    "# label codons and sort by human frequency\n",
    "codon_freqs.index = [dna_lang.index2word[idx] for idx in list(codon_freqs.index)]\n",
    "codon_freqs.reset_index(inplace=True)\n",
    "codon_freqs.columns = ['codon','human','mouse','yeast','random']\n",
    "codon_freqs.sort_values(by='human',ascending=False,inplace=True)\n",
    "\n",
    "print('correlation matrix: ')\n",
    "print(codon_freqs.set_index('codon').corr())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c8cde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot coding frequencies for different species\n",
    "# initialize figure\n",
    "sns.set(rc={'figure.figsize':(5,15)})\n",
    "sns.set(font=\"Arial\")\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# stack dataframe of frequencies\n",
    "plot_freqs = codon_freqs.set_index('codon').stack().reset_index()\n",
    "plot_freqs.columns = ['codon','origin','frequency']\n",
    "\n",
    "# plot codon frequencies for different species\n",
    "sns.barplot(data=plot_freqs,x='frequency',y='codon',hue='origin')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd187f1-4a50-4aa9-b65a-9662b4d83c19",
   "metadata": {},
   "source": [
    "# Step 3: Creating a dataloader\n",
    "Having encoded our DNA sequences as codon frequencies, we are ready to prepare the data for training a neural network. We will create a 'dataloader' that takes care of splitting the data into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f334547-e066-4b86-974a-28fab683b9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################\n",
    "# define a function to create a dataloader for the encoded sequences\n",
    "def get_dataloader(dataset, batch_size):\n",
    "    cur_sampler = RandomSampler(dataset)\n",
    "    cur_dataloader = DataLoader(dataset=dataset, sampler=cur_sampler, batch_size=batch_size, drop_last=True, num_workers=15)\n",
    "    return cur_dataloader    \n",
    "############################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d01963f-3aaf-42eb-a6e9-09194556a7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# how many samples should be trained on simultaneously?\n",
    "batch_size = 300\n",
    "\n",
    "# define dataloader for training\n",
    "train_loader_human = get_dataloader(train_set_human_encoded, batch_size)\n",
    "val_loader_human = get_dataloader(val_set_human_encoded, batch_size)\n",
    "test_loader_human = get_dataloader(test_set_human_encoded, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d11caf-2312-44d9-b79b-7fab1a70a3a9",
   "metadata": {},
   "source": [
    "# Step 4: Define model\n",
    "As a final preparation, we define our [model](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). We create a class that instructs pytorch to follow a certain architecture for the model. Our model initializes all relevant parts (init function) and tells pytorch how to compute the output for a given input (forward function). For our perceptron, we use a single dense linear layer with a sigmoidal activation function. You can play around with the model architecture later - several options are left as comments.\n",
    "We are trying to train a model for solving a classfication problem. The labels for our samples are binary (ones and zeros). We therefore use a binary loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc510394",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the device (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957672b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the model architecture\n",
    "class myPerceptron(nn.Module):\n",
    "    def __init__(self, input_param, hidden_param, output_param, dropout_prob):\n",
    "        super(myPerceptron, self).__init__()\n",
    "        \n",
    "        self.input_param = input_param\n",
    "        self.hidden_param = hidden_param\n",
    "        self.output_param = output_param\n",
    "        \n",
    "#        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.linear0 = nn.Linear(input_param, output_param)\n",
    "        self.sigmoid = nn.Sigmoid() # or other activation function (e.g. ReLU)\n",
    "\n",
    "    def forward(self, inp):\n",
    "#        inp_drop = self.dropout(inp)\n",
    "        layer0 = self.linear0(inp)\n",
    "        output = self.sigmoid(layer0)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda8ce7f-391d-45c8-afdf-7e1a2731eb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# use binary cross entropy los for this classification problem\n",
    "my_loss_function = nn.BCELoss()\n",
    "\n",
    "# initialize an instance of our model class (a variable that is a model following the architecture we defined above)\n",
    "my_model = myPerceptron(dna_lang.n_words, # size of input tensors (the number of codons)\n",
    "                     20, # size of a hidden layer (not used for now)\n",
    "                     1, # size of the model's output\n",
    "                     0 # some additional parameter that could be used (e.g. dropout frequency)\n",
    "                    ).to(device) # send model to device\n",
    "\n",
    "# show model architecture\n",
    "my_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8497c6-3537-47a4-9d87-a65fc4ae284d",
   "metadata": {},
   "source": [
    "# Step 5: Train simple model \n",
    "To train our model, we need a \"training loop\". \n",
    "1) First, we tell pytorch that we want to train our model (so it has to keep track of gradients). \n",
    "2) We then iterate over our data in batches to speed up computations (there is little advantage for computing the gradient with all samples over, say, a few hundred samples). \n",
    "3) We set the gradients to zero (we don't want to re-use previous computations for our next training step).\n",
    "4) We compute the output of the model for the given input sequences. \n",
    "5) We then compute the loss of the model output for the given target labels of the input sequences and [backpropagate](https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html) the loss through the network to compute the gradient. \n",
    "6) Finally, we instruct the optimizer to use the gradient and perform one appropriately-sized [step](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.step.html) to update the model weights. \n",
    "7) To keep track of our effors we compute the accuracy and training loss for the current samples.\n",
    "\n",
    "Finally, we train our model using the given data and training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bd0758-d940-4da9-8d1b-0ec4b610bce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the training loop\n",
    "def train(model, train_loader, optimizer, device):\n",
    "    # training mode\n",
    "    model.train(True)\n",
    "    \n",
    "    # Enabling gradient calculation\n",
    "    with torch.set_grad_enabled(True):\n",
    "        collect_loss = 0\n",
    "        correct = 0\n",
    "        nr_samples = 0\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            # send features and labels to GPU/CPU\n",
    "            model_input = data['sample'].to(device)\n",
    "            target = data['label'].to(device)\n",
    "\n",
    "            # zero the gradients\n",
    "            model.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # compute output of model\n",
    "            output = model(model_input)\n",
    "\n",
    "            # compute the loss and update model parameters\n",
    "            loss = my_loss_function(output, target)\n",
    "            loss.backward()\n",
    "\n",
    "            # adjust learning weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # store training loss\n",
    "            collect_loss += loss.item()*batch_size\n",
    "            \n",
    "            # compute accuracy of training data\n",
    "            pred = torch.round(output,decimals=0)\n",
    "            correct += (pred.eq(target.view_as(pred)).sum().item())\n",
    "            nr_samples += len(target)\n",
    "            \n",
    "        return {'train_loss':collect_loss/nr_samples, 'train_accuracy':correct/nr_samples}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dafc6ad-7bbb-4770-9ad8-8877e21b4365",
   "metadata": {},
   "source": [
    "Similar to the training loop, we need a \"test loop\" to get the output of the model for a given set of validation samples on which we do not train the model. \n",
    "1) First, we tell pytorch that we do NOT want to train our model (no keeping track of gradients - evaluation mode). \n",
    "2) We then iterate over our validation data. \n",
    "3) We compute the output of the model for the given input sequences. \n",
    "4) We then compute the loss of the model output for the given target labels of the input sequences.\n",
    "5) We compute the accuracy and training loss for the current samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842a1b3c-8732-48cc-ba0c-af9e13538a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define the test loop\n",
    "def validate(model, test_loader, device):\n",
    "    # Evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        collect_loss = 0\n",
    "        correct = 0\n",
    "        nr_samples = 0\n",
    "        for data in test_loader:\n",
    "            # send features and labels to GPU/CPU\n",
    "            model_input = data['sample'].to(device)\n",
    "            target = data['label'].to(device)\n",
    "            \n",
    "            # compute output of model\n",
    "            output = model(model_input)\n",
    "\n",
    "            # store test loss\n",
    "            collect_loss += my_loss_function(output, target).item()*batch_size\n",
    "            \n",
    "            # compute accuracy for test data\n",
    "            pred = torch.round(output,decimals=0)\n",
    "            correct += (pred.eq(target.view_as(pred)).sum().item())\n",
    "            nr_samples += len(target)\n",
    "            \n",
    "        return {'val_loss':collect_loss/nr_samples, 'val_accuracy':correct/nr_samples}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da12f2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define the number of epochs - how often should the model (my_model) see all of the data (train_loader_human)?\n",
    "n_epochs = 20\n",
    "\n",
    "# initialize an instance of our model class (a variable that is a model following the architecture we defined above)\n",
    "my_model = myPerceptron(dna_lang.n_words, # size of input tensors (the number of codons)\n",
    "                     20, # size of a hidden layer (not used for now)\n",
    "                     1, # size of the model's output\n",
    "                     0 # some additional parameter that could be used (e.g. dropout frequency)\n",
    "                    ).to(device) # send model to device\n",
    "\n",
    "# use stochastic gradient descent with the given learning rate\n",
    "optimizer = torch.optim.Adam(my_model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model on the current data\n",
    "stats_tracker = []\n",
    "for epoch in range(0, n_epochs):\n",
    "    # train the model and get training loss\n",
    "    test_stats = validate(my_model, val_loader_human, device)\n",
    "    train_stats = train(my_model, train_loader_human, optimizer, device)\n",
    "    stats_tracker.append( train_stats|test_stats )\n",
    "    print('epoch: ', epoch, train_stats, test_stats, '\\t\\t\\t\\t\\t\\t\\t\\t', end='\\r')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f776f5f0-19be-49b5-b6f3-01983a522ae5",
   "metadata": {},
   "source": [
    "# Step 7: Plotting error and accuracy\n",
    "Using the output from training, we can plot the results for each epoch to look at the learning of our model. For this, we average the errors over the different folds from training. Carefully look at the training and testing error to choose an appropriate number of epochs for training (to avoid overfitting). \n",
    "\n",
    "NOTE: the initial model included trains very fast with a small error and high accuracy. The error and accuracy become interesting when looking at training on other species as negative samples in the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222bba0e-7699-47fd-aade-13e9513915ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize figure\n",
    "sns.set(rc={'figure.figsize':(5,5)})\n",
    "sns.set(font=\"Arial\")\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# format loss data\n",
    "plot_data = pd.DataFrame(stats_tracker)\n",
    "plot_data = plot_data.stack().reset_index()\n",
    "plot_data.columns = ['epoch','dataset','value']\n",
    "\n",
    "# plot training and test loss as function of epoch\n",
    "ax=sns.lineplot(data=plot_data, x='epoch', y='value',hue='dataset')\n",
    "#ax.set_yscale('log')\n",
    "ax.set_ylim([0,1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034070b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define function for evaluating the trained model for a given sample\n",
    "def evaluate(model, sample):\n",
    "    # set the model in evaluation mode without computing gradients\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # compute the output of the model for a given sample\n",
    "        output = my_model(sample.to(device))\n",
    "    return output.item()\n",
    "\n",
    "\n",
    "# finally, using a trained model, we can compute a 'probability' that a given input sequence is encoding a protein\n",
    "test_sampler = enumerate(test_loader_human)\n",
    "\n",
    "# here we pick a random sequence that was not used for training, but you can change this to any sequence you would like \n",
    "batch_idx, test_sample = next(test_sampler)\n",
    "\n",
    "# evaluate model for given test sample\n",
    "output = evaluate(my_model, test_sample['sample'])\n",
    "\n",
    "# print output of the model together with the label of the sample\n",
    "print('probability: ',output, test_sample['label'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ae809e-65bd-4080-9894-c84966a4d225",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 8: Questions\n",
    "1) how many parameters does your model have?\n",
    "2) change model architecture (e.g., more layers, other parameters, dropout parameters)\n",
    "3) adapt the code to compute the probabilities for mouse and yeast DNA sequences. Having trained the model on human DNA sequences, what is the difference in average probabilities between species? Why?\n",
    "4) train the model using mouse or yeast sequences as negative samples for coding sequences. This creates a model that learns to classify sequences as being likely from mice/yeast or from humans\n",
    "5) what happens if you change the codon length?\n",
    "6) what happens to the probablities when sequences are frameshifted? can you train the model using frameshifted sequences as input?\n",
    "7) train the model on amino acid frequencies instead of codons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f785956a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a88748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561f95e6-cbb1-451f-bbd6-751cf5335818",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
