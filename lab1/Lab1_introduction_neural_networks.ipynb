{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10ad0467-fc31-4586-9d84-fea0948fb7a1",
   "metadata": {},
   "source": [
    "# Lab 1: Introduction to Neural Networks\n",
    "In this tutorial we introduce some of the concepts for working with neural networks using [Pytorch](https://pytorch.org/tutorials/recipes/recipes_index.html). The entire notebook can be executed as-is, given the lack of time for this first lab session. We encourage you to explore the code yourselves to get comfortable with the concepts of deel learning in the context of biology. A few questions at the end challenge you to play around with the code and try things for yourselves.\n",
    "\n",
    "In this session, you will create a simple neural network that classifies any given DNA sequence as protein coding or not. As a starting point, we use as examples the coding DNA sequences from humans (homo sapiens (HS)). As negatives, we use random sequences of DNA where each nucleotide is drawn from a uniform distribution over the possible nucleotides. We then train a neural network on de [codon frequencies](https://en.wikipedia.org/wiki/DNA_and_RNA_codon_tables) of these sequences.\n",
    "\n",
    "In addition to human DNA sequences, we also take a look at coding sequences from mice ([mus musculus (MM)](https://en.wikipedia.org/wiki/House_mouse)) and yeast ([saccharomyces cerevisiae (SC)](https://en.wikipedia.org/wiki/Saccharomyces_cerevisiae)). There are subtle differences between the coding frequencies of these species. You will test how well your human-trained model is able to recover the coding sequences for mice and yeast (think of your results in the context of evolutionary distances between species). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d75f7f4-c7b1-46a8-8637-5cba8634b516",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# import basic functionality\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# libraries for plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import _pickle as pickle\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4f5ede-7ed4-4cfc-bfdd-a9fddddbabbd",
   "metadata": {},
   "source": [
    "# Step 1: Loading the data\n",
    "For your convenience, we provide the pre-processed and encoded sequences. That is, we pre-processed human, mouse and yeast DNA sequences by filtering for coding sequences that contain an integer number of codons, that are of sufficient length for computing codon frequencies (>=300 base pairs), whose translation encodes a protein (start and stop codon). We removed duplicates and randomly mixed the sequences. \n",
    "\n",
    "We then encoded these sequences by computing the codon frequencies for each sequence. Here we load this data set. To train a neural network on de coding frequencies of these sequences, we encoded the sequences by converting each sequence to an array of frequencies for each possible codon. To do so, each codon gets assigned a index in the array. This array allows us to convert between codon (e.g., 'ATG') and indices in the array (e.g., 'ATG' -> 0) to keep track of the codon frequencies. We use Tensors - the datatype used for pytorch data - to store the coding frequencies.\n",
    "\n",
    "Biologically, [DNA codons](https://en.wikipedia.org/wiki/DNA_and_RNA_codon_tables) consist of three nucleotides, encoding amino acids. However, since we are training a neural network to classify a sequence to be protein coding or not, we can choose any number of nucleotides to represent a 'codon'. For example, we can choose a \"codon length\" of a single nucleotide (which would result in us training the model on the [frequencies of nucleotides in DNA](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC403801/)), or a codon length of two nucleotides (no biological meaning as this does not represent a biological unit - we do not expect a model to learn any biology at all), or a codon length of 6 nucleotides (representing pairs of amino acids - would this yield a model that \"learns\" any biology?). You can play around with the yourself in the notebook that creates the codon frequencies, but we start with a codon length of 3 nucleotides - represeting one amino acid. \n",
    "\n",
    "Finally, for each human sequence, we also created a random sequence of codons following the same sequence length distribution as the DNA sequences. These nucleotides are drawn from a random uniform distribution over the possible codons. These random sequences are used as negatives (i.e., label = 1 will tell the model that a sequence is protein coding, label = 0 tells the model that a sequence is not protein coding). \n",
    "\n",
    "Download the data folder from [here](https://polybox.ethz.ch/index.php/s/qXpo8ZcsuHryxY0). If you would like (not required), you can read how the data was processed in \"Lab1_bonus_encoding\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7ebf8e-5ce5-4ed7-95a8-9c92a338aadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load the encoded the sequence data\n",
    "with open('lab1_seq_data_human_encoded_pos.obj', 'rb') as handle:\n",
    "    seq_data_human_encoded_pos = pickle.load(handle)\n",
    "    \n",
    "with open('lab1_seq_data_human_encoded_neg.obj', 'rb') as handle:\n",
    "    seq_data_human_encoded_neg = pickle.load(handle)\n",
    "    \n",
    "with open('seq_data_mouse_encoded.obj', 'rb') as handle:\n",
    "    seq_data_mouse_encoded = pickle.load(handle)\n",
    "    \n",
    "with open('seq_data_yeast_encoded.obj', 'rb') as handle:\n",
    "    seq_data_yeast_encoded = pickle.load(handle)\n",
    "        \n",
    "with open('sequence_encoding.obj', 'rb') as handle:\n",
    "    dna_lang = pickle.load(handle)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c45663b-6414-458a-9773-29c07e3ee98d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# take a look at the encoding\n",
    "dna_lang.word2index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cd3c73-7b3c-4a54-b1c6-86a3d2e659ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# take a look at a sample\n",
    "seq_data_human_encoded_pos[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a0f6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# explore the correlation of coding frequencies across species\n",
    "# first average and merge codon frequencies of the different species and random sequences\n",
    "codon_freqs = pd.DataFrame([np.mean(np.array([ch['frequencies'] for ch in seq_data_human_encoded_pos if ch['label']==1]),axis=0),\n",
    "                         np.mean(np.array([ch['frequencies'] for ch in seq_data_mouse_encoded if ch['label']==1]),axis=0),\n",
    "                         np.mean(np.array([ch['frequencies'] for ch in seq_data_yeast_encoded if ch['label']==1]),axis=0),\n",
    "                         np.mean(np.array([ch['frequencies'] for ch in seq_data_human_encoded_neg if ch['label']==0]),axis=0)\n",
    "                         ]).T\n",
    "\n",
    "# label codons and sort by human frequency\n",
    "codon_freqs.index = [dna_lang.index2word[idx] for idx in list(codon_freqs.index)]\n",
    "codon_freqs.reset_index(inplace=True)\n",
    "codon_freqs.columns = ['codon','human','mouse','yeast','random']\n",
    "codon_freqs.sort_values(by='human',ascending=False,inplace=True)\n",
    "\n",
    "print('correlation matrix: ')\n",
    "print(codon_freqs.set_index('codon').corr())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c8cde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot coding frequencies for different species\n",
    "# initialize figure\n",
    "sns.set(rc={'figure.figsize':(5,15)})\n",
    "sns.set(font=\"Arial\")\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# stack dataframe of frequencies\n",
    "plot_freqs = codon_freqs.set_index('codon').stack().reset_index()\n",
    "plot_freqs.columns = ['codon','origin','frequency']\n",
    "\n",
    "# plot codon frequencies for different species\n",
    "sns.barplot(data=plot_freqs,x='frequency',y='codon',hue='origin')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd187f1-4a50-4aa9-b65a-9662b4d83c19",
   "metadata": {},
   "source": [
    "# Step 2: Creating a dataloader\n",
    "Having encoded our DNA sequences as codon frequencies, we are ready to prepare the data for training a neural network. We will create a 'dataloader' that takes care of splitting the data into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302e2c98-e639-4a55-a89e-b11a30fbb379",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# merge encoded human sequences and the matching random samples\n",
    "seq_data_human_encoded = seq_data_human_encoded_pos + seq_data_human_encoded_neg\n",
    "random.shuffle(seq_data_human_encoded)\n",
    "\n",
    "# split the sequence data that we defined above into training, validation and test sets\n",
    "#train_set_human, val_set_human, test_set_human = torch.utils.data.random_split(seq_data_human_encoded, [0.5,0.4,0.1])\n",
    "#train_set_mouse, val_set_mouse, test_set_mouse = torch.utils.data.random_split(seq_data_mouse_encoded, [0.5,0.4,0.1])\n",
    "#train_set_yeast, val_set_yeast, test_set_yeast = torch.utils.data.random_split(seq_data_yeast_encoded, [0.5,0.4,0.1])\n",
    "\n",
    "train_set_human, val_set_human, test_set_human = torch.utils.data.random_split(seq_data_human_encoded, [int(ch*len(seq_data_human_encoded)) for ch in [0.5,0.4,0.1]])\n",
    "train_set_mouse, val_set_mouse, test_set_mouse = torch.utils.data.random_split(seq_data_mouse_encoded, [int(ch*len(seq_data_mouse_encoded)) for ch in [0.5,0.4,0.1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f334547-e066-4b86-974a-28fab683b9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################\n",
    "# define a function to create a dataloader for the encoded sequences\n",
    "def get_dataloader(dataset, batch_size):\n",
    "    cur_sampler = RandomSampler(dataset)\n",
    "    cur_dataloader = DataLoader(dataset=dataset, sampler=cur_sampler, batch_size=batch_size, drop_last=True, num_workers=16)\n",
    "    return cur_dataloader    \n",
    "############################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d01963f-3aaf-42eb-a6e9-09194556a7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# how many samples should be trained on simultaneously?\n",
    "batch_size = 300\n",
    "\n",
    "# define dataloader for training\n",
    "train_loader_human = get_dataloader(train_set_human, batch_size)\n",
    "val_loader_human = get_dataloader(val_set_human, batch_size)\n",
    "test_loader_human = get_dataloader(test_set_human, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d11caf-2312-44d9-b79b-7fab1a70a3a9",
   "metadata": {},
   "source": [
    "# Step 3: Define model\n",
    "As a final preparation, we define our [model](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). We create a class that instructs pytorch to follow a certain architecture for the model. Our model initializes all relevant parts (init function) and tells pytorch how to compute the output for a given input (forward function). For our perceptron, we use a single dense linear layer with a sigmoidal activation function. You can play around with the model architecture later - several options are left as comments.\n",
    "We are trying to train a model for solving a classfication problem. The labels for our samples are binary (ones and zeros). We therefore use a binary loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc510394",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the device (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957672b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the model architecture\n",
    "class myPerceptron(nn.Module):\n",
    "    def __init__(self, input_param, output_param):\n",
    "        super(myPerceptron, self).__init__()\n",
    "        \n",
    "        self.input_param = input_param\n",
    "        self.output_param = output_param\n",
    "        \n",
    "        self.linear0 = nn.Linear(input_param, output_param)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        output_linear = self.linear0(input_data)\n",
    "        output_activation = self.sigmoid(output_linear)\n",
    "        return output_activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda8ce7f-391d-45c8-afdf-7e1a2731eb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# use binary cross entropy los for this classification problem\n",
    "my_loss_function = nn.BCELoss()\n",
    "\n",
    "# initialize an instance of our model class (a variable that is a model following the architecture we defined above)\n",
    "my_model = myPerceptron(dna_lang.n_words, # size of input tensors (the number of codons)\n",
    "                     1, # size of the model's output\n",
    "                    ).to(device) # send model to device\n",
    "\n",
    "# show model architecture\n",
    "my_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8497c6-3537-47a4-9d87-a65fc4ae284d",
   "metadata": {},
   "source": [
    "# Step 4: Train simple model \n",
    "To train our model, we need a \"training loop\". \n",
    "1) First, we tell pytorch that we want to train our model (so it has to keep track of gradients). \n",
    "2) We then iterate over our data in batches to speed up computations (there is little advantage for computing the gradient with all samples over, say, a few hundred samples). \n",
    "3) We set the gradients to zero (we don't want to re-use previous computations for our next training step).\n",
    "4) We compute the output of the model for the given input sequences. \n",
    "5) We then compute the loss of the model output for the given target labels of the input sequences and [backpropagate](https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html) the loss through the network to compute the gradient. \n",
    "6) Finally, we instruct the optimizer to use the gradient and perform one appropriately-sized [step](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.step.html) to update the model weights. \n",
    "7) To keep track of our effors we compute the accuracy and training loss for the current samples.\n",
    "\n",
    "Finally, we train our model using the given data and training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bd0758-d940-4da9-8d1b-0ec4b610bce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the training loop\n",
    "def train(model, train_loader, optimizer, device):\n",
    "    # training mode\n",
    "    model.train(True)\n",
    "    \n",
    "    # Enabling gradient calculation\n",
    "    with torch.set_grad_enabled(True):\n",
    "        collect_loss = 0\n",
    "        correct = 0\n",
    "        nr_samples = 0\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            # send features and labels to GPU/CPU\n",
    "            model_input = data['frequencies'].to(device)\n",
    "            target = data['label'].to(device)\n",
    "\n",
    "            # zero the gradients\n",
    "            model.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # compute output of model\n",
    "            output = model(model_input)\n",
    "\n",
    "            # compute the loss and update model parameters\n",
    "            loss = my_loss_function(output, target)\n",
    "            loss.backward()\n",
    "\n",
    "            # adjust learning weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # store training loss\n",
    "            collect_loss += loss.item()*batch_size\n",
    "            \n",
    "            # compute accuracy of training data\n",
    "            pred = torch.round(output,decimals=0)\n",
    "            correct += (pred.eq(target.view_as(pred)).sum().item())\n",
    "            nr_samples += len(target)\n",
    "            \n",
    "        return {'train_loss':collect_loss/nr_samples, 'train_accuracy':correct/nr_samples}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dafc6ad-7bbb-4770-9ad8-8877e21b4365",
   "metadata": {},
   "source": [
    "Similar to the training loop, we need a \"test loop\" to get the output of the model for a given set of validation samples on which we do not train the model. \n",
    "1) First, we tell pytorch that we do NOT want to train our model (no keeping track of gradients - evaluation mode). \n",
    "2) We then iterate over our validation data. \n",
    "3) We compute the output of the model for the given input sequences. \n",
    "4) We then compute the loss of the model output for the given target labels of the input sequences.\n",
    "5) We compute the accuracy and training loss for the current samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842a1b3c-8732-48cc-ba0c-af9e13538a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define the test loop\n",
    "def validate(model, test_loader, device):\n",
    "    # Evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        collect_loss = 0\n",
    "        correct = 0\n",
    "        nr_samples = 0\n",
    "        for data in test_loader:\n",
    "            # send features and labels to GPU/CPU\n",
    "            model_input = data['frequencies'].to(device)\n",
    "            target = data['label'].to(device)\n",
    "            \n",
    "            # compute output of model\n",
    "            output = model(model_input)\n",
    "\n",
    "            # store test loss\n",
    "            collect_loss += my_loss_function(output, target).item()*batch_size\n",
    "            \n",
    "            # compute accuracy for test data\n",
    "            pred = torch.round(output,decimals=0)\n",
    "            correct += (pred.eq(target.view_as(pred)).sum().item())\n",
    "            nr_samples += len(target)\n",
    "            \n",
    "        return {'val_loss':collect_loss/nr_samples, 'val_accuracy':correct/nr_samples}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da12f2af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# define the number of epochs - how often should the model (my_model) see all of the data (train_loader_human)?\n",
    "n_epochs = 20\n",
    "\n",
    "# initialize an instance of our model class (a variable that is a model following the architecture we defined above)\n",
    "my_model = myPerceptron(dna_lang.n_words, # size of input tensors (the number of codons)\n",
    "                     1, # size of the model's output\n",
    "                    ).to(device) # send model to device\n",
    "\n",
    "# use stochastic gradient descent with the given learning rate\n",
    "optimizer = torch.optim.Adam(my_model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model on the current data\n",
    "stats_tracker = []\n",
    "for epoch in range(0, n_epochs):\n",
    "    # train the model and get training loss\n",
    "    test_stats = validate(my_model, val_loader_human, device)\n",
    "    train_stats = train(my_model, train_loader_human, optimizer, device)\n",
    "    stats_tracker.append( train_stats|test_stats )\n",
    "    print('epoch: ', epoch, train_stats, test_stats, '\\t\\t\\t\\t\\t\\t\\t\\t', end='\\r')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f776f5f0-19be-49b5-b6f3-01983a522ae5",
   "metadata": {},
   "source": [
    "# Step 5: Plotting error and accuracy\n",
    "Using the output from training, we can plot the results for each epoch to look at the learning of our model. For this, we average the errors over the different folds from training. Carefully look at the training and testing error to choose an appropriate number of epochs for training (to avoid overfitting). \n",
    "\n",
    "NOTE: the initial model included trains very fast with a small error and high accuracy. The error and accuracy become interesting when looking at training on other species as negative samples in the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222bba0e-7699-47fd-aade-13e9513915ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize figure\n",
    "sns.set(rc={'figure.figsize':(5,5)})\n",
    "sns.set(font=\"Arial\")\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# format loss data\n",
    "plot_data = pd.DataFrame(stats_tracker)\n",
    "plot_data = plot_data.stack().reset_index()\n",
    "plot_data.columns = ['epoch','dataset','value']\n",
    "\n",
    "# plot training and test loss as function of epoch\n",
    "ax=sns.lineplot(data=plot_data, x='epoch', y='value',hue='dataset')\n",
    "#ax.set_yscale('log')\n",
    "ax.set_ylim([0,1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034070b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define function for evaluating the trained model for a given sample\n",
    "def evaluate(model, sample):\n",
    "    # set the model in evaluation mode without computing gradients\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # compute the output of the model for a given sample\n",
    "        output = my_model(sample.to(device))\n",
    "    return output.item()\n",
    "\n",
    "\n",
    "# finally, using a trained model, we can compute a 'probability' that a given input sequence is encoding a protein\n",
    "test_sampler = enumerate(test_loader_human)\n",
    "\n",
    "# here we pick a random sequence that was not used for training, but you can change this to any sequence you would like \n",
    "batch_idx, test_sample = next(test_sampler)\n",
    "\n",
    "# evaluate model for given test sample\n",
    "output = evaluate(my_model, test_sample['frequencies'])\n",
    "\n",
    "# print output of the model together with the label of the sample\n",
    "print('probability: ',output, test_sample['label'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ae809e-65bd-4080-9894-c84966a4d225",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 6: Questions\n",
    "1) how many parameters does your model have?\n",
    "3) change model architecture, you can add more layers with hidden parameters, other activation functions, dropout parameters, etc. Make sure that the model does not overfit.\n",
    "4) make predictions for mouse sequences using the model trained for human sequences. What changes for the accuracy? What happens if you make predictions for yeast sequences?\n",
    "5) now train the model to use yeast sequences as negative samples for coding sequences. And then for mouse sequences as negative samples. This creates a model that learns to classify sequences as being likely from mice/yeast or from humans. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3723679-9818-4f01-a310-eeea082afbd6",
   "metadata": {},
   "source": [
    "# Step 7: Bonus (hard - use encoding notebook)\n",
    "6) what happens if you change the codon length?\n",
    "7) what happens to the probablities when sequences are frameshifted? can you train the model using frameshifted sequences as input?\n",
    "8) train the model on amino acid frequencies instead of codons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbeede5-3ea0-4977-93e7-9837ff0b92a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a88748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561f95e6-cbb1-451f-bbd6-751cf5335818",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
