{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8d781c1-022f-4af6-8ba3-4da21a9544cf",
   "metadata": {},
   "source": [
    "# Lab 3: Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01b68ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### step2a: create languages for DNA and protein sequences\n",
    "########################\n",
    "\n",
    "# create a language for DNA and protein sequences\n",
    "dna_lang = Language(name=\"dna\", codon_len=3)\n",
    "prot_lang = Language(name=\"prot\", codon_len=1)\n",
    "\n",
    "# split the sequence data ('seq_data') that we defined above into sensible training, validation and test sets\n",
    "# think about how much data would realistically be necessary to learn the problem of translating DNA sequences\n",
    "train_set, val_set, test_set, _ = torch.utils.data.random_split(seq_data, [0.1,0.1,0.1,0.7])\n",
    "\n",
    "# memorize the dna and protein languages by parsing all sequences\n",
    "for cur_seq in train_set:\n",
    "    dna_lang.addSentence(cur_seq['dna'])\n",
    "    prot_lang.addSentence(cur_seq['prot'])\n",
    "\n",
    "# create an one-hot-encoding for all words codons and a simple encoding for all amino acids\n",
    "# call the appropriate functions for each of the two languages\n",
    "dna_lang.as_one_hot()\n",
    "prot_lang.as_one_hot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6d24eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### step2b: encode your sequences here\n",
    "########################\n",
    "\n",
    "# define maximum number of codons\n",
    "# we truncate any sequence longer than this length, and pad any sequence shorter than this length\n",
    "# think about a sensible length for the input sequences\n",
    "max_length = None\n",
    "\n",
    "# encode the training and validation data\n",
    "train_set_encoded = encode_dataset(train_set, dna_lang, prot_lang, max_length) \n",
    "val_set_encoded = encode_dataset(val_set, dna_lang, prot_lang, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03de687-77f7-442c-8e89-75039b83137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### step 2c: create a dataloader for the validation and training sequences\n",
    "########################\n",
    "\n",
    "# how many samples should be trained on simultaneously?\n",
    "batch_size = 1\n",
    "\n",
    "# define dataloader for training\n",
    "train_loader = get_dataloader(train_set_encoded, batch_size)\n",
    "val_loader = get_dataloader(val_set_encoded, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030f08f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### step 3a: define the model architecture\n",
    "########################\n",
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MyRNN,self).__init__()\n",
    "        \n",
    "        # input parameters\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # define model layers (rnn), pseudocode:\n",
    "#        self.rnn = nn.LSTM(self.input_size, self.output_size, num_layers=1, batch_first = True, bias=False)\n",
    "        self.rnn = nn.RNN(self.input_size, self.output_size, num_layers=1, batch_first = True, bias=False)\n",
    "\n",
    "    def forward(self,inp):\n",
    "        inp1 = inp.to(device)\n",
    "        \n",
    "        # define initial hidden and cell states of rnn, e.g.:\n",
    "        h0 = torch.randn(1, inp1.size(0), self.output_size).to(inp.device)\n",
    "#        c0 = torch.randn(1, inp1.size(0), self.output_size).to(inp.device)\n",
    "\n",
    "        # run LSTM, pseudocode:\n",
    "#        output_rnn, (hn,cn) = self.rnn(inp1, (h0,c0))\n",
    "        output_rnn, (hn) = self.rnn(inp1, (h0))\n",
    "\n",
    "        return output_rnn#[:,2::3] # when using nucleotides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c898b30-16e4-45ff-9bc3-a5d61253b139",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### step 3b: define the lightning module to train the model\n",
    "########################\n",
    "\n",
    "# lightning module to train the sequence model\n",
    "class SequenceModelLightning(L.LightningModule):\n",
    "    def __init__(self, input_size, hidden_size, output_size, lr=0.1):\n",
    "        super().__init__()\n",
    "        self.model = MyRNN(input_size, hidden_size, output_size)\n",
    "        self.lr = lr\n",
    "\n",
    "        # define loss function here, pseudocode:\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_tensor = batch['dna']\n",
    "        target_tensor = batch['prot']\n",
    "        \n",
    "        output = self.model(input_tensor)\n",
    "        loss = self.loss(input=output, target=target_tensor)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_tensor = batch['dna']\n",
    "        target_tensor = batch['prot']\n",
    "\n",
    "        output = self.model(input_tensor)\n",
    "        loss = self.loss(input=output, target=target_tensor)\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # define optimizer here\n",
    "        return optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a47bdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### step3c: define the input parameters for the training loop\n",
    "########################\n",
    "\n",
    "# define the model and training loop\n",
    "# think of the dimensionality of your input data (dna sequences) and output data (protein sequence), and where these numbers are stored\n",
    "lit_model = SequenceModelLightning(input_size = dna_lang.n_words,\n",
    "                                  hidden_size = 0,#prot_lang.n_words,\n",
    "                                  output_size = prot_lang.n_words,\n",
    "                                  lr = 0.05)\n",
    "\n",
    "# define the trainer\n",
    "trainer = L.Trainer(devices = 1, \n",
    "                    max_epochs = 10)\n",
    "\n",
    "# learn the weights of the model\n",
    "trainer.fit(lit_model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590813ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### step4: define the input tensor and get the prediction from your model\n",
    "########################\n",
    "\n",
    "# pick a random sequence from the test set\n",
    "random_pair = np.random.randint(0,len(test_set))\n",
    "\n",
    "# get the encoded dna sequence and its known protein translation\n",
    "dna_sequence = np.array([test_set_encoded[random_pair]['dna']])\n",
    "protein_translation = test_set[random_pair]['prot']\n",
    "target_tensor = test_set_encoded[random_pair]['prot']\n",
    "\n",
    "# send model and input sequence to device, compute translation of sequence\n",
    "my_rnn.to(device)\n",
    "input_tensor = torch.Tensor(dna_sequence).to(device)\n",
    "output = my_rnn(input_tensor)\n",
    "\n",
    "loss = ((target_tensor - output.cpu())**2).mean()\n",
    "print('loss', loss)\n",
    "\n",
    "# convert output back to protein sequence by taking the most likely amino acid per position, print results\n",
    "result = \"\".join([prot_lang.index2word[i] for i in output.cpu().topk(1)[1].view(-1).numpy()])\n",
    "print(''+protein_translation)\n",
    "print(result, end='\\n\\n')\n",
    "\n",
    "# print accuracy\n",
    "result = \"\".join([prot_lang.index2word[i] for i in output.cpu().topk(1)[1].view(-1).numpy() if i not in [key for key in Language('',1).index2word]])\n",
    "min_len = np.min([len(result),len(protein_translation)])\n",
    "print('Accuracy of aa calling over the sequence: ', np.sum([protein_translation[i] == result[i] for i in range(min_len)])/min_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e34b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### step5: interpret the hidden state of your RNN\n",
    "########################\n",
    "\n",
    "# load the hidden state from your RNN.\n",
    "# annotate the hidden state with the matching codons (in 'dna_lang.index2word') and amino acids (in 'prot_lang.index2word')\n",
    "# store the result in a dataframe called 'rnn_param'\n",
    "rnn_param = pd.DataFrame(next(my_rnn.rnn.parameters()).detach().numpy()).T\n",
    "rnn_param.index = list(dna_lang.index2word.values())\n",
    "rnn_param.columns = list(prot_lang.index2word.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c1378d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecbf1a7-63a8-4d5f-8abb-106e5e6c240f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c4f401-ce2c-4a4b-8968-77e288b49115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e2a7c7b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
