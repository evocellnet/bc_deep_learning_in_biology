{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8d781c1-022f-4af6-8ba3-4da21a9544cf",
   "metadata": {},
   "source": [
    "# Lab 3a: LSTM\n",
    "In this tutorial we create neural networks using [Long Short-Term Memory (LSTM)](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) cells. The data and its pre-processing for of this notebook is identical to the first lab (Perceptron) to keep the amount of new information limited. Some of the required code-blocks are empty - requiring your imput to complete the model. A few additional questions at the end challenge you to play around with the code and try things for yourselves.\n",
    "\n",
    "During the session, you will create a LSTM to translate DNA sequences into protein sequences. \n",
    "classifies any given DNA sequence as protein coding or not. As a starting point, we use as examples the coding DNA sequences from humans (homo sapiens (HS)). As negatives, we use random sequences of DNA where each nucleotide is drawn from a uniform distribution over the possible nucleotides. We then train a neural network on de [codon frequencies](https://en.wikipedia.org/wiki/DNA_and_RNA_codon_tables) of these sequences.\n",
    "\n",
    "In addition to human DNA sequences, we also take a look at coding sequences from mice ([mus musculus (MM)](https://en.wikipedia.org/wiki/House_mouse)) and yeast ([saccharomyces cerevisiae (SC)](https://en.wikipedia.org/wiki/Saccharomyces_cerevisiae)). There are subtle differences between the coding frequencies of these species. You will test how well your human-trained model is able to recover the coding sequences for mice and yeast (think of your results in the context of evolutionary distances between species). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c25dd79e-1645-44c5-84ce-c9b0f540de5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "import lightning as L\n",
    "\n",
    "# import basic functionality\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "# libraries for plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0814c75e-4c3e-4d74-bcca-6adfba516948",
   "metadata": {},
   "source": [
    "# Step 1: Pre-processing the data\n",
    "Here we download and pre-process the dataset. As before, we only consider DNA sequences that are protein coding, contain a integer number of codons, have a start and stop codon, and do not contain any uncertain nucleotides. Finally, we remove duplicates and randomly mix the sequences. Nothing is different from the last time, so you can simply execute these steps and move on to Step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3fb6e8-f264-4584-b4d2-f8462239cc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# download and unpack DNA coding sequences for human, mouse and yeast\n",
    "############################\n",
    "\n",
    "!mkdir -p ~/all_seqs\n",
    "%cd ~/\n",
    "\n",
    "!wget -P ~/all_seqs/ https://ftp.ensembl.org/pub/current_fasta/homo_sapiens/cds/Homo_sapiens.GRCh38.cds.all.fa.gz\n",
    "!gzip -df \"all_seqs/Homo_sapiens.GRCh38.cds.all.fa.gz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350c0eeb-b2da-4ae8-a5cf-4149a018f7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# function that loads and processes a FASTA file containing coding sequences\n",
    "def load_species_cds(file_name):\n",
    "    train_cds = []\n",
    "    train_prot = []\n",
    "    for record in SeqIO.parse(file_name, \"fasta\"):\n",
    "        # ensure that sequences are protein coding\n",
    "        if 'gene_biotype:protein_coding' in record.description:\n",
    "            if 'transcript_biotype:protein_coding' in record.description:\n",
    "                if ' cds ' in record.description:\n",
    "                    if len(record.seq) % 3 == 0:\n",
    "                        train_cds.append(str(record.seq))\n",
    "                        train_prot.append(str(record.seq.translate()))\n",
    "                        \n",
    "    # keep sequences that are protein coding\n",
    "    train_cds_filtered = []\n",
    "    train_prot_filtered = []\n",
    "    for i in range(len(train_prot)):\n",
    "        if (train_prot[i][0]=='M') & (train_prot[i][-1]=='*'):\n",
    "            train_cds_filtered.append(train_cds[i])\n",
    "            train_prot_filtered.append(train_prot[i])\n",
    "\n",
    "    # avoid sequences with undetermined/uncertain nucleotides\n",
    "    train_cds_filtered = [train_cds_filtered[i] for i in range(len(train_cds_filtered)) if ('N' not in train_cds_filtered[i])]\n",
    "    train_prot_filtered = [train_prot_filtered[i] for i in range(len(train_cds_filtered)) if ('N' not in train_cds_filtered[i])]\n",
    " \n",
    "    # remove duplicates and randomly mix the list of sequences\n",
    "    seqs = list(zip(train_cds_filtered, train_prot_filtered))\n",
    "    seqs = list(set(seqs))\n",
    "    random.shuffle(seqs)\n",
    "    train_cds_filtered, train_prot_filtered = zip(*seqs)\n",
    "    \n",
    "    return list(train_cds_filtered), list(train_prot_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4ce519-1289-42de-9007-7a1690110b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load coding sequences for different species\n",
    "print('loading human proteins')\n",
    "#max_len_cds = 100\n",
    "DNAseq, ProtSeq = load_species_cds(\"all_seqs/Homo_sapiens.GRCh38.cds.all.fa\")\n",
    "\n",
    "# take a look at some sequences\n",
    "[ProtSeq[i][0:40]+'...' for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a350a-500e-43fd-8bd5-2becb21d0965",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('number of sequences: ', len(ProtSeq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e6e124-b7f8-4a62-bcbe-f2275cf2bb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nr_test_seqs = 100\n",
    "\n",
    "# we'll the 100 first sequences as test sequences\n",
    "test_DNAseq = DNAseq[0:nr_test_seqs]\n",
    "test_ProtSeq = ProtSeq[0:nr_test_seqs]\n",
    "\n",
    "# use the remaining sequences as training data\n",
    "DNAseq = DNAseq[nr_test_seqs:]\n",
    "ProtSeq = ProtSeq[nr_test_seqs:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076ce85a-62cb-484c-a4d2-4acdb208b4b2",
   "metadata": {},
   "source": [
    "# Step 2: Encoding the sequences\n",
    "Having prepared the coding sequences and their translation, we convert them into a numeric representation as vectors. To do so, we first construct a language class that stores words of each language and allows us to convert between encoding/indices and words in a language. Next, we define a function that allows us to store any sequence of words (i.e., codons or bases) as a numeric representation. Here we extend every sequence with a start of sentence <SOS> and end of sentence <EOS> token, such that the model knows when to start and stop translating. Finally, we extend every sentence to the same length by padding with an empty \"word\" that is not translated or used, but allows us to use the identical-length numerical representation of the sentences as input for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4050adab-da35-4577-a3bb-3b336caf1a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to store a language\n",
    "class Lang:\n",
    "    # initialize the language, as standard we have start of sentence (SOS), end of sentence (EOS) and a padding to equalize sentence lengths (PAD)\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2}\n",
    "        self.index2word = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\"}\n",
    "        self.n_words = 3  # Count SOS and EOS\n",
    "\n",
    "    # function to add sentence to language (add new words in the sentence to our language)\n",
    "    def addSentence(self, sentence, codon_length):\n",
    "        for word in [sentence[i:i+codon_length] for i in range(0, len(sentence), codon_length)]:\n",
    "            self.addWord(word)\n",
    "\n",
    "    # function to add word to language\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "            \n",
    "    # function to convert indices to encodings (i.e., 3 becomes [0,0,0,1,0,0,...])\n",
    "    def to_encoding(self):\n",
    "        for key in self.word2index:\n",
    "            new_val = np.zeros(len(self.word2index),dtype=np.int32)\n",
    "            new_val[self.word2index[key]] = 1\n",
    "            self.word2index[key] = new_val\n",
    "    \n",
    "\n",
    "# function to encode and pad a sentence\n",
    "# we use this to take an input sentence and convert it to a sequence of arrays that represent that sentence in a given language\n",
    "# in the context of proteins, think of this as encoding the bases or codons\n",
    "def encode_and_pad(language, sentence, codon_length, max_length):\n",
    "    sos = [language.word2index[\"<SOS>\"]]\n",
    "    eos = [language.word2index[\"<EOS>\"]]\n",
    "    pad = [language.word2index[\"<PAD>\"]]\n",
    "\n",
    "    # split sentence in blocks of a given codon_length\n",
    "    sentence_split = [sentence[i:i+codon_length] for i in range(0, len(sentence), codon_length)]\n",
    "        \n",
    "    # encode sentence in the given language\n",
    "    sentence_encoded = [language.word2index[word] for word in sentence_split]\n",
    "    if len(sentence_split) < max_length - 2: \n",
    "        # sentence is shorter than max length -2; add SOS and EOS and pad to maximum length\n",
    "        n_pads = max_length - 2 - len(sentence_split)\n",
    "        return np.array(sos + sentence_encoded   + eos + pad * n_pads)\n",
    "    else: \n",
    "        # sentence is longer than max length; truncate and add SOS and EOS\n",
    "        sentence_truncated = sentence_encoded[:max_length - 2]\n",
    "        return np.array(sos + sentence_truncated + eos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd7e702-514b-4bc6-a1f9-c6ba9f027665",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create languages for both the protein and DNA sequences\n",
    "dna_lang = Lang(\"dna_seq\")\n",
    "prot_lang = Lang(\"prot_seq\")\n",
    "\n",
    "# define codon length\n",
    "codon_length = 3\n",
    "\n",
    "# add all sequences to the languages\n",
    "for i in range(len(DNAseq)):\n",
    "    dna_lang.addSentence(DNAseq[i], codon_length)\n",
    "    prot_lang.addSentence(ProtSeq[i], 1)\n",
    "\n",
    "# convert the DNA sequence to an encoding\n",
    "# that is, we create a vector-representation of the input sequences for the model\n",
    "dna_lang.to_encoding()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c7cf40-69a6-417b-8a13-2dc0c6fc2587",
   "metadata": {},
   "source": [
    "\n",
    "As example, we encode here the first six bases and the first 10 amino acids of the first sequence in our data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03de687-77f7-442c-8e89-75039b83137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the encoding of a DNA sequence\n",
    "DNAseq[0][0:6], encode_and_pad(dna_lang, DNAseq[0], codon_length, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1ecc13-1767-4997-a7f0-b295a5e8a413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the encoding of a protein sequence\n",
    "ProtSeq[0][0:10], encode_and_pad(prot_lang, ProtSeq[0], 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbab684-4766-4b87-9a78-f3d6de7b8ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### encode your sequences here\n",
    "### put the results in the variables 'DNAseq_encoded' and 'ProtSeq_encoded'\n",
    "### use then encode function defined above\n",
    "########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ed7fd8-d2f6-473a-8899-ef27021e3060",
   "metadata": {},
   "outputs": [],
   "source": [
    "#take a look at your encoded DNA sequences\n",
    "DNAseq_encoded[0:3], DNAseq_encoded[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5a700c-aabe-441f-b5d1-07f66c04e52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#take a look at your encoded protein sequences\n",
    "ProtSeq_encoded[0:3], ProtSeq_encoded[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f248f0d0-cae4-4922-ba09-f84658a11cf9",
   "metadata": {},
   "source": [
    "We have now encoded all DNA sequences in the same format: an array where each of the elements encodes one of the possible codons + the SOS, EOS, and PAD elements, each codon in the sequence is then represented by an array (thus, each sequence is now a matrix of (codons in sequence x possible codons). Analogously, for the protein sequences, each element encodes one of the codons in the sequence (by its index from our protein language). Thus, we converted the DNA sequences (variable-length text) into a consistent format (matrices of numbers with fixed length). These variables can now be used as input for a model. Finally, we create the dataloaders for the encoded sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c39ccb-62b2-44b8-8413-83edb84a494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define dataloader for the encoded sequences\n",
    "def get_dataloader(dna_encoded, prot_encoded, batch_size):\n",
    "    train_x = np.array(dna_encoded)\n",
    "    train_y = np.array(prot_encoded)\n",
    "    train_data = TensorDataset(torch.from_numpy(train_x).double(), torch.from_numpy(train_y).double())\n",
    "\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, drop_last=True)\n",
    "    return train_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4277c89-c68b-4a62-8954-9d3ad428c13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### create a dataloader for the test and training sequences\n",
    "### put the results in the variables 'train_dl' and 'val_dl'\n",
    "### use then dataloader function defined above\n",
    "### think about batch sizes and the number of training and validation samples\n",
    "########################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d563e8-254f-495e-9a96-ffbfc60e54c3",
   "metadata": {},
   "source": [
    "# Step 3: Define model\n",
    "As a final preparation, we define our [model](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html). \n",
    "Create a class that instructs pytorch to make a LSTM model using the given input, hidden size and output parameters. You'll have to define the init and forward functions. Additionally, in the Pytorch Lightning class, you must choose a loss function and optimizer that are appropriate for the problem you are trying to solve. Once you have thought about your model, we will go over the architecture together with the class. Hint: for the LSTM, you'll need to also define the hidden and cell states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0dacb684-577f-4acc-91c0-11ccf6decd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the device (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c898b30-16e4-45ff-9bc3-a5d61253b139",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### create the model architecture\n",
    "########################\n",
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MyLSTM,self).__init__()\n",
    "        \n",
    "        # save input parameters\n",
    "        # define model layers (LSTM) (pseudocode:)\n",
    "        self.LSTM = nn.LSTM(input_1, ..., input_k, batch_first = True)\n",
    "        \n",
    "    def forward(self,inp):\n",
    "        inp = inp.to(device)\n",
    "        \n",
    "        # define initial hidden and cell states of LSTM\n",
    "        # run LSTM (pseudocode:)\n",
    "        output_1, ..., output_n = self.LSTM(input_1, ..., input_m) \n",
    "        \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e930bbd3-ba4b-4963-915f-8d494ff7870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### define the loss function and optimizer\n",
    "########################\n",
    "\n",
    "# lightning module to train the sequence model\n",
    "class SequenceModelLightning(L.LightningModule):\n",
    "    def __init__(self, input_size, hidden_size, output_size, lr=0.1):\n",
    "        super().__init__()\n",
    "        self.model = MyLSTM(input_size, hidden_size, output_size).double()\n",
    "        self.lr = lr\n",
    "        # define loss function here (pseudocode:)\n",
    "        self.loss = nn.MyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_tensor = batch[0].double()\n",
    "        target_tensor = batch[1].double()\n",
    "\n",
    "        output = self.model(input_tensor)\n",
    "        loss = self.loss(output.view(-1, output.shape[2]),target_tensor.view(-1).long())\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_tensor = batch[0].double()\n",
    "        target_tensor = batch[1].double()\n",
    "\n",
    "        output = self.model(input_tensor)\n",
    "        loss = self.loss(output.view(-1, output.shape[2]),target_tensor.view(-1).long())\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # define optimizer here\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31037253-b195-48d9-9a8d-5949ea73e229",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### define the input parameters for the training loop\n",
    "########################\n",
    "\n",
    "# define the model and training loop\n",
    "lit_model = SequenceModelLightning(input_size = ...,\n",
    "                                  hidden_size = ...,\n",
    "                                  output_size = ...,\n",
    "                                  lr = ...)\n",
    "\n",
    "# define the trainer\n",
    "trainer = L.Trainer(devices = 1, \n",
    "                    max_epochs = ...)\n",
    "\n",
    "# learn the weights of the model\n",
    "trainer.fit(lit_model, train_dl, val_dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4678abab-c139-4349-98c3-8fdbae185664",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# performance of model on validation data\n",
    "trainer.validate(lit_model, val_dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8966ad5f-f4f5-495c-953c-af6c696b6b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print model architecture\n",
    "mmm = lit_model.model\n",
    "mmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f54f51-08c2-454c-896a-89db621a39c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%reload_ext tensorboard\n",
    "#%tensorboard --logdir=lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436053e7-f5ac-4250-acb0-a101bcffbfdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cd6ef5b-8de7-400a-b7bd-e62130182ec7",
   "metadata": {},
   "source": [
    "# Step 4: Test the model\n",
    "Finally, we test the model on a random sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c98e35-de82-402d-a555-e6528908ab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### encode your test sequences here\n",
    "### put the results in the variables 'test_DNAseq_encoded' and 'test_ProtSeq_encoded'\n",
    "### use then encode function defined before\n",
    "########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facfb381-1fa3-45d9-93e6-2975fc09136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a random sequence\n",
    "random_pair = np.random.randint(0,100)\n",
    "\n",
    "# send input sequence to device\n",
    "input_tensor = torch.tensor([test_DNAseq_encoded[random_pair]]).double()\n",
    "protein_translation = test_ProtSeq[random_pair]\n",
    "\n",
    "# compute translation of sequence\n",
    "output = mmm(input_tensor)\n",
    "\n",
    "# convert output back to protein sequence by taking the most likely amino acid per position and skipping SOS, EOS and PAD\n",
    "result = \"\".join([prot_lang.index2word[i] for i in output.cpu().topk(1)[1].view(-1).numpy()]).split('<PAD><PAD><PAD>')[0]# if i not in [key for key in Lang('').index2word]])\n",
    "\n",
    "# print sequences\n",
    "min_len = np.min([len(result),len(protein_translation)])\n",
    "print('     '+protein_translation)\n",
    "print(result,end='\\n\\n')\n",
    "\n",
    "# print accuracy\n",
    "result = \"\".join([prot_lang.index2word[i] for i in output.cpu().topk(1)[1].view(-1).numpy() if i not in [key for key in Lang('').index2word]])\n",
    "print('Accuracy of aa calling over the sequence: ', np.sum([protein_translation[i] == result[i] for i in range(min_len)])/min_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0981196-e4ed-43bf-a98f-5cc29905f974",
   "metadata": {},
   "source": [
    "# Step 5: Questions\n",
    "1) Encode the training and test sequences\n",
    "2) Define a LSTM and loss function\n",
    "3) Can you achieve a perfect accuracy on arbitrary-length sequences?\n",
    "4) What is the minimum model size to reach a 'good' accuracy?\n",
    "4) Can you train the model and translate DNA without using the SOS / EOS tokens?\n",
    "5) Can you modify the code to train it on a single DNA sequence, and achieve reasonable accuracy?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
