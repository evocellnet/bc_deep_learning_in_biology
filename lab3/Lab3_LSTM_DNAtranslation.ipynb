{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8d781c1-022f-4af6-8ba3-4da21a9544cf",
   "metadata": {},
   "source": [
    "# Lab 3: RNNs and LSTMs\n",
    "In this tutorial we create neural networks using [Recurrent Neural Networks (RNN)](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) and [Long Short-Term Memory (LSTM)](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) cells. The data and its pre-processing for of this notebook is identical to the first lab (Perceptron) to keep the amount of new information limited. Some of the required code-blocks are empty - requiring your imput to complete the model. A few additional questions at the end challenge you to play around with the code and try things for yourselves.\n",
    "\n",
    "During the session, you will create a RNN to translate DNA sequences into protein sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25dd79e-1645-44c5-84ce-c9b0f540de5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "import lightning as L\n",
    "\n",
    "# import basic functionality\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "# libraries for plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0814c75e-4c3e-4d74-bcca-6adfba516948",
   "metadata": {},
   "source": [
    "# Step 1: Pre-processing the data\n",
    "Here we download and pre-process the dataset. As before, we only consider DNA sequences that are protein coding, contain a integer number of codons, have a start and stop codon, and do not contain any uncertain nucleotides. Finally, we remove duplicates and randomly mix the sequences. Nothing is different from the last time, so you can simply execute these steps and move on to Step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3fb6e8-f264-4584-b4d2-f8462239cc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# download and unpack DNA coding sequences for human, mouse and yeast\n",
    "############################\n",
    "\n",
    "!mkdir -p all_seqs\n",
    "!curl https://ftp.ensembl.org/pub/current_fasta/homo_sapiens/cds/Homo_sapiens.GRCh38.cds.all.fa.gz > all_seqs/Homo_sapiens.GRCh38.cds.all.fa.gz\n",
    "!gzip -df \"all_seqs/Homo_sapiens.GRCh38.cds.all.fa.gz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350c0eeb-b2da-4ae8-a5cf-4149a018f7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# function that loads and processes a FASTA file containing coding sequences\n",
    "def load_species_cds(file_name):\n",
    "    dna_seq = []\n",
    "    prot_seq = []\n",
    "    for record in SeqIO.parse(file_name, \"fasta\"):\n",
    "        # ensure that sequences are protein coding\n",
    "        if 'gene_biotype:protein_coding' in record.description:\n",
    "            if 'transcript_biotype:protein_coding' in record.description:\n",
    "                if ' cds ' in record.description:\n",
    "                    if len(record.seq) % 3 == 0:\n",
    "                        dna_seq.append(str(record.seq))\n",
    "                        prot_seq.append(str(record.seq.translate()))\n",
    "                        \n",
    "    # keep sequences that are protein coding\n",
    "    dna_seq_cod = []\n",
    "    prot_seq_cod = []\n",
    "    for i in range(len(prot_seq)):\n",
    "        if (prot_seq[i][0]=='M') & (prot_seq[i][-1]=='*'):\n",
    "            dna_seq_cod.append(dna_seq[i])\n",
    "            prot_seq_cod.append(prot_seq[i])\n",
    "\n",
    "    # avoid sequences with undetermined/uncertain nucleotides\n",
    "    dna_seq_cod = [dna_seq_cod[i] for i in range(len(dna_seq_cod)) if ('N' not in dna_seq_cod[i])]\n",
    "    prot_seq_cod = [prot_seq_cod[i] for i in range(len(dna_seq_cod)) if ('N' not in dna_seq_cod[i])]\n",
    " \n",
    "    # remove duplicates and randomly mix the list of sequences\n",
    "    seqs = list(zip(dna_seq_cod, prot_seq_cod))\n",
    "    seqs = list(set(seqs))\n",
    "    random.shuffle(seqs)\n",
    "    dna_seq_cod, prot_seq_cod = zip(*seqs)\n",
    "\n",
    "    # pack samples as a list of dictionaries and return result\n",
    "    seq_data = [{'dna':dna_seq_cod[i],'prot':prot_seq_cod[i]} for i in range(len(dna_seq_cod))]\n",
    "    return seq_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4ce519-1289-42de-9007-7a1690110b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load coding sequences for different species\n",
    "print('loading human proteins')\n",
    "seq_data = load_species_cds(\"all_seqs/Homo_sapiens.GRCh38.cds.all.fa\")\n",
    "\n",
    "# take a look at some sequences\n",
    "[seq_data[i]['dna'][0:20]+'...'+seq_data[i]['dna'][-20:] for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a350a-500e-43fd-8bd5-2becb21d0965",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('number of sequences: ', len(seq_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076ce85a-62cb-484c-a4d2-4acdb208b4b2",
   "metadata": {},
   "source": [
    "# Step 2: Encoding the sequences\n",
    "Having prepared the coding sequences and their translation, we convert them into a numeric representation as vectors. To do so, we first construct a language class that stores words of each language and allows us to convert between encoding/indices and words in a language. We define a function that allows us to store any sequence of words (i.e., codons or bases) as a numeric representation. Here we extend every sequence with a start of sentence <SOS> and end of sentence <EOS> token, such that the model knows when to start and stop translating. Finally, we can extend every sentence to the same length by padding with an empty \"word\" that is not translated or used, but allows us to use the identical-length numerical representation of the sentences as input for the model. The language allows for a simple encoding of words as numbers or as numerical vectors (one-hot-encoding). The 'encode' function converts an input sentence to the specified encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4050adab-da35-4577-a3bb-3b336caf1a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class to store a language\n",
    "class Language:\n",
    "    # initialize the language, as standard we have a padding to equalize sentence lengths (PAD)\n",
    "    def __init__(self, name, codon_len):\n",
    "        self.name = name\n",
    "        self.word2index = {\"<PAD>\": 0}\n",
    "        self.encoding = {}\n",
    "        self.index2word = {0: \"<PAD>\"}\n",
    "        self.n_words = 1  # Count SOS and EOS\n",
    "        self.codon_length = codon_len\n",
    "\n",
    "    # function to add sentence to language (add new words in the sentence to our language)\n",
    "    def addSentence(self, sentence):\n",
    "        for word in [sentence[i:i+self.codon_length] for i in range(0, len(sentence), self.codon_length)]:\n",
    "            self.addWord(word)\n",
    "\n",
    "    # function to add word to language\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "            \n",
    "    # function to convert indices to one-hot encodings (i.e., 3 becomes [0,0,0,1,0,0,...])\n",
    "    def as_one_hot(self):\n",
    "        for key in self.word2index:\n",
    "            new_val = np.zeros(len(self.word2index),dtype=np.int32)\n",
    "            new_val[self.word2index[key]] = 1\n",
    "            self.encoding[key] = new_val\n",
    "    \n",
    "    # function to convert indices to simple encodings\n",
    "    def as_encoding(self):\n",
    "        self.encoding = self.word2index\n",
    "\n",
    "    # function to encode (and pad) a sentence\n",
    "    # we use this to take an input sentence and convert it to a sequence of arrays that represent that sentence in a given language\n",
    "    # in the context of proteins, think of this as encoding the bases or codons\n",
    "    def encode(self, sentence, max_len):\n",
    "        pad = [self.encoding[\"<PAD>\"]]\n",
    "\n",
    "        # split sentence in blocks of a given codon_length\n",
    "        sentence_split = [sentence[i:i+self.codon_length] for i in range(0, len(sentence), self.codon_length)]\n",
    "            \n",
    "        # encode sentence in the given language\n",
    "        sentence_encoded = [self.encoding[word] for word in sentence_split]\n",
    "\n",
    "        # only pad or truncate if a maximum length is specified\n",
    "        if max_len is not None:\n",
    "            if len(sentence_split) < max_len: \n",
    "                # sentence is shorter than max length; pad to maximum length\n",
    "                n_pads = max_len - len(sentence_split)\n",
    "                return torch.Tensor(np.array(sentence_encoded + pad * n_pads))\n",
    "            else: \n",
    "                # sentence is longer than max length; truncate\n",
    "                sentence_truncated = sentence_encoded[:max_len]\n",
    "                return torch.Tensor(np.array(sentence_truncated))\n",
    "        else:\n",
    "            return torch.Tensor(np.array(sentence_encoded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01b68ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### step2a: create languages for DNA and protein sequences\n",
    "########################\n",
    "\n",
    "# create a language for DNA and protein sequences\n",
    "dna_lang = Language(name=\"dna\", codon_len = 3) # IMPORTANT: here we set the codon length to 3!\n",
    "prot_lang = Language(name=\"prot\", codon_len = 1)\n",
    "\n",
    "# split the sequence data ('seq_data') that we defined above into sensible training, validation and test sets\n",
    "# think about how much data would realistically be necessary to learn the problem of translating DNA sequences\n",
    "train_set, val_set, test_set, _ = torch.utils.data.random_split(..., ...)\n",
    "\n",
    "# memorize the dna and protein languages by parsing all sequences\n",
    "for cur_seq in train_set:\n",
    "    dna_lang.addSentence(cur_seq['dna'])\n",
    "    prot_lang.addSentence(cur_seq['prot'])\n",
    "\n",
    "# create an one-hot-encoding for all words codons and a simple encoding for all amino acids\n",
    "# call the appropriate functions for each of the two languages\n",
    "dna_lang.FUNCTION_CALL()\n",
    "prot_lang.FUNCTION_CALL()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8fd9a7-1368-4dd7-b4a8-7b41d96e096b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4eeb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# here we define a function for encoding a dataset of dna and protein sequences\n",
    "def encode_dataset(dataset, dna_lang, prot_lang, max_length):\n",
    "    dataset_encoded = [\n",
    "        { \n",
    "          'dna'  : dna_lang.encode(dataset[i]['dna'], max_len=max_length),\n",
    "          'prot' : prot_lang.encode(dataset[i]['prot'], max_len=max_length)\n",
    "        } for i in range(len(dataset))\n",
    "    ]\n",
    "    return dataset_encoded\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6d24eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### step2b: encode your sequences here\n",
    "########################\n",
    "\n",
    "# define maximum number of codons\n",
    "# we truncate any sequence longer than this length, and pad any sequence shorter than this length\n",
    "# think about a sensible length for the input sequences\n",
    "max_length = ...\n",
    "\n",
    "# encode the training and validation data\n",
    "train_set_encoded = FUNCTION_CALL(..., dna_lang, prot_lang, ...) \n",
    "val_set_encoded = FUNCTION_CALL(..., dna_lang, prot_lang, ...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f4600e-4463-4d06-8078-57095cfcc6b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c90e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# take a look at the encoding of a DNA\n",
    "train_set[0]['dna'], train_set_encoded[0]['dna'].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2859dc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# take a look at the encoding of a protein\n",
    "train_set[0]['prot'], train_set_encoded[0]['prot'].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c56393",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define dataloader for the encoded sequences\n",
    "def get_dataloader(dataset, batch_size):\n",
    "    cur_sampler = RandomSampler(dataset)\n",
    "    cur_dataloader = DataLoader(dataset, sampler=cur_sampler, batch_size=batch_size, drop_last=True, num_workers=16)\n",
    "    return cur_dataloader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03de687-77f7-442c-8e89-75039b83137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### step 2c: create a dataloader for the validation and training sequences\n",
    "########################\n",
    "\n",
    "# how many samples should be trained on simultaneously?\n",
    "batch_size = ...\n",
    "\n",
    "# define dataloader for training\n",
    "train_loader = FUNCTION_CALL(..., ...)\n",
    "val_loader = FUNCTION_CALL(..., ...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09e9ace-2dfe-4f7a-a188-acccd586216a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09d563e8-254f-495e-9a96-ffbfc60e54c3",
   "metadata": {},
   "source": [
    "# Step 3: Define model\n",
    "We created languages for DNA and protein sequences, and encoded all sequences through the encodings defined by these languages. We then created data loaders for these encoded sequences. As a final preparation, we define our [RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html).\n",
    "Create a class that instructs pytorch to make a RNN model using the given input, hidden size and output parameters. You'll have to define the init and forward functions. Additionally, in the Pytorch Lightning class, you must choose a loss function and optimizer that are appropriate for the problem you are trying to solve. Once you have thought about your model, we will go over the architecture together with the class. Hint: for the RNN, you'll need to also define the hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dacb684-577f-4acc-91c0-11ccf6decd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the device (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030f08f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### step 3a: define the model architecture\n",
    "########################\n",
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MyRNN,self).__init__()\n",
    "\n",
    "        ########################\n",
    "        # define model layers (rnn), pseudocode:\n",
    "        # look up in the documentation! https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
    "        self.rnn = nn.RNN(...)\n",
    "        ########################\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        input_data = input_data.to(device)\n",
    "        \n",
    "        ########################\n",
    "        # define model and hidden state\n",
    "        ...\n",
    "        \n",
    "        return ...\n",
    "        ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da78f3f0-2557-4fd1-87b8-c859596fc250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c898b30-16e4-45ff-9bc3-a5d61253b139",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### step 3b: define the lightning module to train the model\n",
    "########################\n",
    "\n",
    "# lightning module to train the sequence model\n",
    "class SequenceModelLightning(L.LightningModule):\n",
    "    def __init__(self, input_size, hidden_size, output_size, lr=0.1):\n",
    "        super().__init__()\n",
    "        self.model = MyRNN(input_size, hidden_size, output_size)\n",
    "        self.lr = lr\n",
    "\n",
    "        ########################\n",
    "        # define loss function here, pseudocode:\n",
    "        self.loss = ...\n",
    "        ########################\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_tensor = batch['dna']\n",
    "        target_tensor = batch['prot']\n",
    "        \n",
    "        output = self.model(input_tensor)\n",
    "        loss = self.loss(input=output, target=target_tensor)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_tensor = batch['dna']\n",
    "        target_tensor = batch['prot']\n",
    "\n",
    "        output = self.model(input_tensor)\n",
    "        loss = self.loss(input=output, target=target_tensor)\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # define optimizer here\n",
    "        return optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea8a372-8d21-4319-aa64-de48824bf314",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a47bdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### step3c: define the input parameters for the training loop\n",
    "########################\n",
    "\n",
    "# define the model and training loop\n",
    "# think of the dimensionality of your input data (dna sequences) and output data (protein sequence), and where these numbers are stored\n",
    "lit_model = SequenceModelLightning(input_size = ...,\n",
    "                                  hidden_size = 0,\n",
    "                                  output_size = ...,\n",
    "                                  lr = ...)\n",
    "\n",
    "# define the trainer\n",
    "trainer = L.Trainer(devices = 1, \n",
    "                    max_epochs = ...)\n",
    "\n",
    "# learn the weights of the model\n",
    "trainer.fit(lit_model, ..., ...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62741354-698d-4b9f-8621-1e0d776c376d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eac6a490",
   "metadata": {},
   "source": [
    "# Step 4: Test the model on random test sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590813ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# show the model architecture\n",
    "my_rnn = lit_model.model\n",
    "my_rnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8ebc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# we encode the test data using the same dna and protein language encodings we defined before\n",
    "# if you change the languages, you need to re-encode the test sequences as well!\n",
    "test_set_encoded = encode_dataset(test_set, dna_lang, prot_lang, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3446bc3f-5338-474f-b6b8-b829f5a7d1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### step4: define the input tensor and get the prediction from your model\n",
    "########################\n",
    "\n",
    "# pick a random sequence from the test set\n",
    "random_pair = np.random.randint(0,len(test_set))\n",
    "\n",
    "# get the encoded dna sequence and its known protein translation\n",
    "dna_sequence = np.array([test_set_encoded[random_pair]['dna']])\n",
    "protein_translation = test_set[random_pair]['prot']\n",
    "target_tensor = test_set_encoded[random_pair]['prot']\n",
    "\n",
    "# send model and input sequence to device, compute translation of sequence\n",
    "my_rnn.to(device)\n",
    "########################\n",
    "input_tensor = ...\n",
    "output = ...\n",
    "########################\n",
    "\n",
    "loss = ((target_tensor - output.cpu())**2).mean()\n",
    "print('loss', loss)\n",
    "\n",
    "# convert output back to protein sequence by taking the most likely amino acid per position, print results\n",
    "result = \"\".join([prot_lang.index2word[i] for i in output.cpu().topk(1)[1].view(-1).numpy()])\n",
    "print(''+protein_translation)\n",
    "print(result, end='\\n\\n')\n",
    "\n",
    "# print accuracy\n",
    "result = \"\".join([prot_lang.index2word[i] for i in output.cpu().topk(1)[1].view(-1).numpy() if i not in [key for key in Language('',1).index2word]])\n",
    "min_len = np.min([len(result),len(protein_translation)])\n",
    "print('Accuracy of aa calling over the sequence: ', np.sum([protein_translation[i] == result[i] for i in range(min_len)])/min_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a25793",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dbf71d51",
   "metadata": {},
   "source": [
    "# Step 5: Interpret the RNN hidden state\n",
    "The RNN you trained has a hidden state, a matrix of size (# of codons) x (# of amino acids). Load the hidden state and interpret the parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f259847d",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### step5: interpret the hidden state of your RNN\n",
    "########################\n",
    "\n",
    "# load the hidden state from your RNN.\n",
    "# annotate the hidden state with the matching codons (in 'dna_lang.index2word') and amino acids (in 'prot_lang.index2word')\n",
    "# store the result in a dataframe called 'rnn_param'\n",
    "rnn_param = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f8cb7d-58ce-436d-a5d9-949e4aba36df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7082540d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot the hidden state of the RNN. What is the interpretation?\n",
    "use_map =  sns.color_palette(\"RdBu\",as_cmap=True)\n",
    "clus = sns.clustermap(\n",
    "                        rnn_param,\n",
    "                        center=0,\n",
    "                        xticklabels=True, yticklabels=True, \n",
    "                        row_cluster=True, col_cluster=True, \n",
    "                        cmap=use_map,\n",
    "                        figsize=(5,10),\n",
    "                        method='complete', metric='cityblock'\n",
    "                     )\n",
    "\n",
    "plt.rcParams['pdf.fonttype'] = 42 \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0981196-e4ed-43bf-a98f-5cc29905f974",
   "metadata": {},
   "source": [
    "# Steps:\n",
    "2a: create languages for the DNA and protein sequences <br>\n",
    "2b: encode the training and validation sequences <br>\n",
    "2c: create a dataloader for the validation and training sequences <br>\n",
    "3a: define your RNN model <br>\n",
    "3b: define the lightning module to train the model <br>\n",
    "3c: define the input parameters for the training loop <br>\n",
    "Train your model :) <br>\n",
    "4: define the input tensor and get the prediction from your model <br>\n",
    "5: interpret the hidden state of your RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2a7c7b",
   "metadata": {},
   "source": [
    "# Questions:\n",
    "-the model is trained on truncated and padded sequences. Change the setup to train your model on arbitrary length sequences (their actual length). Before training your model, think about the number of samples to use for training, the batch size, and number of epochs. <br>\n",
    "-change the codon length to train on nucleotides instead of codons. This is a bit tricky because of the dimensionality of your input / output data! Does your RNN still train well? <br>\n",
    "-add a different model that uses a LSTM instead of a RNN. Does the LSTM train better? <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6059ac09-f42a-4755-bb77-d22fe0cf88ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7968ce-3d51-4a3b-9048-c605e978accd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdfe759-bad8-4e93-87b5-610b36511fad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
