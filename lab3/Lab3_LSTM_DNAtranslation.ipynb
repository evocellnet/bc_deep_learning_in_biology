{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8d781c1-022f-4af6-8ba3-4da21a9544cf",
   "metadata": {},
   "source": [
    "# Lab 3: RNNs and LSTMs\n",
    "In this tutorial we create neural networks using [Long Short-Term Memory (LSTM)](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) cells. The data and its pre-processing for of this notebook is identical to the first lab (Perceptron) to keep the amount of new information limited. Some of the required code-blocks are empty - requiring your imput to complete the model. A few additional questions at the end challenge you to play around with the code and try things for yourselves.\n",
    "\n",
    "During the session, you will create a LSTM to translate DNA sequences into protein sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25dd79e-1645-44c5-84ce-c9b0f540de5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "import lightning as L\n",
    "\n",
    "# import basic functionality\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "# libraries for plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0814c75e-4c3e-4d74-bcca-6adfba516948",
   "metadata": {},
   "source": [
    "# Step 1: Pre-processing the data\n",
    "Here we download and pre-process the dataset. As before, we only consider DNA sequences that are protein coding, contain a integer number of codons, have a start and stop codon, and do not contain any uncertain nucleotides. Finally, we remove duplicates and randomly mix the sequences. Nothing is different from the last time, so you can simply execute these steps and move on to Step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3fb6e8-f264-4584-b4d2-f8462239cc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# download and unpack DNA coding sequences for human, mouse and yeast\n",
    "############################\n",
    "\n",
    "!mkdir -p ~/all_seqs\n",
    "%cd ~/\n",
    "\n",
    "!wget -P ~/all_seqs/ https://ftp.ensembl.org/pub/current_fasta/homo_sapiens/cds/Homo_sapiens.GRCh38.cds.all.fa.gz\n",
    "!gzip -df \"all_seqs/Homo_sapiens.GRCh38.cds.all.fa.gz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350c0eeb-b2da-4ae8-a5cf-4149a018f7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# function that loads and processes a FASTA file containing coding sequences\n",
    "def load_species_cds(file_name):\n",
    "    dna_seq = []\n",
    "    prot_seq = []\n",
    "    for record in SeqIO.parse(file_name, \"fasta\"):\n",
    "        # ensure that sequences are protein coding\n",
    "        if 'gene_biotype:protein_coding' in record.description:\n",
    "            if 'transcript_biotype:protein_coding' in record.description:\n",
    "                if ' cds ' in record.description:\n",
    "                    if len(record.seq) % 3 == 0:\n",
    "                        dna_seq.append(str(record.seq))\n",
    "                        prot_seq.append(str(record.seq.translate()))\n",
    "                        \n",
    "    # keep sequences that are protein coding\n",
    "    dna_seq_cod = []\n",
    "    prot_seq_cod = []\n",
    "    for i in range(len(prot_seq)):\n",
    "        if (prot_seq[i][0]=='M') & (prot_seq[i][-1]=='*'):\n",
    "            dna_seq_cod.append(dna_seq[i])\n",
    "            prot_seq_cod.append(prot_seq[i])\n",
    "\n",
    "    # avoid sequences with undetermined/uncertain nucleotides\n",
    "    dna_seq_cod = [dna_seq_cod[i] for i in range(len(dna_seq_cod)) if ('N' not in dna_seq_cod[i])]\n",
    "    prot_seq_cod = [prot_seq_cod[i] for i in range(len(dna_seq_cod)) if ('N' not in dna_seq_cod[i])]\n",
    " \n",
    "    # remove duplicates and randomly mix the list of sequences\n",
    "    seqs = list(zip(dna_seq_cod, prot_seq_cod))\n",
    "    seqs = list(set(seqs))\n",
    "    random.shuffle(seqs)\n",
    "    dna_seq_cod, prot_seq_cod = zip(*seqs)\n",
    "\n",
    "    # pack samples as a list of dictionaries and return result\n",
    "    seq_data = [{'dna':dna_seq_cod[i],'prot':prot_seq_cod[i]} for i in range(len(dna_seq_cod))]\n",
    "    return seq_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4ce519-1289-42de-9007-7a1690110b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load coding sequences for different species\n",
    "print('loading human proteins')\n",
    "seq_data = load_species_cds(\"all_seqs/Homo_sapiens.GRCh38.cds.all.fa\")\n",
    "\n",
    "# take a look at some sequences\n",
    "[seq_data[i]['dna'][0:40]+'...' for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a350a-500e-43fd-8bd5-2becb21d0965",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('number of sequences: ', len(seq_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076ce85a-62cb-484c-a4d2-4acdb208b4b2",
   "metadata": {},
   "source": [
    "# Step 2: Encoding the sequences\n",
    "Having prepared the coding sequences and their translation, we convert them into a numeric representation as vectors. To do so, we first construct a language class that stores words of each language and allows us to convert between encoding/indices and words in a language. We define a function that allows us to store any sequence of words (i.e., codons or bases) as a numeric representation. Here we extend every sequence with a start of sentence <SOS> and end of sentence <EOS> token, such that the model knows when to start and stop translating. Finally, we can extend every sentence to the same length by padding with an empty \"word\" that is not translated or used, but allows us to use the identical-length numerical representation of the sentences as input for the model. The language allows for a simple encoding of words as numbers or as numerical vectors (one-hot-encoding). The 'encode' function converts an input sentence to the specified encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4050adab-da35-4577-a3bb-3b336caf1a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class to store a language\n",
    "class Language:\n",
    "    # initialize the language, as standard we have start of sentence (SOS), end of sentence (EOS) and a padding to equalize sentence lengths (PAD)\n",
    "    def __init__(self, name, codon_len):\n",
    "        self.name = name\n",
    "        self.word2index = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2}\n",
    "        self.encoding = {}\n",
    "        self.index2word = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\"}\n",
    "        self.n_words = 3  # Count SOS and EOS\n",
    "        self.codon_length = codon_len\n",
    "\n",
    "    # function to add sentence to language (add new words in the sentence to our language)\n",
    "    def addSentence(self, sentence):\n",
    "        for word in [sentence[i:i+self.codon_length] for i in range(0, len(sentence), self.codon_length)]:\n",
    "            self.addWord(word)\n",
    "\n",
    "    # function to add word to language\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "            \n",
    "    # function to convert indices to one-hot encodings (i.e., 3 becomes [0,0,0,1,0,0,...])\n",
    "    def as_one_hot(self):\n",
    "        for key in self.word2index:\n",
    "            new_val = np.zeros(len(self.word2index),dtype=np.int32)\n",
    "            new_val[self.word2index[key]] = 1\n",
    "            self.encoding[key] = new_val\n",
    "    \n",
    "    # function to convert indices to simple encodings\n",
    "    def as_encoding(self):\n",
    "        self.encoding = self.word2index\n",
    "\n",
    "    # function to encode (and pad) a sentence\n",
    "    # we use this to take an input sentence and convert it to a sequence of arrays that represent that sentence in a given language\n",
    "    # in the context of proteins, think of this as encoding the bases or codons\n",
    "    def encode(self, sentence, max_len=None):\n",
    "        sos = [self.encoding[\"<SOS>\"]]\n",
    "        eos = [self.encoding[\"<EOS>\"]]\n",
    "        pad = [self.encoding[\"<PAD>\"]]\n",
    "\n",
    "        # split sentence in blocks of a given codon_length\n",
    "        sentence_split = [sentence[i:i+self.codon_length] for i in range(0, len(sentence), self.codon_length)]\n",
    "            \n",
    "        # encode sentence in the given language\n",
    "        sentence_encoded = [self.encoding[word] for word in sentence_split]\n",
    "\n",
    "        # only pad or truncate if a maximum length is specified\n",
    "        if max_len is not None:\n",
    "            if len(sentence_split) < max_len - 2: \n",
    "                # sentence is shorter than max length-2; add SOS and EOS and pad to maximum length\n",
    "                n_pads = max_len - 2 - len(sentence_split)\n",
    "                return torch.Tensor(np.array(sos + sentence_encoded + eos + pad * n_pads))\n",
    "            else: \n",
    "                # sentence is longer than max length; truncate and add SOS and EOS\n",
    "                sentence_truncated = sentence_encoded[:max_len - 2]\n",
    "                return torch.Tensor(np.array(sos + sentence_truncated + eos))\n",
    "        else:\n",
    "            return torch.Tensor(np.array(sos + sentence_encoded + eos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01b68ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### step2a: create languages for DNA and protein sequences\n",
    "########################\n",
    "\n",
    "# create a language for DNA and protein sequences\n",
    "dna_lang = Language(name=\"dna\", codon_len=3)\n",
    "prot_lang = Language(name=\"prot\", codon_len=1)\n",
    "\n",
    "# split the sequence data ('seq_data') that we defined above into sensible training, validation and test sets\n",
    "# think about how much data would realistically be necessary to learn the problem of translating DNA sequences\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(..., ...)\n",
    "\n",
    "# memorize the dna and protein languages by parsing all sequences\n",
    "for cur_seq in train_set:\n",
    "    dna_lang.addSentence(cur_seq['dna'])\n",
    "    prot_lang.addSentence(cur_seq['prot'])\n",
    "\n",
    "# create an one-hot-encoding for all words codons and a simple encoding for all amino acids\n",
    "# call the appropriate functions for each of the two languages\n",
    "dna_lang.FUNCTION_CALL()\n",
    "prot_lang.FUNCTION_CALL()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be9f436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4eeb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# here we define a function for encoding a dataset of dna and protein sequences\n",
    "def encode_dataset(dataset, dna_lang, prot_lang, max_length):\n",
    "    dataset_encoded = [\n",
    "        { \n",
    "          'dna'  : dna_lang.encode(dataset[i]['dna'], max_len=max_length),\n",
    "          'prot' : prot_lang.encode(dataset[i]['prot'], max_len=max_length)\n",
    "        } for i in range(len(dataset))\n",
    "    ]\n",
    "    return dataset_encoded\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6d24eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### step2b: encode your sequences here\n",
    "########################\n",
    "\n",
    "# define maximum number of codons\n",
    "# we truncate any sequence longer than this length, and pad any sequence shorter than this length\n",
    "# think about a sensible length for the input sequences\n",
    "max_length = ...\n",
    "\n",
    "# encode the training and validation data\n",
    "train_set_encoded = FUNCTION_CALL(..., dna_lang, prot_lang, ...)\n",
    "val_set_encoded = FUNCTION_CALL(..., dna_lang, prot_lang, ...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057b7fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c90e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# take a look at the encoding of a DNA\n",
    "train_set[0]['dna'], train_set_encoded[0]['dna']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2859dc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# take a look at the encoding of a protein\n",
    "train_set[0]['prot'], train_set_encoded[0]['prot']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c56393",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define dataloader for the encoded sequences\n",
    "def get_dataloader(dataset, batch_size):\n",
    "    cur_sampler = RandomSampler(dataset)\n",
    "    cur_dataloader = DataLoader(dataset, sampler=cur_sampler, batch_size=batch_size, drop_last=True, num_workers=15)\n",
    "    return cur_dataloader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03de687-77f7-442c-8e89-75039b83137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### step 2c: create a dataloader for the validation and training sequences\n",
    "########################\n",
    "\n",
    "# how many samples should be trained on simultaneously?\n",
    "batch_size = ...\n",
    "\n",
    "# define dataloader for training\n",
    "train_loader = FUNCTION_CALL(..., ...)\n",
    "val_loader = FUNCTION_CALL(..., ...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1ecc13-1767-4997-a7f0-b295a5e8a413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09d563e8-254f-495e-9a96-ffbfc60e54c3",
   "metadata": {},
   "source": [
    "# Step 3: Define model\n",
    "We created languages for DNA and protein sequences, and encoded all sequences through the encodings defined by these languages. We then created data loaders for these encoded sequences. As a final preparation, we define our [model](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html). \n",
    "Create a class that instructs pytorch to make a LSTM model using the given input, hidden size and output parameters. You'll have to define the init and forward functions. Additionally, in the Pytorch Lightning class, you must choose a loss function and optimizer that are appropriate for the problem you are trying to solve. Once you have thought about your model, we will go over the architecture together with the class. Hint: for the LSTM, you'll need to also define the hidden and cell states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dacb684-577f-4acc-91c0-11ccf6decd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the device (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c12bb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### step 3a: define the model architecture\n",
    "########################\n",
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MyLSTM,self).__init__()\n",
    "        \n",
    "        # input parameters\n",
    "        # ...\n",
    "        # ...\n",
    "        # ...\n",
    "\n",
    "        # define model layers (LSTM), pseudocode:\n",
    "        self.LSTM = nn.LSTM(input_1, ..., input_k, batch_first = True)\n",
    "        # ...\n",
    "\n",
    "    def forward(self,inp):\n",
    "        inp1 = inp.to(device)\n",
    "        \n",
    "        # define initial hidden and cell states of LSTM, e.g.:\n",
    "        # HIDDEN = torch.zeros(1, inp1.size(0), ...).double().to(inp.device)\n",
    "        # ...\n",
    "        \n",
    "        # run LSTM, pseudocode:\n",
    "        output_1, ..., output_n = self.LSTM(input_1, ..., input_m) \n",
    "        \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce47f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c898b30-16e4-45ff-9bc3-a5d61253b139",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### step 3b: define the lightning module to train the model\n",
    "########################\n",
    "\n",
    "# lightning module to train the sequence model\n",
    "class SequenceModelLightning(L.LightningModule):\n",
    "    def __init__(self, input_size, hidden_size, output_size, lr=0.1):\n",
    "        super().__init__()\n",
    "        self.model = MyLSTM(input_size, hidden_size, output_size).double()\n",
    "        self.lr = lr\n",
    "        # define loss function here, pseudocode:\n",
    "        self.loss = nn.MY_LOSS_FUNCTION()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_tensor = batch[0].double()\n",
    "        target_tensor = batch[1].double()\n",
    "\n",
    "        output = self.model(input_tensor)\n",
    "        loss = self.loss(output.view(-1, output.shape[2]),target_tensor.view(-1).long())\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_tensor = batch[0].double()\n",
    "        target_tensor = batch[1].double()\n",
    "\n",
    "        output = self.model(input_tensor)\n",
    "        loss = self.loss(output.view(-1, output.shape[2]),target_tensor.view(-1).long())\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # define optimizer here\n",
    "        return MY_OPTIMIZER_CALL()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e930bbd3-ba4b-4963-915f-8d494ff7870d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a47bdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### step3c: define the input parameters for the training loop\n",
    "########################\n",
    "\n",
    "# define the model and training loop\n",
    "# think of the dimensionality of your input data (dna sequences) and output data (protein sequence), and where these numbers are stored\n",
    "lit_model = SequenceModelLightning(input_size = ...,\n",
    "                                  hidden_size = ...,\n",
    "                                  output_size = ...,\n",
    "                                  lr = ...)\n",
    "\n",
    "# define the trainer\n",
    "trainer = L.Trainer(devices = 1, \n",
    "                    max_epochs = ...)\n",
    "\n",
    "# learn the weights of the model\n",
    "trainer.fit(lit_model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590813ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4678abab-c139-4349-98c3-8fdbae185664",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# evaluate the performance of your model on the validation data\n",
    "trainer.validate(lit_model, val_loader)\n",
    "\n",
    "# show the model architecture\n",
    "my_lstm = lit_model.model\n",
    "my_lstm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd6ef5b-8de7-400a-b7bd-e62130182ec7",
   "metadata": {},
   "source": [
    "# Step 4: Test the model on random test sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c98e35-de82-402d-a555-e6528908ab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# we encode the test data using the same dna and protein language encodings we defined before\n",
    "# if you change the languages, you need to re-encode the test sequences as well!\n",
    "test_set_encoded = encode_dataset(test_set, dna_lang, prot_lang, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47a6dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### step4: define the input tensor and get the prediction from your model\n",
    "########################\n",
    "\n",
    "# pick a random sequence from the test set\n",
    "random_pair = np.random.randint(0,len(test_set))\n",
    "\n",
    "# get the encoded dna sequence and its known protein translation\n",
    "dna_sequence = np.array([test_set_encoded[random_pair]['dna']])\n",
    "protein_translation = test_set[random_pair]['prot']\n",
    "\n",
    "# send model and input sequence to device, compute translation of sequence\n",
    "my_lstm.cuda()\n",
    "input_tensor = ...\n",
    "output = ...\n",
    "\n",
    "# convert output back to protein sequence by taking the most likely amino acid per position, print results\n",
    "result = \"\".join([prot_lang.index2word[i] for i in output.cpu().topk(1)[1].view(-1).numpy()])\n",
    "print('     '+protein_translation)\n",
    "print(result, end='\\n\\n')\n",
    "\n",
    "# print accuracy\n",
    "result = \"\".join([prot_lang.index2word[i] for i in output.cpu().topk(1)[1].view(-1).numpy() if i not in [key for key in Language('',1).index2word]])\n",
    "min_len = np.min([len(result),len(protein_translation)])\n",
    "print('Accuracy of aa calling over the sequence: ', np.sum([protein_translation[i] == result[i] for i in range(min_len)])/min_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7503d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0981196-e4ed-43bf-a98f-5cc29905f974",
   "metadata": {},
   "source": [
    "# Steps: <br>\n",
    "2a: create languages for the DNA and protein sequences <br>\n",
    "2b: encode the training and validation sequences <br>\n",
    "2c: create a dataloader for the validation and training sequences <br>\n",
    "3a: define your LSTM model <br>\n",
    "3b: define the lightning module to train the model <br>\n",
    "3c: define the input parameters for the training loop <br>\n",
    "Train your model :) <br>\n",
    "4: define the input tensor and get the prediction from your model <br>\n",
    " <br>\n",
    "# Questions: <br>\n",
    "-The test sequences are truncated at or padded to max_length. Change the encoding of the test sequences to work for arbitrary lengths (the actual length of the sequences). <br>\n",
    "-The model is trained on truncated and padded sequences. Change the setup to train your model on arbitrary length sequences (their actual length). Before you train the model, think about a) the number of samples to use for training, b) batch sizes, c) number of epochs <br>\n",
    "-Can you modify the code to train it on a single DNA sequence (i.e. a single sample)? Can you achieve perfect/reasonable accuracy? <br>\n",
    "-What is the minimum model size to reach a 'good' accuracy? What is the minimum number of required samples? Think about these questions in context of a classical machine learning classifier <br>\n",
    "-Can you change the code to use a DNA language for single nucleotides instead of codons and train the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2a7c7b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
