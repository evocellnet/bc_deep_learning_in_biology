{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de201704-f280-477c-9a65-121bdf5c12ec",
   "metadata": {},
   "source": [
    "# Lab 3a solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec23c75-a7c1-4c1c-8bf0-19158c5fb469",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### STEP 2a - encoding of sequences\n",
    "# define the length \n",
    "len_dna=3*6\n",
    "len_prot=round(len_dna/3)\n",
    "\n",
    "# encode the samples\n",
    "encode_nr_samples = 5000\n",
    "DNAseq_encoded = [encode_and_pad(dna_lang, sentence[0:len_dna], codon_length, context) for sentence in DNAseq[0:encode_nr_samples]]\n",
    "ProtSeq_encoded = [encode_and_pad(prot_lang, sentence[0:len_prot], 1, context) for sentence in ProtSeq[0:encode_nr_samples]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f10b1a-202c-43be-9025-058b06be68c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### STEP 2a - encoding of sequences\n",
    "# the samples themselves are irrelevant for training! we do not need to stick to a certain sample size for LSTMs\n",
    "# we can simply use other sample sizes after training\n",
    "# merge the first thousand sequences for training\n",
    "DNAseq_merged = ''.join(DNAseq[0:1000])\n",
    "ProtSeq_merged = ''.join(ProtSeq[0:1000])\n",
    "\n",
    "# cut sequences up into snippets of defined length\n",
    "DNAseq_snippets = [DNAseq_merged[i*len_dna:(i+1)*len_dna] for i in range(round(len(DNAseq_merged)/len_dna)) if len(DNAseq_merged[i*len_dna:(i+1)*len_dna]) == len_dna]\n",
    "protSeq_snippets = [ProtSeq_merged[i*len_prot:(i+1)*len_prot] for i in range(round(len(ProtSeq_merged)/len_prot)) if len(ProtSeq_merged[i*len_prot:(i+1)*len_prot]) == len_prot]\n",
    "\n",
    "# encode the samples\n",
    "DNAseq_encoded = [encode_and_pad(dna_lang, sentence, codon_length, context) for sentence in DNAseq_snippets[0:encode_nr_samples]]\n",
    "ProtSeq_encoded = [encode_and_pad(prot_lang, sentence, 1, context) for sentence in protSeq_snippets[0:encode_nr_samples]]\n",
    "\n",
    "# take a look ad the snippets\n",
    "DNAseq_snippets[0:5], protSeq_snippets[0:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9798c8-1eab-4d15-a5b1-6d1288ba8e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### STEP 2b - dataloader\n",
    "batch_size = 200\n",
    "\n",
    "# define dataloader for training\n",
    "nr_training_samples = 1000\n",
    "train_dl = get_dataloader(DNAseq_encoded[0:nr_training_samples], ProtSeq_encoded[0:nr_training_samples], batch_size)\n",
    "\n",
    "# define dataloader for validation\n",
    "len_val_set = 100\n",
    "val_dl = get_dataloader(DNAseq_encoded[nr_training_samples:nr_training_samples+len_val_set], ProtSeq_encoded[nr_training_samples:nr_training_samples+len_val_set], len_val_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596eccb1-881f-42f7-b2cb-aa8461ab4d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### STEP 3a - model architecture\n",
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MyLSTM,self).__init__()\n",
    "        # input parameters\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # linear layer\n",
    "       # self.lin0 = nn.Linear(input_size, input_size)\n",
    "        \n",
    "        # define LSTM\n",
    "        self.LSTM = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        \n",
    "        # linear layer\n",
    "        self.lin1 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self,inp):\n",
    "        inp1 = inp.to(device)\n",
    "        \n",
    "        # run first linear layer (embedding)\n",
    "#        inp1 = self.lin0(inp1)\n",
    "        \n",
    "        # define initial hidden and cell states \n",
    "        h0 = torch.zeros(1, inp1.size(0), self.hidden_size).double().to(inp.device)\n",
    "        c0 = torch.zeros(1, inp1.size(0), self.hidden_size).double().to(inp.device)\n",
    "        \n",
    "        # run LSTM\n",
    "        lstm_output, _ = self.LSTM(inp1,(h0,c0))\n",
    "        \n",
    "        # return second linear layer and return output\n",
    "        return self.lin1(lstm_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec0dc7d-4151-489d-9f33-76b6c547ef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### STEP 3b - loss function and optimizer\n",
    "# lightning module to train the sequence model\n",
    "class SequenceModelLightning(L.LightningModule):\n",
    "    def __init__(self, input_size, hidden_size, output_size, lr=0.1):\n",
    "        super().__init__()\n",
    "        self.model = MyLSTM(input_size, hidden_size, output_size).double()\n",
    "        self.lr = lr\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_tensor = batch[0].double()\n",
    "        target_tensor = batch[1].double()\n",
    "\n",
    "        output = self.model(input_tensor)\n",
    "        loss = self.loss(output.view(-1, output.shape[2]),target_tensor.view(-1).long())\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_tensor = batch[0].double()\n",
    "        target_tensor = batch[1].double()\n",
    "\n",
    "        output = self.model(input_tensor)\n",
    "        loss = self.loss(output.view(-1, output.shape[2]),target_tensor.view(-1).long())\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # define optimizer here\n",
    "        return optim.Adam(self.parameters(), lr=self.lr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f588f722-6cc6-4d3c-abb2-becf41553adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### STEP 3c - model and training loop\n",
    "lit_model = SequenceModelLightning(input_size = len(dna_lang.word2index),\n",
    "                                  hidden_size = 24,\n",
    "                                  output_size = len(prot_lang.index2word),\n",
    "                                  lr = 0.1)\n",
    "\n",
    "# define the trainer\n",
    "trainer = L.Trainer(devices = 1, \n",
    "                    max_epochs = 20)\n",
    "\n",
    "\n",
    "# learn the weights of the model\n",
    "trainer.fit(lit_model, train_dl, val_dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7142dca-009f-4cb6-a26e-9275905745ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### STEP 4 - encode test sequences of arbitrary length\n",
    "max_len = 10000\n",
    "test_DNAseq_encoded  = [encode_and_pad(dna_lang, sentence, codon_length, max_len) for sentence in test_DNAseq]\n",
    "test_ProtSeq_encoded = [encode_and_pad(prot_lang, sentence, 1, max_len) for sentence in test_ProtSeq]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9790db01-2c80-4f91-aa32-67144420b26c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e7987e-f714-4afc-a94e-ec7430753136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ad39cf-fb07-40fa-9b67-a46acd02b790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64ef403-908b-4421-ac67-52d4311d9434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d914ec15-6304-4931-a8b0-7c840c307bb7",
   "metadata": {},
   "source": [
    "# Lab 3b solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e8df80-6d1d-4a2f-b48b-1cd3b82a251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "#### STEP 1\n",
    "# select sentence and create language\n",
    "gen_lang = Lang(\"text_gen\")\n",
    "gen_lang.addSentence(cur_text.split(), 1)\n",
    "gen_lang.to_encoding()\n",
    "\n",
    "# number of words in language\n",
    "gen_lang.n_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18c72d3-ae3c-4abd-b36b-3a80a66bb58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### STEP 2a\n",
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MyLSTM,self).__init__()\n",
    "        # input parameters\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # linear layer\n",
    "        self.lin0 = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        # define LSTM\n",
    "        self.LSTM = nn.LSTM(hidden_size, hidden_size)\n",
    "        \n",
    "        # linear layer\n",
    "        self.lin1 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        inp1 = inp.to(device)\n",
    "        \n",
    "        # first linear layer\n",
    "        inp1 = self.lin0(inp1)\n",
    "        \n",
    "        # define initial hidden and cell states \n",
    "        h0 = torch.zeros(1, 1, self.hidden_size).double().to(inp.device)\n",
    "        c0 = torch.zeros(1, 1, self.hidden_size).double().to(inp.device)\n",
    "        \n",
    "        # run LSTM\n",
    "        lstm_output, _ = self.LSTM(inp1,(h0,c0))\n",
    "        \n",
    "        # return second linear layer and return output\n",
    "        return self.lin1(lstm_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75beba5-fbc4-4cdb-8429-0c2c36d8fe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### STEP 2b\n",
    "# initialize optimizer\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr=0.1)\n",
    "\n",
    "# initialize loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
